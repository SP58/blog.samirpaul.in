[{"content":"Table of Contents The Study Plan What is it? Why use it? How to use it Don\u0026rsquo;t feel you aren\u0026rsquo;t smart enough A Note About Video Resources Choose a Programming Language Books for Data Structures and Algorithms Interview Prep Books Don\u0026rsquo;t Make My Mistakes What you Won\u0026rsquo;t See Covered The Daily Plan Coding Question Practice Coding Problems Topics of Study Algorithmic complexity / Big-O / Asymptotic analysis Data Structures Arrays Linked Lists Stack Queue Hash table More Knowledge Binary search Bitwise operations Trees Trees - Intro Binary search trees: BSTs Heap / Priority Queue / Binary Heap balanced search trees (general concept, not details) traversals: preorder, inorder, postorder, BFS, DFS Sorting selection insertion heapsort quicksort merge sort Graphs directed undirected adjacency matrix adjacency list traversals: BFS, DFS Even More Knowledge Recursion Dynamic Programming Design Patterns Combinatorics (n choose k) \u0026amp; Probability NP, NP-Complete and Approximation Algorithms How computers process a program Caches Processes and Threads Testing String searching \u0026amp; manipulations Tries Floating Point Numbers Unicode Endianness Networking Final Review Getting the Job Update Your Resume Find a Job Interview Process \u0026amp; General Interview Prep Be thinking of for when the interview comes Have questions for the interviewer Once You\u0026rsquo;ve Got The Job \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- Everything below this point is optional \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\nOptional Extra Topics \u0026amp; Resources Additional Books System Design, Scalability, Data Handling (if you have 4+ years experience) Additional Learning Compilers Emacs and vi(m) Unix command line tools Information theory Parity \u0026amp; Hamming Code Entropy Cryptography Compression Computer Security Garbage collection Parallel Programming Messaging, Serialization, and Queueing Systems A* Fast Fourier Transform Bloom Filter HyperLogLog Locality-Sensitive Hashing van Emde Boas Trees Augmented Data Structures Balanced search trees AVL trees Splay trees Red/black trees 2-3 search trees 2-3-4 Trees (aka 2-4 trees) N-ary (K-ary, M-ary) trees B-Trees k-D Trees Skip lists Network Flows Disjoint Sets \u0026amp; Union Find Math for Fast Processing Treap Linear Programming Geometry, Convex hull Discrete math Additional Detail on Some Subjects Video Series Computer Science Courses Papers Why use it? If you want to work as a software engineer for a large company, these are the things you have to know.\nIf you missed out on getting a degree in computer science, like I did, this will catch you up and save four years of your life.\nWhen I started this project, I didn\u0026rsquo;t know a stack from a heap, didn\u0026rsquo;t know Big-O anything, or anything about trees, or how to traverse a graph. If I had to code a sorting algorithm, I can tell ya it would have been terrible. Every data structure I had ever used was built into the language, and I didn\u0026rsquo;t know how they worked under the hood at all. I never had to manage memory unless a process I was running would give an \u0026ldquo;out of memory\u0026rdquo; error, and then I\u0026rsquo;d have to find a workaround. I used a few multidimensional arrays in my life and thousands of associative arrays, but I never created data structures from scratch.\nIt\u0026rsquo;s a long plan. It may take you months. If you are familiar with a lot of this already it will take you a lot less time.\nHow to use it Everything below is an outline, and you should tackle the items in order from top to bottom.\nI\u0026rsquo;m using GitHub\u0026rsquo;s special markdown flavor, including tasks lists to track progress.\nMore about GitHub-flavored markdown If you don\u0026rsquo;t want to use git On this page, click the Code button near the top, then click \u0026ldquo;Download ZIP\u0026rdquo;. Unzip the file and you can work with the text files.\nIf you\u0026rsquo;re open in a code editor that understands markdown, you\u0026rsquo;ll see everything formatted nicely.\nIf you\u0026rsquo;re comfortable with git Create a new branch so you can check items like this, just put an x in the brackets: [x]\nFork the GitHub repo: https://github.com/jwasham/coding-interview-university by clicking on the Fork button.\nClone to your local repo:\ngit clone git@github.com:\u0026lt;your_github_username\u0026gt;/coding-interview-university.git cd coding-interview-university git checkout -b progress git remote add jwasham https://github.com/jwasham/coding-interview-university git fetch --all Mark all boxes with X after you completed your changes:\ngit add . git commit -m \u0026#34;Marked x\u0026#34; git rebase jwasham/main git push --set-upstream origin progress git push --force Don\u0026rsquo;t feel you aren\u0026rsquo;t smart enough Successful software engineers are smart, but many have an insecurity that they aren\u0026rsquo;t smart enough. Following videos may help you overcome this insecurity: The myth of the Genius Programmer It\u0026rsquo;s Dangerous to Go Alone: Battling the Invisible Monsters in Tech A Note About Video Resources Some videos are available only by enrolling in a Coursera or EdX class. These are called MOOCs. Sometimes the classes are not in session so you have to wait a couple of months, so you have no access.\nIt would be great to replace the online course resources with free and always-available public sources, such as YouTube videos (preferably university lectures), so that you people can study these anytime, not just when a specific online course is in session.\nChoose a Programming Language You\u0026rsquo;ll need to choose a programming language for the coding interviews you do, but you\u0026rsquo;ll also need to find a language that you can use to study computer science concepts.\nPreferably the language would be the same, so that you only need to be proficient in one.\nFor this Study Plan When I did the study plan, I used 2 languages for most of it: C and Python\nC: Very low level. Allows you to deal with pointers and memory allocation/deallocation, so you feel the data structures and algorithms in your bones. In higher level languages like Python or Java, these are hidden from you. In day to day work, that\u0026rsquo;s terrific, but when you\u0026rsquo;re learning how these low-level data structures are built, it\u0026rsquo;s great to feel close to the metal. C is everywhere. You\u0026rsquo;ll see examples in books, lectures, videos, everywhere while you\u0026rsquo;re studying. The C Programming Language, Vol 2 This is a short book, but it will give you a great handle on the C language and if you practice it a little you\u0026rsquo;ll quickly get proficient. Understanding C helps you understand how programs and memory work. You don\u0026rsquo;t need to go super deep in the book (or even finish it). Just get to where you\u0026rsquo;re comfortable reading and writing in C. Answers to questions in the book Python: Modern and very expressive, I learned it because it\u0026rsquo;s just super useful and also allows me to write less code in an interview. This is my preference. You do what you like, of course.\nYou may not need it, but here are some sites for learning a new language:\nExercism Codewars HackerEarth Scaler Topics (Java, C++) For your Coding Interview You can use a language you are comfortable in to do the coding part of the interview, but for large companies, these are solid choices:\nC++ Java Python You could also use these, but read around first. There may be caveats:\nJavaScript Ruby Here is an article I wrote about choosing a language for the interview: Pick One Language for the Coding Interview. This is the original article my post was based on: Choosing a Programming Language for Interviews\nYou need to be very comfortable in the language and be knowledgeable.\nRead more about choices:\nChoose the Right Language for Your Coding Interview Books for Data Structures and Algorithms This book will form your foundation for computer science.\nJust choose one, in a language that you will be comfortable with. You\u0026rsquo;ll be doing a lot of reading and coding.\nC Algorithms in C, Parts 1-5 (Bundle), 3rd Edition Fundamentals, Data Structures, Sorting, Searching, and Graph Algorithms Python Data Structures and Algorithms in Python by Goodrich, Tamassia, Goldwasser I loved this book. It covered everything and more. Pythonic code my glowing book report: https://startupnextdoor.com/book-report-data-structures-and-algorithms-in-python/ Java Your choice:\nGoodrich, Tamassia, Goldwasser Data Structures and Algorithms in Java Sedgewick and Wayne: Algorithms Free Coursera course that covers the book (taught by the authors!): Algorithms I Algorithms II C++ Your choice:\nGoodrich, Tamassia, and Mount Data Structures and Algorithms in C++, 2nd Edition Sedgewick and Wayne Algorithms in C++, Parts 1-4: Fundamentals, Data Structure, Sorting, Searching Algorithms in C++ Part 5: Graph Algorithms Interview Prep Books You don\u0026rsquo;t need to buy a bunch of these. Honestly \u0026ldquo;Cracking the Coding Interview\u0026rdquo; is probably enough, but I bought more to give myself more practice. But I always do too much.\nI bought both of these. They gave me plenty of practice.\nProgramming Interviews Exposed: Coding Your Way Through the Interview, 4th Edition Answers in C++ and Java This is a good warm-up for Cracking the Coding Interview Not too difficult. Most problems may be easier than what you\u0026rsquo;ll see in an interview (from what I\u0026rsquo;ve read) Cracking the Coding Interview, 6th Edition answers in Java If you have tons of extra time: Choose one:\nElements of Programming Interviews (C++ version) Elements of Programming Interviews in Python Elements of Programming Interviews (Java version) - Companion Project - Method Stub and Test Cases for Every Problem in the Book Don\u0026rsquo;t Make My Mistakes This list grew over many months, and yes, it got out of hand.\nHere are some mistakes I made so you\u0026rsquo;ll have a better experience. And you\u0026rsquo;ll save months of time.\n1. You Won\u0026rsquo;t Remember it All I watched hours of videos and took copious notes, and months later there was much I didn\u0026rsquo;t remember. I spent 3 days going through my notes and making flashcards, so I could review. I didn\u0026rsquo;t need all of that knowledge.\nPlease, read so you won\u0026rsquo;t make my mistakes:\nRetaining Computer Science Knowledge.\n2. Use Flashcards To solve the problem, I made a little flashcards site where I could add flashcards of 2 types: general and code. Each card has different formatting. I made a mobile-first website, so I could review on my phone or tablet, wherever I am.\nMake your own for free:\nFlashcards site repo I DON\u0026rsquo;T RECOMMEND using my flashcards. There are too many and most of them are trivia that you don\u0026rsquo;t need.\nBut if you don\u0026rsquo;t want to listen to me, here you go:\nMy flash cards database (1200 cards): My flash cards database (extreme - 1800 cards): Keep in mind I went overboard and have cards covering everything from assembly language and Python trivia to machine learning and statistics. It\u0026rsquo;s way too much for what\u0026rsquo;s required.\nNote on flashcards: The first time you recognize you know the answer, don\u0026rsquo;t mark it as known. You have to see the same card and answer it several times correctly before you really know it. Repetition will put that knowledge deeper in your brain.\nAn alternative to using my flashcard site is Anki, which has been recommended to me numerous times. It uses a repetition system to help you remember. It\u0026rsquo;s user-friendly, available on all platforms and has a cloud sync system. It costs $25 on iOS but is free on other platforms.\nMy flashcard database in Anki format: https://ankiweb.net/shared/info/25173560 (thanks @xiewenya).\nSome students have mentioned formatting issues with white space that can be fixed by doing the following: open deck, edit card, click cards, select the \u0026ldquo;styling\u0026rdquo; radio button, add the member \u0026ldquo;white-space: pre;\u0026rdquo; to the card class.\n3. Do Coding Interview Questions While You\u0026rsquo;re Learning THIS IS VERY IMPORTANT.\nStart doing coding interview questions while you\u0026rsquo;re learning data structures and algorithms.\nYou need to apply what you\u0026rsquo;re learning to solving problems, or you\u0026rsquo;ll forget. I made this mistake.\nOnce you\u0026rsquo;ve learned a topic, and feel somewhat comfortable with it, for example, linked lists:\nOpen one of the coding interview books (or coding problem websites, listed below) Do 2 or 3 questions regarding linked lists. Move on to the next learning topic. Later, go back and do another 2 or 3 linked list problems. Do this with each new topic you learn. Keep doing problems while you\u0026rsquo;re learning all this stuff, not after.\nYou\u0026rsquo;re not being hired for knowledge, but how you apply the knowledge.\nThere are many resources for this, listed below. Keep going.\n4. Focus There are a lot of distractions that can take up valuable time. Focus and concentration are hard. Turn on some music without lyrics and you\u0026rsquo;ll be able to focus pretty well.\nWhat you won\u0026rsquo;t see covered These are prevalent technologies but not part of this study plan:\nSQL Javascript HTML, CSS, and other front-end technologies The Daily Plan This course goes over a lot of subjects. Each will probably take you a few days, or maybe even a week or more. It depends on your schedule.\nEach day, take the next subject in the list, watch some videos about that subject, and then write an implementation of that data structure or algorithm in the language you chose for this course.\nYou can see my code here:\nC C++ Python You don\u0026rsquo;t need to memorize every algorithm. You just need to be able to understand it enough to be able to write your own implementation.\nCoding Question Practice 🤔 Why is this here? I\u0026rsquo;m not ready to interview.\nThen go back and read this.\nWhy you need to practice doing programming problems:\nProblem recognition, and where the right data structures and algorithms fit in Gathering requirements for the problem Talking your way through the problem like you will in the interview Coding on a whiteboard or paper, not a computer Coming up with time and space complexity for your solutions (see Big-O below) Testing your solutions There is a great intro for methodical, communicative problem solving in an interview. You\u0026rsquo;ll get this from the programming interview books, too, but I found this outstanding: Algorithm design canvas\nWrite code on a whiteboard or paper, not a computer. Test with some sample inputs. Then type it and test it out on a computer.\nIf you don\u0026rsquo;t have a whiteboard at home, pick up a large drawing pad from an art store. You can sit on the couch and practice. This is my \u0026ldquo;sofa whiteboard\u0026rdquo;. I added the pen in the photo just for scale. If you use a pen, you\u0026rsquo;ll wish you could erase. Gets messy quick. I use a pencil and eraser.\nCoding question practice is not about memorizing answers to programming problems.\nCoding Problems Don\u0026rsquo;t forget your key coding interview books here.\nSolving Problems:\nHow to Find a Solution How to Dissect a Topcoder Problem Statement Coding Interview Question Videos:\nIDeserve (88 videos) Tushar Roy (5 playlists) Super for walkthroughs of problem solutions Nick White - LeetCode Solutions (187 Videos) Good explanations of solution and the code You can watch several in a short time FisherCoder - LeetCode Solutions Challenge/Practice sites:\nLeetCode My favorite coding problem site. It\u0026rsquo;s worth the subscription money for the 1-2 months you\u0026rsquo;ll likely be preparing. See Nick White and FisherCoder Videos above for code walk-throughs. HackerRank TopCoder Codeforces Codility Geeks for Geeks InterviewBit AlgoExpert Created by Google engineers, this is also an excellent resource to hone your skills. Project Euler very math focused, and not really suited for coding interviews Let\u0026rsquo;s Get Started Alright, enough talk, let\u0026rsquo;s learn!\nBut don\u0026rsquo;t forget to do coding problems from above while you learn!\nAlgorithmic complexity / Big-O / Asymptotic analysis Nothing to implement here, you\u0026rsquo;re just watching videos and taking notes! Yay! There are a lot of videos here. Just watch enough until you understand it. You can always come back and review. Don\u0026rsquo;t worry if you don\u0026rsquo;t understand all the math behind it. You just need to understand how to express the complexity of an algorithm in terms of Big-O. Harvard CS50 - Asymptotic Notation (video) Big O Notations (general quick tutorial) (video) Big O Notation (and Omega and Theta) - best mathematical explanation (video) Skiena (video) UC Berkeley Big O (video) Amortized Analysis (video) TopCoder (includes recurrence relations and master theorem): Computational Complexity: Section 1 Computational Complexity: Section 2 Cheat sheet [Review] Big-O notation in 5 minutes (video) Well, that\u0026rsquo;s about enough of that.\nWhen you go through \u0026ldquo;Cracking the Coding Interview\u0026rdquo;, there is a chapter on this, and at the end there is a quiz to see if you can identify the runtime complexity of different algorithms. It\u0026rsquo;s a super review and test.\nData Structures Arrays About Arrays: Arrays (video) UC Berkeley CS61B - Linear and Multi-Dim Arrays (video) (Start watching from 15m 32s) Dynamic Arrays (video) Jagged Arrays (video) Implement a vector (mutable array with automatic resizing): Practice coding using arrays and pointers, and pointer math to jump to an index instead of using indexing. New raw data array with allocated memory can allocate int array under the hood, just not use its features start with 16, or if starting number is greater, use power of 2 - 16, 32, 64, 128 size() - number of items capacity() - number of items it can hold is_empty() at(index) - returns item at given index, blows up if index out of bounds push(item) insert(index, item) - inserts item at index, shifts that index\u0026rsquo;s value and trailing elements to the right prepend(item) - can use insert above at index 0 pop() - remove from end, return value delete(index) - delete item at index, shifting all trailing elements left remove(item) - looks for value and removes index holding it (even if in multiple places) find(item) - looks for value and returns first index with that value, -1 if not found resize(new_capacity) // private function when you reach capacity, resize to double the size when popping an item, if size is 1/4 of capacity, resize to half Time O(1) to add/remove at end (amortized for allocations for more space), index, or update O(n) to insert/remove elsewhere Space contiguous in memory, so proximity helps performance space needed = (array capacity, which is \u0026gt;= n) * size of item, but even if 2n, still O(n) Linked Lists Description: Singly Linked Lists (video) CS 61B - Linked Lists 1 (video) CS 61B - Linked Lists 2 (video) [Review] Linked lists in 4 minutes (video) C Code (video) - not the whole video, just portions about Node struct and memory allocation Linked List vs Arrays: Core Linked Lists Vs Arrays (video) In The Real World Linked Lists Vs Arrays (video) Why you should avoid linked lists (video) Gotcha: you need pointer to pointer knowledge: (for when you pass a pointer to a function that may change the address where that pointer points) This page is just to get a grasp on ptr to ptr. I don\u0026rsquo;t recommend this list traversal style. Readability and maintainability suffer due to cleverness. Pointers to Pointers Implement (I did with tail pointer \u0026amp; without): size() - returns number of data elements in list empty() - bool returns true if empty value_at(index) - returns the value of the nth item (starting at 0 for first) push_front(value) - adds an item to the front of the list pop_front() - remove front item and return its value push_back(value) - adds an item at the end pop_back() - removes end item and returns its value front() - get value of front item back() - get value of end item insert(index, value) - insert value at index, so current item at that index is pointed to by new item at index erase(index) - removes node at given index value_n_from_end(n) - returns the value of the node at nth position from the end of the list reverse() - reverses the list remove_value(value) - removes the first item in the list with this value Doubly-linked List Description (video) No need to implement Stack Stacks (video) [Review] Stacks in 3 minutes (video) Will not implement. Implementing with array is trivial Queue Queue (video) Circular buffer/FIFO [Review] Queues in 3 minutes (video) Implement using linked-list, with tail pointer: enqueue(value) - adds value at position at tail dequeue() - returns value and removes least recently added element (front) empty() Implement using fixed-sized array: enqueue(value) - adds item at end of available storage dequeue() - returns value and removes least recently added element empty() full() Cost: a bad implementation using linked list where you enqueue at head and dequeue at tail would be O(n) because you\u0026rsquo;d need the next to last element, causing a full traversal each dequeue enqueue: O(1) (amortized, linked list and array [probing]) dequeue: O(1) (linked list and array) empty: O(1) (linked list and array) Hash table Videos:\nHashing with Chaining (video) Table Doubling, Karp-Rabin (video) Open Addressing, Cryptographic Hashing (video) PyCon 2010: The Mighty Dictionary (video) PyCon 2017: The Dictionary Even Mightier (video) (Advanced) Randomization: Universal \u0026amp; Perfect Hashing (video) (Advanced) Perfect hashing (video) [Review] Hash tables in 4 minutes (video) Online Courses:\nCore Hash Tables (video) Data Structures (video) Phone Book Problem (video) distributed hash tables: Instant Uploads And Storage Optimization In Dropbox (video) Distributed Hash Tables (video) Implement with array using linear probing\nhash(k, m) - m is size of hash table add(key, value) - if key already exists, update value exists(key) get(key) remove(key) More Knowledge Binary search Binary Search (video) Binary Search (video) detail blueprint [Review] Binary search in 4 minutes (video) Implement: binary search (on sorted array of integers) binary search using recursion Bitwise operations Bits cheat sheet - you should know many of the powers of 2 from (2^1 to 2^16 and 2^32) Get a really good understanding of manipulating bits with: \u0026amp;, |, ^, ~, \u0026raquo;, \u0026laquo; words Good intro: Bit Manipulation (video) C Programming Tutorial 2-10: Bitwise Operators (video) Bit Manipulation Bitwise Operation Bithacks The Bit Twiddler The Bit Twiddler Interactive Bit Hacks (video) Practice Operations 2s and 1s complement Binary: Plusses \u0026amp; Minuses (Why We Use Two\u0026rsquo;s Complement) (video) 1s Complement 2s Complement Count set bits 4 ways to count bits in a byte (video) Count Bits How To Count The Number Of Set Bits In a 32 Bit Integer Swap values: Swap Absolute value: Absolute Integer Trees Trees - Intro Intro to Trees (video) Tree Traversal (video) BFS(breadth-first search) and DFS(depth-first search) (video) BFS notes: level order (BFS, using queue) time complexity: O(n) space complexity: best: O(1), worst: O(n/2)=O(n) DFS notes: time complexity: O(n) space complexity: best: O(log n) - avg. height of tree worst: O(n) inorder (DFS: left, self, right) postorder (DFS: left, right, self) preorder (DFS: self, left, right) [Review] Breadth-first search in 4 minutes (video) [Review] Depth-first search in 4 minutes (video) [Review] Tree Traversal (playlist) in 11 minutes (video) Binary search trees: BSTs Binary Search Tree Review (video) Introduction (video) MIT (video) C/C++: Binary search tree - Implementation in C/C++ (video) BST implementation - memory allocation in stack and heap (video) Find min and max element in a binary search tree (video) Find height of a binary tree (video) Binary tree traversal - breadth-first and depth-first strategies (video) Binary tree: Level Order Traversal (video) Binary tree traversal: Preorder, Inorder, Postorder (video) Check if a binary tree is binary search tree or not (video) Delete a node from Binary Search Tree (video) Inorder Successor in a binary search tree (video) Implement: insert // insert value into tree get_node_count // get count of values stored print_values // prints the values in the tree, from min to max delete_tree is_in_tree // returns true if given value exists in the tree get_height // returns the height in nodes (single node\u0026rsquo;s height is 1) get_min // returns the minimum value stored in the tree get_max // returns the maximum value stored in the tree is_binary_search_tree delete_value get_successor // returns next-highest value in tree after given value, -1 if none Heap / Priority Queue / Binary Heap visualized as a tree, but is usually linear in storage (array, linked list) Heap Introduction (video) Binary Trees (video) Tree Height Remark (video) Basic Operations (video) Complete Binary Trees (video) Pseudocode (video) Heap Sort - jumps to start (video) Heap Sort (video) Building a heap (video) MIT: Heaps and Heap Sort (video) CS 61B Lecture 24: Priority Queues (video) Linear Time BuildHeap (max-heap) [Review] Heap (playlist) in 13 minutes (video) Implement a max-heap: insert sift_up - needed for insert get_max - returns the max item, without removing it get_size() - return number of elements stored is_empty() - returns true if heap contains no elements extract_max - returns the max item, removing it sift_down - needed for extract_max remove(x) - removes item at index x heapify - create a heap from an array of elements, needed for heap_sort heap_sort() - take an unsorted array and turn it into a sorted array in-place using a max heap or min heap Sorting Notes:\nImplement sorts \u0026amp; know best case/worst case, average complexity of each: no bubble sort - it\u0026rsquo;s terrible - O(n^2), except when n \u0026lt;= 16 Stability in sorting algorithms (\u0026ldquo;Is Quicksort stable?\u0026rdquo;) Sorting Algorithm Stability Stability In Sorting Algorithms Stability In Sorting Algorithms Sorting Algorithms - Stability Which algorithms can be used on linked lists? Which on arrays? Which on both? I wouldn\u0026rsquo;t recommend sorting a linked list, but merge sort is doable. Merge Sort For Linked List For heapsort, see Heap data structure above. Heap sort is great, but not stable\nSedgewick - Mergesort (5 videos)\n1. Mergesort 2. Bottom up Mergesort 3. Sorting Complexity 4. Comparators 5. Stability Sedgewick - Quicksort (4 videos)\n1. Quicksort 2. Selection 3. Duplicate Keys 4. System Sorts UC Berkeley:\nCS 61B Lecture 29: Sorting I (video) CS 61B Lecture 30: Sorting II (video) CS 61B Lecture 32: Sorting III (video) CS 61B Lecture 33: Sorting V (video) CS 61B 2014-04-21: Radix Sort(video) Bubble Sort (video)\nAnalyzing Bubble Sort (video)\nInsertion Sort, Merge Sort (video)\nInsertion Sort (video)\nMerge Sort (video)\nQuicksort (video)\nSelection Sort (video)\nMerge sort code:\nUsing output array (C) Using output array (Python) In-place (C++) Quick sort code:\nImplementation (C) Implementation (C) Implementation (Python) [Review] Sorting (playlist) in 18 minutes\nQuick sort in 4 minutes (video) Heap sort in 4 minutes (video) Merge sort in 3 minutes (video) Bubble sort in 2 minutes (video) Selection sort in 3 minutes (video) Insertion sort in 2 minutes (video) Implement:\nMergesort: O(n log n) average and worst case Quicksort O(n log n) average case Selection sort and insertion sort are both O(n^2) average and worst case For heapsort, see Heap data structure above Not required, but I recommended them:\nSedgewick - Radix Sorts (6 videos) 1. Strings in Java 2. Key Indexed Counting 3. Least Significant Digit First String Radix Sort 4. Most Significant Digit First String Radix Sort 5. 3 Way Radix Quicksort 6. Suffix Arrays Radix Sort Radix Sort (video) Radix Sort, Counting Sort (linear time given constraints) (video) Randomization: Matrix Multiply, Quicksort, Freivalds\u0026rsquo; algorithm (video) Sorting in Linear Time (video) As a summary, here is a visual representation of 15 sorting algorithms. If you need more detail on this subject, see \u0026ldquo;Sorting\u0026rdquo; section in Additional Detail on Some Subjects\nGraphs Graphs can be used to represent many problems in computer science, so this section is long, like trees and sorting were.\nNotes:\nThere are 4 basic ways to represent a graph in memory: objects and pointers adjacency matrix adjacency list adjacency map Familiarize yourself with each representation and its pros \u0026amp; cons BFS and DFS - know their computational complexity, their trade offs, and how to implement them in real code When asked a question, look for a graph-based solution first, then move on if none MIT(videos):\nBreadth-First Search Depth-First Search Skiena Lectures - great intro:\nCSE373 2020 - Lecture 10 - Graph Data Structures (video) CSE373 2020 - Lecture 11 - Graph Traversal (video) CSE373 2020 - Lecture 12 - Depth First Search (video) CSE373 2020 - Lecture 13 - Minimum Spanning Trees (video) CSE373 2020 - Lecture 14 - Minimum Spanning Trees (con\u0026rsquo;t) (video) CSE373 2020 - Lecture 15 - Graph Algorithms (con\u0026rsquo;t 2) (video) Graphs (review and more):\n6.006 Single-Source Shortest Paths Problem (video) 6.006 Dijkstra (video) 6.006 Bellman-Ford (video) 6.006 Speeding Up Dijkstra (video) Aduni: Graph Algorithms I - Topological Sorting, Minimum Spanning Trees, Prim\u0026rsquo;s Algorithm - Lecture 6 (video) Aduni: Graph Algorithms II - DFS, BFS, Kruskal\u0026rsquo;s Algorithm, Union Find Data Structure - Lecture 7 (video) Aduni: Graph Algorithms III: Shortest Path - Lecture 8 (video) Aduni: Graph Alg. IV: Intro to geometric algorithms - Lecture 9 (video) CS 61B 2014: Weighted graphs (video) Greedy Algorithms: Minimum Spanning Tree (video) Strongly Connected Components Kosaraju\u0026rsquo;s Algorithm Graph Algorithm (video) [Review] Shortest Path Algorithms (playlist) in 16 minutes (video) [Review] Minimum Spanning Trees (playlist) in 4 minutes (video) Full Coursera Course:\nAlgorithms on Graphs (video) I\u0026rsquo;ll implement:\nDFS with adjacency list (recursive) DFS with adjacency list (iterative with stack) DFS with adjacency matrix (recursive) DFS with adjacency matrix (iterative with stack) BFS with adjacency list BFS with adjacency matrix single-source shortest path (Dijkstra) minimum spanning tree DFS-based algorithms (see Aduni videos above): check for cycle (needed for topological sort, since we\u0026rsquo;ll check for cycle before starting) topological sort count connected components in a graph list strongly connected components check for bipartite graph Even More Knowledge Recursion Stanford lectures on recursion \u0026amp; backtracking: Lecture 8 | Programming Abstractions (video) Lecture 9 | Programming Abstractions (video) Lecture 10 | Programming Abstractions (video) Lecture 11 | Programming Abstractions (video) When it is appropriate to use it? How is tail recursion better than not? What Is Tail Recursion Why Is It So Bad? Tail Recursion (video) 5 Simple Steps for Solving Any Recursive Problem(video) Backtracking Blueprint: Java Python\nDynamic Programming You probably won\u0026rsquo;t see any dynamic programming problems in your interview, but it\u0026rsquo;s worth being able to recognize a problem as being a candidate for dynamic programming. This subject can be pretty difficult, as each DP soluble problem must be defined as a recursion relation, and coming up with it can be tricky. I suggest looking at many examples of DP problems until you have a solid understanding of the pattern involved. Videos: Skiena: CSE373 2020 - Lecture 19 - Introduction to Dynamic Programming (video) Skiena: CSE373 2020 - Lecture 20 - Edit Distance (video) Skiena: CSE373 2020 - Lecture 20 - Edit Distance (continued) (video) Skiena: CSE373 2020 - Lecture 21 - Dynamic Programming (video) Skiena: CSE373 2020 - Lecture 21 - Dynamic Programming and Review (video) Simonson: Dynamic Programming 0 (starts at 59:18) (video) Simonson: Dynamic Programming I - Lecture 11 (video) Simonson: Dynamic programming II - Lecture 12 (video) List of individual DP problems (each is short): Dynamic Programming (video) Yale Lecture notes: Dynamic Programming Coursera: The RNA secondary structure problem (video) A dynamic programming algorithm (video) Illustrating the DP algorithm (video) Running time of the DP algorithm (video) DP vs. recursive implementation (video) Global pairwise sequence alignment (video) Local pairwise sequence alignment (video) Design patterns Quick UML review (video) Learn these patterns: strategy singleton adapter prototype decorator visitor factory, abstract factory facade observer proxy delegate command state memento iterator composite flyweight Series of videos (27 videos) Book: Head First Design Patterns I know the canonical book is \u0026ldquo;Design Patterns: Elements of Reusable Object-Oriented Software\u0026rdquo;, but Head First is great for beginners to OO. Handy reference: 101 Design Patterns \u0026amp; Tips for Developers Combinatorics (n choose k) \u0026amp; Probability Math Skills: How to find Factorial, Permutation and Combination (Choose) (video) Make School: Probability (video) Make School: More Probability and Markov Chains (video) Khan Academy: Course layout: Basic Theoretical Probability Just the videos - 41 (each are simple and each are short): Probability Explained (video) NP, NP-Complete and Approximation Algorithms Know about the most famous classes of NP-complete problems, such as traveling salesman and the knapsack problem, and be able to recognize them when an interviewer asks you them in disguise. Know what NP-complete means. Computational Complexity (video) Simonson: Greedy Algs. II \u0026amp; Intro to NP Completeness (video) NP Completeness II \u0026amp; Reductions (video) NP Completeness III (Video) NP Completeness IV (video) Skiena: CSE373 2020 - Lecture 23 - NP-Completeness (video) CSE373 2020 - Lecture 24 - Satisfiability (video) CSE373 2020 - Lecture 25 - More NP-Completeness (video) CSE373 2020 - Lecture 26 - NP-Completeness Challenge (video) Complexity: P, NP, NP-completeness, Reductions (video) Complexity: Approximation Algorithms (video) Complexity: Fixed-Parameter Algorithms (video) Peter Norvig discusses near-optimal solutions to traveling salesman problem: Jupyter Notebook Pages 1048 - 1140 in CLRS if you have it. How computers process a program How CPU executes a program (video) How computers calculate - ALU (video) Registers and RAM (video) The Central Processing Unit (CPU) (video) Instructions and Programs (video) Caches LRU cache: The Magic of LRU Cache (100 Days of Google Dev) (video) Implementing LRU (video) LeetCode - 146 LRU Cache (C++) (video) CPU cache: MIT 6.004 L15: The Memory Hierarchy (video) MIT 6.004 L16: Cache Issues (video) Processes and Threads Computer Science 162 - Operating Systems (25 videos): for processes and threads see videos 1-11 Operating Systems and System Programming (video) What Is The Difference Between A Process And A Thread? Covers: Processes, Threads, Concurrency issues Difference between processes and threads Processes Threads Locks Mutexes Semaphores Monitors How they work? Deadlock Livelock CPU activity, interrupts, context switching Modern concurrency constructs with multicore processors Paging, segmentation and virtual memory (video) Interrupts (video) Process resource needs (memory: code, static storage, stack, heap, and also file descriptors, i/o) Thread resource needs (shares above (minus stack) with other threads in the same process but each has its own pc, stack counter, registers, and stack) Forking is really copy on write (read-only) until the new process writes to memory, then it does a full copy. Context switching How context switching is initiated by the operating system and underlying hardware? threads in C++ (series - 10 videos) CS 377 Spring \u0026lsquo;14: Operating Systems from University of Massachusetts concurrency in Python (videos): Short series on threads Python Threads Understanding the Python GIL (2010) reference David Beazley - Python Concurrency From the Ground Up: LIVE! - PyCon 2015 Keynote David Beazley - Topics of Interest (Python Asyncio) Mutex in Python Testing To cover: how unit testing works what are mock objects what is integration testing what is dependency injection Agile Software Testing with James Bach (video) Open Lecture by James Bach on Software Testing (video) Steve Freeman - Test-Driven Development (that’s not what we meant) (video) slides Dependency injection: video Tao Of Testing How to write tests String searching \u0026amp; manipulations Sedgewick - Suffix Arrays (video) Sedgewick - Substring Search (videos) 1. Introduction to Substring Search 2. Brute-Force Substring Search 3. Knuth-Morris Pratt 4. Boyer-Moore 5. Rabin-Karp Search pattern in text (video) If you need more detail on this subject, see \u0026ldquo;String Matching\u0026rdquo; section in Additional Detail on Some Subjects.\nTries Note there are different kinds of tries. Some have prefixes, some don\u0026rsquo;t, and some use string instead of bits to track the path I read through code, but will not implement Sedgewick - Tries (3 videos) 1. R Way Tries 2. Ternary Search Tries 3. Character Based Operations Notes on Data Structures and Programming Techniques Short course videos: Introduction To Tries (video) Performance Of Tries (video) Implementing A Trie (video) The Trie: A Neglected Data Structure TopCoder - Using Tries Stanford Lecture (real world use case) (video) MIT, Advanced Data Structures, Strings (can get pretty obscure about halfway through) (video) Floating Point Numbers simple 8-bit: Representation of Floating Point Numbers - 1 (video - there is an error in calculations - see video description) Unicode The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets What Every Programmer Absolutely, Positively Needs To Know About Encodings And Character Sets To Work With Text Endianness Big And Little Endian Big Endian Vs Little Endian (video) Big And Little Endian Inside/Out (video) Very technical talk for kernel devs. Don\u0026rsquo;t worry if most is over your head. The first half is enough. Networking If you have networking experience or want to be a reliability engineer or operations engineer, expect questions Otherwise, this is just good to know Khan Academy UDP and TCP: Comparison of Transport Protocols (video) TCP/IP and the OSI Model Explained! (video) Packet Transmission across the Internet. Networking \u0026amp; TCP/IP tutorial. (video) HTTP (video) SSL and HTTPS (video) SSL/TLS (video) HTTP 2.0 (video) Video Series (21 videos) (video) Subnetting Demystified - Part 5 CIDR Notation (video) Sockets: Java - Sockets - Introduction (video) Socket Programming (video) Final Review This section will have shorter videos that you can watch pretty quickly to review most of the important concepts. It\u0026rsquo;s nice if you want a refresher often.\nSeries of 2-3 minutes short subject videos (23 videos)\nVideos Series of 2-5 minutes short subject videos - Michael Sambol (38 videos):\nVideos Sedgewick Videos - Algorithms I\nSedgewick Videos - Algorithms II\nUpdate Your Resume See Resume prep information in the books: \u0026ldquo;Cracking The Coding Interview\u0026rdquo; and \u0026ldquo;Programming Interviews Exposed\u0026rdquo; I don\u0026rsquo;t know how important this is (you can do your own research) but here is an article on making your resume ATS Compliant: How to Create or Check if your Resume is ATS Compliant \u0026ldquo;This Is What A GOOD Resume Should Look Like\u0026rdquo; by Gayle McDowell (author of Cracking the Coding Interview), Note by the author: \u0026ldquo;This is for a US-focused resume. CVs for India and other countries have different expectations, although many of the points will be the same.\u0026rdquo; \u0026ldquo;Step-by-step resume guide\u0026rdquo; by Tech Interview Handbook Detailed guide on how to set up your resume from scratch, write effective resume content, optimize it, and test your resume Find a Job Sites for Finding Jobs Interview Process \u0026amp; General Interview Prep How to Pass the Engineering Interview in 2021 Demystifying Tech Recruiting How to Get a Job at the Big 4: How to Get a Job at the Big 4 - Amazon, Facebook, Google \u0026amp; Microsoft (video) How to Get a Job at the Big 4.1 (Follow-up video) Cracking The Coding Interview Set 1: Gayle L McDowell - Cracking The Coding Interview (video) Cracking the Coding Interview with Author Gayle Laakmann McDowell (video) Cracking the Facebook Coding Interview: The Approach Problem Walkthrough Prep Courses: Software Engineer Interview Unleashed (paid course): Learn how to make yourself ready for software engineer interviews from a former Google interviewer. Python for Data Structures, Algorithms, and Interviews (paid course): A Python centric interview prep course which covers data structures, algorithms, mock interviews and much more. Intro to Data Structures and Algorithms using Python (Udacity free course): A free Python centric data structures and algorithms course. Data Structures and Algorithms Nanodegree! (Udacity paid Nanodegree): Get hands-on practice with over 100 data structures and algorithm exercises and guidance from a dedicated mentor to help prepare you for interviews and on-the-job scenarios. Grokking the Behavioral Interview (Educative free course): Many times, it’s not your technical competency that holds you back from landing your dream job, it’s how you perform on the behavioral interview. Mock Interviews:\nGainlo.co: Mock interviewers from big companies - I used this and it helped me relax for the phone screen and on-site interview Pramp: Mock interviews from/with peers - peer-to-peer model of practice interviews interviewing.io: Practice mock interview with senior engineers - anonymous algorithmic/systems design interviews with senior engineers from FAANG anonymously Be thinking of for when the interview comes Think of about 20 interview questions you\u0026rsquo;ll get, along with the lines of the items below. Have at least one answer for each. Have a story, not just data, about something you accomplished.\nWhy do you want this job?\nWhat\u0026rsquo;s a tough problem you\u0026rsquo;ve solved?\nBiggest challenges faced?\nBest/worst designs seen?\nIdeas for improving an existing product\nHow do you work best, as an individual and as part of a team?\nWhich of your skills or experiences would be assets in the role and why?\nWhat did you most enjoy at [job x / project y]?\nWhat was the biggest challenge you faced at [job x / project y]?\nWhat was the hardest bug you faced at [job x / project y]?\nWhat did you learn at [job x / project y]?\nWhat would you have done better at [job x / project y]?\nIf you find it hard to come up with good answers of these types of interview questions, here are some ideas:\nGeneral Interview Questions and their Answers Have questions for the interviewer Some of mine (I already may know the answers, but want their opinion or team perspective):\nHow large is your team? What does your dev cycle look like? Do you do waterfall/sprints/agile? Are rushes to deadlines common? Or is there flexibility? How are decisions made in your team? How many meetings do you have per week? Do you feel your work environment helps you concentrate? What are you working on? What do you like about it? What is the work life like? How is the work/life balance? Once You\u0026rsquo;ve Got The Job Congratulations!\nKeep learning.\nYou\u0026rsquo;re never really done.\nEverything below this point is optional. It is NOT needed for an entry-level interview. However, by studying these, you\u0026rsquo;ll get greater exposure to more CS concepts, and will be better prepared for any software engineering job. You\u0026rsquo;ll be a much more well-rounded software engineer.\nAdditional Books 📚 These are here so you can dive into a topic you find interesting.\nThe Unix Programming Environment An oldie but a goodie The Linux Command Line: A Complete Introduction A modern option TCP/IP Illustrated Series Head First Design Patterns A gentle introduction to design patterns Design Patterns: Elements of Reusable Object-Oriente​d Software AKA the \u0026ldquo;Gang Of Four\u0026rdquo; book, or GOF The canonical design patterns book Algorithm Design Manual (Skiena) As a review and problem recognition The algorithm catalog portion is well beyond the scope of difficulty you\u0026rsquo;ll get in an interview This book has 2 parts: Class textbook on data structures and algorithms Pros: Is a good review as any algorithms textbook would be Nice stories from his experiences solving problems in industry and academia Code examples in C Cons: Can be as dense or impenetrable as CLRS, and in some cases, CLRS may be a better alternative for some subjects Chapters 7, 8, 9 can be painful to try to follow, as some items are not explained well or require more brain than I have Don\u0026rsquo;t get me wrong: I like Skiena, his teaching style, and mannerisms, but I may not be Stony Brook material Algorithm catalog: This is the real reason you buy this book. This book is better as an algorithm reference, and not something you read cover to cover. Can rent it on Kindle Answers: Solutions Errata Write Great Code: Volume 1: Understanding the Machine The book was published in 2004, and is somewhat outdated, but it\u0026rsquo;s a terrific resource for understanding a computer in brief The author invented HLA, so take mentions and examples in HLA with a grain of salt. Not widely used, but decent examples of what assembly looks like These chapters are worth the read to give you a nice foundation: Chapter 2 - Numeric Representation Chapter 3 - Binary Arithmetic and Bit Operations Chapter 4 - Floating-Point Representation Chapter 5 - Character Representation Chapter 6 - Memory Organization and Access Chapter 7 - Composite Data Types and Memory Objects Chapter 9 - CPU Architecture Chapter 10 - Instruction Set Architecture Chapter 11 - Memory Architecture and Organization Introduction to Algorithms Important: Reading this book will only have limited value. This book is a great review of algorithms and data structures, but won\u0026rsquo;t teach you how to write good code. You have to be able to code a decent solution efficiently AKA CLR, sometimes CLRS, because Stein was late to the game Computer Architecture, Sixth Edition: A Quantitative Approach For a richer, more up-to-date (2017), but longer treatment System Design, Scalability, Data Handling You can expect system design questions if you have 4+ years of experience.\nScalability and System Design are very large topics with many topics and resources, since there is a lot to consider when designing a software/hardware system that can scale. Expect to spend quite a bit of time on this Considerations: Scalability Distill large data sets to single values Transform one data set to another Handling obscenely large amounts of data System design features sets interfaces class hierarchies designing a system under certain constraints simplicity and robustness tradeoffs performance analysis and optimization START HERE: The System Design Primer System Design from HiredInTech How Do I Prepare To Answer Design Questions In A Technical Interview? 8 Things You Need to Know Before a System Design Interview Database Normalization - 1NF, 2NF, 3NF and 4NF (video) System Design Interview - There are a lot of resources in this one. Look through the articles and examples. I put some of them below How to ace a systems design interview Numbers Everyone Should Know How long does it take to make a context switch? Transactions Across Datacenters (video) A plain English introduction to CAP Theorem MIT 6.824: Distributed Systems, Spring 2020 (20 videos) Consensus Algorithms: Paxos - Paxos Agreement - Computerphile (video) Raft - An Introduction to the Raft Distributed Consensus Algorithm (video) Easy-to-read paper Infographic Consistent Hashing NoSQL Patterns Scalability: You don\u0026rsquo;t need all of these. Just pick a few that interest you. Great overview (video) Short series: Clones Database Cache Asynchronism Scalable Web Architecture and Distributed Systems Fallacies of Distributed Computing Explained Jeff Dean - Building Software Systems At Google and Lessons Learned (video) Introduction to Architecting Systems for Scale Scaling mobile games to a global audience using App Engine and Cloud Datastore (video) How Google Does Planet-Scale Engineering for Planet-Scale Infra (video) The Importance of Algorithms Sharding Engineering for the Long Game - Astrid Atkinson Keynote(video) 7 Years Of YouTube Scalability Lessons In 30 Minutes video How PayPal Scaled To Billions Of Transactions Daily Using Just 8VMs How to Remove Duplicates in Large Datasets A look inside Etsy\u0026rsquo;s scale and engineering culture with Jon Cowie (video) What Led Amazon to its Own Microservices Architecture To Compress Or Not To Compress, That Was Uber\u0026rsquo;s Question When Should Approximate Query Processing Be Used? Google\u0026rsquo;s Transition From Single Datacenter, To Failover, To A Native Multihomed Architecture The Image Optimization Technology That Serves Millions Of Requests Per Day A Patreon Architecture Short Tinder: How Does One Of The Largest Recommendation Engines Decide Who You\u0026rsquo;ll See Next? Design Of A Modern Cache Live Video Streaming At Facebook Scale A Beginner\u0026rsquo;s Guide To Scaling To 11 Million+ Users On Amazon\u0026rsquo;s AWS A 360 Degree View Of The Entire Netflix Stack Latency Is Everywhere And It Costs You Sales - How To Crush It What Powers Instagram: Hundreds of Instances, Dozens of Technologies Salesforce Architecture - How They Handle 1.3 Billion Transactions A Day ESPN\u0026rsquo;s Architecture At Scale - Operating At 100,000 Duh Nuh Nuhs Per Second See \u0026ldquo;Messaging, Serialization, and Queueing Systems\u0026rdquo; way below for info on some of the technologies that can glue services together Twitter: O\u0026rsquo;Reilly MySQL CE 2011: Jeremy Cole, \u0026ldquo;Big and Small Data at @Twitter\u0026rdquo; (video) Timelines at Scale For even more, see \u0026ldquo;Mining Massive Datasets\u0026rdquo; video series in the Video Series section Practicing the system design process: Here are some ideas to try working through on paper, each with some documentation on how it was handled in the real world: review: The System Design Primer System Design from HiredInTech cheat sheet flow: Understand the problem and scope: Define the use cases, with interviewer\u0026rsquo;s help Suggest additional features Remove items that interviewer deems out of scope Assume high availability is required, add as a use case Think about constraints: Ask how many requests per month Ask how many requests per second (they may volunteer it or make you do the math) Estimate reads vs. writes percentage Keep 80/20 rule in mind when estimating How much data written per second Total storage required over 5 years How much data read per second Abstract design: Layers (service, data, caching) Infrastructure: load balancing, messaging Rough overview of any key algorithm that drives the service Consider bottlenecks and determine solutions Exercises: Design a random unique ID generation system Design a key-value database Design a picture sharing system Design a recommendation system Design a URL-shortener system: copied from above Design a cache system Additional Learning I added them to help you become a well-rounded software engineer, and to be aware of certain technologies and algorithms, so you\u0026rsquo;ll have a bigger toolbox.\nCompilers How a Compiler Works in ~1 minute (video) Harvard CS50 - Compilers (video) C++ (video) Understanding Compiler Optimization (C++) (video) Emacs and vi(m) Familiarize yourself with a unix-based code editor vi(m): Editing With vim 01 - Installation, Setup, and The Modes (video) VIM Adventures set of 4 videos: The vi/vim editor - Lesson 1 The vi/vim editor - Lesson 2 The vi/vim editor - Lesson 3 The vi/vim editor - Lesson 4 Using Vi Instead of Emacs emacs: Basics Emacs Tutorial (video) set of 3 (videos): Emacs Tutorial (Beginners) -Part 1- File commands, cut/copy/paste, cursor commands Emacs Tutorial (Beginners) -Part 2- Buffer management, search, M-x grep and rgrep modes Emacs Tutorial (Beginners) -Part 3- Expressions, Statements, ~/.emacs file and packages Evil Mode: Or, How I Learned to Stop Worrying and Love Emacs (video) Writing C Programs With Emacs The Absolute Beginner\u0026rsquo;s Guide to Emacs (video by David Wilson) The Absolute Beginner\u0026rsquo;s Guide to Emacs (notes by David Wilson) Unix command line tools I filled in the list below from good tools. bash cat grep sed awk curl or wget sort tr uniq strace tcpdump Information theory (videos) Khan Academy More about Markov processes: Core Markov Text Generation Core Implementing Markov Text Generation Project = Markov Text Generation Walk Through See more in MIT 6.050J Information and Entropy series below Parity \u0026amp; Hamming Code (videos) Intro Parity Hamming Code: Error detection Error correction Error Checking Entropy Also see videos below Make sure to watch information theory videos first Information Theory, Claude Shannon, Entropy, Redundancy, Data Compression \u0026amp; Bits (video) Cryptography Also see videos below Make sure to watch information theory videos first Khan Academy Series Cryptography: Hash Functions Cryptography: Encryption Compression Make sure to watch information theory videos first Computerphile (videos): Compression Entropy in Compression Upside Down Trees (Huffman Trees) EXTRA BITS/TRITS - Huffman Trees Elegant Compression in Text (The LZ 77 Method) Text Compression Meets Probabilities Compressor Head videos (optional) Google Developers Live: GZIP is not enough! Computer Security MIT (23 videos) Introduction, Threat Models Control Hijacking Attacks Buffer Overflow Exploits and Defenses Privilege Separation Capabilities Sandboxing Native Code Web Security Model Securing Web Applications Symbolic Execution Network Security Network Protocols Side-Channel Attacks Garbage collection GC in Python (video) Deep Dive Java: Garbage Collection is Good! Deep Dive Python: Garbage Collection in CPython (video) Parallel Programming Coursera (Scala) Efficient Python for High Performance Parallel Computing (video) Messaging, Serialization, and Queueing Systems Thrift Tutorial Protocol Buffers Tutorials gRPC gRPC 101 for Java Developers (video) Redis Tutorial Amazon SQS (queue) Amazon SNS (pub-sub) RabbitMQ Get Started Celery First Steps With Celery ZeroMQ Intro - Read The Manual ActiveMQ Kafka MessagePack Avro A* A Search Algorithm A* Pathfinding (E01: algorithm explanation) (video) Fast Fourier Transform An Interactive Guide To The Fourier Transform What is a Fourier transform? What is it used for? What is the Fourier Transform? (video) Divide \u0026amp; Conquer: FFT (video) Understanding The FFT Bloom Filter Given a Bloom filter with m bits and k hashing functions, both insertion and membership testing are O(k) Bloom Filters (video) Bloom Filters | Mining of Massive Datasets | Stanford University (video) Tutorial How To Write A Bloom Filter App HyperLogLog How To Count A Billion Distinct Objects Using Only 1.5KB Of Memory Locality-Sensitive Hashing Used to determine the similarity of documents The opposite of MD5 or SHA which are used to determine if 2 documents/strings are exactly the same Simhashing (hopefully) made simple van Emde Boas Trees Divide \u0026amp; Conquer: van Emde Boas Trees (video) MIT Lecture Notes Augmented Data Structures CS 61B Lecture 39: Augmenting Data Structures Balanced search trees Know at least one type of balanced binary tree (and know how it\u0026rsquo;s implemented):\n\u0026ldquo;Among balanced search trees, AVL and 2/3 trees are now passé, and red-black trees seem to be more popular. A particularly interesting self-organizing data structure is the splay tree, which uses rotations to move any accessed key to the root.\u0026rdquo; - Skiena\nOf these, I chose to implement a splay tree. From what I\u0026rsquo;ve read, you won\u0026rsquo;t implement a balanced search tree in your interview. But I wanted exposure to coding one up and let\u0026rsquo;s face it, splay trees are the bee\u0026rsquo;s knees. I did read a lot of red-black tree code\nSplay tree: insert, search, delete functions If you end up implementing red/black tree try just these: Search and insertion functions, skipping delete I want to learn more about B-Tree since it\u0026rsquo;s used so widely with very large data sets\nSelf-balancing binary search tree\nAVL trees\nIn practice: From what I can tell, these aren\u0026rsquo;t used much in practice, but I could see where they would be: The AVL tree is another structure supporting O(log n) search, insertion, and removal. It is more rigidly balanced than red–black trees, leading to slower insertion and removal but faster retrieval. This makes it attractive for data structures that may be built once and loaded without reconstruction, such as language dictionaries (or program dictionaries, such as the opcodes of an assembler or interpreter) MIT AVL Trees / AVL Sort (video) AVL Trees (video) AVL Tree Implementation (video) Split And Merge Splay trees\nIn practice: Splay trees are typically used in the implementation of caches, memory allocators, routers, garbage collectors, data compression, ropes (replacement of string used for long text strings), in Windows NT (in the virtual memory, networking and file system code) etc CS 61B: Splay Trees (video) MIT Lecture: Splay Trees: Gets very mathy, but watch the last 10 minutes for sure. Video Red/black trees\nThese are a translation of a 2-3 tree (see below). In practice: Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in computational geometry can be based on red–black trees, and the Completely Fair Scheduler used in current Linux kernels uses red–black trees. In the version 8 of Java, the Collection HashMap has been modified such that instead of using a LinkedList to store identical elements with poor hashcodes, a Red-Black tree is used Aduni - Algorithms - Lecture 4 (link jumps to starting point) (video) Aduni - Algorithms - Lecture 5 (video) Red-Black Tree An Introduction To Binary Search And Red Black Tree [Review] Red-Black Trees (playlist) in 30 minutes (video) 2-3 search trees\nIn practice: 2-3 trees have faster inserts at the expense of slower searches (since height is more compared to AVL trees). You would use 2-3 tree very rarely because its implementation involves different types of nodes. Instead, people use Red Black trees. 23-Tree Intuition and Definition (video) Binary View of 23-Tree 2-3 Trees (student recitation) (video) 2-3-4 Trees (aka 2-4 trees)\nIn practice: For every 2-4 tree, there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2-4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2-4 trees just before red–black trees, even though 2-4 trees are not often used in practice. CS 61B Lecture 26: Balanced Search Trees (video) Bottom Up 234-Trees (video) Top Down 234-Trees (video) N-ary (K-ary, M-ary) trees\nnote: the N or K is the branching factor (max branches) binary trees are a 2-ary tree, with branching factor = 2 2-3 trees are 3-ary K-Ary Tree B-Trees\nFun fact: it\u0026rsquo;s a mystery, but the B could stand for Boeing, Balanced, or Bayer (co-inventor). In Practice: B-Trees are widely used in databases. Most modern filesystems use B-trees (or Variants). In addition to its use in databases, the B-tree is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block i address into a disk block (or perhaps to a cylinder-head-sector) address B-Tree B-Tree Datastructure Introduction to B-Trees (video) B-Tree Definition and Insertion (video) B-Tree Deletion (video) MIT 6.851 - Memory Hierarchy Models (video) - covers cache-oblivious B-Trees, very interesting data structures - the first 37 minutes are very technical, may be skipped (B is block size, cache line size) [Review] B-Trees (playlist) in 26 minutes (video) k-D Trees Great for finding number of points in a rectangle or higher dimension object A good fit for k-nearest neighbors kNN K-d tree algorithm (video) Skip lists \u0026ldquo;These are somewhat of a cult data structure\u0026rdquo; - Skiena Randomization: Skip Lists (video) For animations and a little more detail Network Flows Ford-Fulkerson in 5 minutes — Step by step example (video) Ford-Fulkerson Algorithm (video) Network Flows (video) Disjoint Sets \u0026amp; Union Find UCB 61B - Disjoint Sets; Sorting \u0026amp; selection (video) Sedgewick Algorithms - Union-Find (6 videos) Math for Fast Processing Integer Arithmetic, Karatsuba Multiplication (video) The Chinese Remainder Theorem (used in cryptography) (video) Treap Combination of a binary search tree and a heap Treap Data Structures: Treaps explained (video) Applications in set operations Linear Programming (videos) Linear Programming Finding minimum cost Finding maximum value Solve Linear Equations with Python - Simplex Algorithm Geometry, Convex hull (videos) Graph Alg. IV: Intro to geometric algorithms - Lecture 9 Geometric Algorithms: Graham \u0026amp; Jarvis - Lecture 10 Divide \u0026amp; Conquer: Convex Hull, Median Finding Discrete math Computer Science 70, 001 - Spring 2015 - Discrete Mathematics and Probability Theory Discrete Mathematics by Shai Simonson (19 videos) Discrete Mathematics By IIT Ropar NPTEL Additional Detail on Some Subjects I added these to reinforce some ideas already presented above, but didn\u0026rsquo;t want to include them above because it\u0026rsquo;s just too much. It\u0026rsquo;s easy to overdo it on a subject. You want to get hired in this century, right?\nSOLID\nBob Martin SOLID Principles of Object Oriented and Agile Design (video) S - Single Responsibility Principle | Single responsibility to each Object more flavor O - Open/Closed Principle | On production level Objects are ready for extension but not for modification more flavor L - Liskov Substitution Principle | Base Class and Derived class follow ‘IS A’ Principle more flavor I - Interface segregation principle | clients should not be forced to implement interfaces they don\u0026rsquo;t use Interface Segregation Principle in 5 minutes (video) more flavor D -Dependency Inversion principle | Reduce the dependency In composition of objects. Why Is The Dependency Inversion Principle And Why Is It Important more flavor Union-Find\nOverview Naive Implementation Trees Union By Rank Path Compression Analysis Options More Dynamic Programming (videos)\n6.006: Dynamic Programming I: Fibonacci, Shortest Paths 6.006: Dynamic Programming II: Text Justification, Blackjack 6.006: DP III: Parenthesization, Edit Distance, Knapsack 6.006: DP IV: Guitar Fingering, Tetris, Super Mario Bros. 6.046: Dynamic Programming \u0026amp; Advanced DP 6.046: Dynamic Programming: All-Pairs Shortest Paths 6.046: Dynamic Programming (student recitation) Advanced Graph Processing (videos)\nSynchronous Distributed Algorithms: Symmetry-Breaking. Shortest-Paths Spanning Trees Asynchronous Distributed Algorithms: Shortest-Paths Spanning Trees MIT Probability (mathy, and go slowly, which is good for mathy things) (videos):\nMIT 6.042J - Probability Introduction MIT 6.042J - Conditional Probability MIT 6.042J - Independence MIT 6.042J - Random Variables MIT 6.042J - Expectation I MIT 6.042J - Expectation II MIT 6.042J - Large Deviations MIT 6.042J - Random Walks Simonson: Approximation Algorithms (video)\nString Matching\nRabin-Karp (videos): Rabin Karps Algorithm Precomputing Optimization: Implementation and Analysis Table Doubling, Karp-Rabin Rolling Hashes, Amortized Analysis Knuth-Morris-Pratt (KMP): TThe Knuth-Morris-Pratt (KMP) String Matching Algorithm Boyer–Moore string search algorithm Boyer-Moore String Search Algorithm Advanced String Searching Boyer-Moore-Horspool Algorithms (video) Coursera: Algorithms on Strings starts off great, but by the time it gets past KMP it gets more complicated than it needs to be nice explanation of tries can be skipped Sorting\nStanford lectures on sorting: Lecture 15 | Programming Abstractions (video) Lecture 16 | Programming Abstractions (video) Shai Simonson, Aduni.org: Algorithms - Sorting - Lecture 2 (video) Algorithms - Sorting II - Lecture 3 (video) Steven Skiena lectures on sorting: CSE373 2020 - Mergesort/Quicksort (video) CSE373 2020 - Linear Sorting (video) Video Series Sit back and enjoy.\nList of individual Dynamic Programming problems (each is short)\nx86 Architecture, Assembly, Applications (11 videos)\nMIT 18.06 Linear Algebra, Spring 2005 (35 videos)\nExcellent - MIT Calculus Revisited: Single Variable Calculus\nSkiena lectures from Algorithm Design Manual - CSE373 2020 - Analysis of Algorithms (26 videos)\nUC Berkeley 61B (Spring 2014): Data Structures (25 videos)\nUC Berkeley 61B (Fall 2006): Data Structures (39 videos)\nUC Berkeley 61C: Machine Structures (26 videos)\nOOSE: Software Dev Using UML and Java (21 videos)\nMIT 6.004: Computation Structures (49 videos)\nCarnegie Mellon - Computer Architecture Lectures (39 videos)\nMIT 6.006: Intro to Algorithms (47 videos)\nMIT 6.033: Computer System Engineering (22 videos)\nMIT 6.034 Artificial Intelligence, Fall 2010 (30 videos)\nMIT 6.042J: Mathematics for Computer Science, Fall 2010 (25 videos)\nMIT 6.046: Design and Analysis of Algorithms (34 videos)\nMIT 6.824: Distributed Systems, Spring 2020 (20 videos)\nMIT 6.851: Advanced Data Structures (22 videos)\nMIT 6.854: Advanced Algorithms, Spring 2016 (24 videos)\nHarvard COMPSCI 224: Advanced Algorithms (25 videos)\nMIT 6.858 Computer Systems Security, Fall 2014\nStanford: Programming Paradigms (27 videos)\nIntroduction to Cryptography by Christof Paar\nCourse Website along with Slides and Problem Sets Mining Massive Datasets - Stanford University (94 videos)\nGraph Theory by Sarada Herke (67 videos)\nComputer Science Courses Directory of Online CS Courses Directory of CS Courses (many with online lectures) Algorithms implementation Multiple Algorithms implementation by Princeton University Papers Love classic papers? 1978: Communicating Sequential Processes implemented in Go 2003: The Google File System replaced by Colossus in 2012 2004: MapReduce: Simplified Data Processing on Large Clusters mostly replaced by Cloud Dataflow? 2006: Bigtable: A Distributed Storage System for Structured Data 2006: The Chubby Lock Service for Loosely-Coupled Distributed Systems 2007: Dynamo: Amazon’s Highly Available Key-value Store The Dynamo paper kicked off the NoSQL revolution 2007: What Every Programmer Should Know About Memory (very long, and the author encourages skipping of some sections) 2012: AddressSanitizer: A Fast Address Sanity Checker: paper video 2013: Spanner: Google’s Globally-Distributed Database: paper video 2015: Continuous Pipelines at Google 2015: High-Availability at Massive Scale: Building Google’s Data Infrastructure for Ads 2015: How Developers Search for Code: A Case Study More papers: 1,000 papers ","permalink":"https://samirpaul1.github.io/blog/posts/complete-computer-science-study-plan-to-become-a-software-engineer/","summary":"Complete Computer Science Study Plan to Become a Software Engineer","title":"Complete Computer Science Study Plan to Become a Software Engineer"},{"content":"Chapter 1: Computer Networks and the Internet 1.1 What is the Internet? 1.1.1 A nuts-and-bolts description The Internet is a computer networks that interconnects hundreds of millions of computing devices through the world. Today not only computers and workstation are being connected to the network, therefore the term computer network may sound a bit dated.\nAll the devices connected to the Internet are called hosts or end systems. End systems are connected together by a network of communication links and packets switches.\nDifferent links can transmit data at different rates, with the transmission rate of a link measured in bits/second.\nWhen one end system has data to send to another end system, the sending end system segments the data and adds header bytes to each segment. The resulting packages of information, called packets, are then sent through the network to the destination and system where they a reassembled into the original data.\nA packet switch takes a packet arriving on one of its incoming communication links and forwards that packet on one of its outgoing communication links. The two most prominent types of packets switches are routers and link switches. The sequence of communication links and packet switches traversed by a packet from the sending end system to the receiving end system is known as route or path.\nEnd systems access the Internet through Internet Service Providers (ISPs), including residential ISPs (cable or phone company), corporate, university ISPs \u0026hellip; Each ISP in itself is a network of packet switches and communication links. Lower tier (which interconnect end-systems) ISPs are interconnected through national and international upper tier ISP. An upper-tier ISP consists of high speed routers interconnected with high-speed fiber-optic links. Each ISP network is managed independently.\nEnd systems, packet switches and other pieces of the Internet run protocols that control the sending and receiving of information within the Internet.\n1.1.2 A Services Description The Internet can be described as an infrastructure that provides services to applications. These applications (Web, social networks, VoIP\u0026hellip;) are said to be distributed since they involve multiple end systems that exchange data with each other. Internet applications run on end systems, not in the packet switches or routers, packet switches facilitate the exchange of data, but they are not concerned with the application that is the source or sink of data.\nEnd systems attached to the Internet provide and Application Programming Interface (API) that specifies how a program running on one end system asks the Internet infrastructure to deliver data to a specific destination program running on another end system.\n1.1.3 What Is a Protocol? All the activity in the Internet that involves two or more communicating remote entities is governed by a protocol.\nA protocol defines the format and the order of messages exchanged between two or more communicating entities, as weel as the actions taken on the trasmission and/or receipt of a message or other event\n1.2 The Network Edge Computers and other devices connected to the Internet are often referred to as end systems as they sit at the edge of the Internet. They are also called hosts as they host, run, applications programs such as a Web Browser or an email client.\nHosts are sometimes further divided into two categories: clients and servers. The former being desktop, mobile pcs, smartphones, the latter being powerful machines that store and distribute Web pages, streams\u0026hellip; Nowadays most of the servers reside in large data centers\n1.2.1 Access Networks They are the networks that physically connect end systems to the first router on a path from the end system to any other distant end system. Examples: mobile network, national or global ISP, local or regional ISP, home networks enterprise networks.\nHome Access: DSL, Cable, FITH, Dial-Up and Satellite Today, the two most prevalent types of broadband residential access are digital subscriber line (DSL) and cable.\nA residence typically obtains DSL access from the telephone company (telco) that provides its wired local phone access. The customer\u0026rsquo;s telco is therefore its ISP. DSL modem use the existing telephone lines to exchange data with DSLAMs (digital subscriber line access multiplexer) located in the telco local central office. The DSL modem takes digital data and translates it to high-frequency tones for transmission over telephone wires, these analog signals from many houses are translated back into digital format at the DSLAM. The use of different frequencies allows the phone line to carry a high-speed downstream channel, a medium-speed upstream channel and an ordinary two-way telephone channel. Hundreds or even thousands of households connect to a single DSLAM.\nDSL: 24 Mbps downstream and 2.5 Mbps upstream (MAX VALUES). Because of the difference between these two values, the access is said to be asymmetric.\nCable Internet access makes use of the cable television company\u0026rsquo;s existing cable television infrastructure. Cable modems connect to CMTS (Cablem Modem Termination System) which does the same job the DSLAM does for phone lines. The access is typically asymmetric. CABLE: 42.8 Mbps downstream and 30.7 Mbps upstream (MAX VALUES). Cable Internet access is a shared broadcast medium: each packet travels downstream on every link to every home and viceversa. For this, if several users are simultaneously using the downstream channel, the actual rate will be significantly lower.\nAnother up-and-coming technology that promises very high speeds is fiber to the home (FTTH). The concept is simple: provide an optical fiber path from the Central Office (CO)\nAccess in the Enterprise and the Home: Ethernet and WiFi On corporate and university campuses, and increasingly in home settings, a Local Area Network (LAN) is used to connect an end system to the edge router. Ethernet is by far the most prevalent access technology is corporate, university and home networks. Ethernet uses twisted-pair copper wire to connect to an Ethernet switch which is connected into the larger Internet. The Internet is increasingly accessed wirelessly: wireless users transmit/receive packets to/from an access point connected into the enterprise\u0026rsquo;s network which in turn is connected to the wired Internet.\nWide-Area Wireless Access: 3G and LTE Smartphones and Tablets employ the same wireless infrastructure used for cellular telephony to send/receive packets through a base station operated by the cellular network provider. Third generation (3G) wireless and fourth generation (4G) of wide-area network are being deployed. LTE (\u0026ldquo;Long-Term Evolution\u0026rdquo;) has its root in 3G and can potentially achieve rates in excess of 10 Mbps.\n1.2.2 Physical Media The book talks about it in detail but we haven\u0026rsquo;t talked about it in class\nA bit, when traveling from source to destination, passes through a series of transmitter-receiver pairs, for each pair, the bit is sent by propagating electromagnetic waves or optical pulses across a physical medium. This can take many shapes and forms and doesn\u0026rsquo;t have to be of the same type for each transmitter-receiver pair along the path. Physical media fall into two categories:\nguided media: the waves are guided along a solid medium (fiber-optic cable, twisted-pair copper wire, coaxial cable) unguided media: the waves propagate in the atmosphere and in outer space (wireless LAN, digital satellite channel) 1.3 The Network Core 1.3.1 Packet Switching In a network application, end systems exchange messages with each other. To send a message from a source end system to a destination end system, the source breaks long messages into smaller chunks of data known as packets. Between source and destination, each packet travels through communication links and packet switches (for which there are two predominant types, routers and link-layer switches). Packets are transmitted over each communication link at a rate equal to the full transmission rate of the link. So, if a source end system or a packet switch is send a packet of L bits over a link with transmission rate R bits/sec, then the time to transmit the packet is L/R seconds.\nStore-and-forward Transmission Most packet switches use store-and-forward transmission at the inputs to the links. Store-and-forward transmission means that the packet switch must receive the entire packet before it can begin to transmit the first bit of the packet onto the outbound link. The link must buffer (\u0026ldquo;store\u0026rdquo;) the packet\u0026rsquo;s bits and only after the router has received all of the packet\u0026rsquo;s bits can it begin to transmit (\u0026ldquo;forward\u0026rdquo;) the packet onto the outbound link.\nQueuing Delays and Packet Loss Each packet switch has multiple links attached to it. For each attached link, the packet switch has an output buffer (or output queue) which stores packets that the router is about to send into that link. If an arriving packet needs to be transmitted onto a link but finds the link busy with the transmission of another packet, the arriving packet must wait in the output buffer. Thus, packets suffer output buffer queuing delays which are variable and depend on the level of congestion in the network. Since the amount of buffer space is finite, an arriving packet may find the buffer completely full. In this case, packet loss will occur, either the arriving packet or one of the already queued packets will be dropped.\nForwarding tables and routing protocols In the Internet, every end system has an address called an IP address. When a source end system wants to send a packet to a destination end system, the source includes the destination\u0026rsquo;s IP address in the packet\u0026rsquo;s header. Each router has a forwarding table that maps destination addresses (or portions of the destination addresses) to that router\u0026rsquo;s outbound links. When a packet arrives at the router, the router examines the address and searches its forwarding table, using this destination address, to find the appropriate outbound link. A number of special routing protocols are used to automatically set the forwarding tables.\n1.3.2 Circuit Switching In circuit-switched networks, the resources needed along a path(buffers, link transmission rate) to provide for communication between the end systems are reserved for the duration of the communication sessions. When two hosts want to communicate, the network establishes a dedicated end-to-end connection between them.\nMultiplexing in Circuit-Switched Networks A circuit in a link is implemented with either frequency-division multiplexing (FDM) or time-division multiplexing (TDM). With FDM, the frequency spectrum of a link is divided up among the connections established across the link. The width of the band is called the bandwidth. For a TDM link, time is divided into frames of fixed duration, and each frame is divided into a fixed number of time slots.\nPacket Switching Versus Circuit Switching Packet switching is more flexible, uses resources efficiently and is simpler to implement (even if it requires congestion control). Circuit switching offers performance guarantees but uses resources inefficiently\n1.3.3 A Network of Networks To create the Internet, ISPs must be interconnected, thus creating a network of networks. Much of the evolution of the structure of the Internet is driven by economics and national policy, rather than by performance consideration.\nToday\u0026rsquo;s Internet is complex, consisting of a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs are diverse in their coverage, with some spanning multiple continents and oceans, and others limited to narrow geographic regions. The lower-tier ISPs connect to the higher-tier ISPs and the higher-tier ISPs interconnect with one another. Users and content providers are customers of lower-tier ISPs and lower-tier ISPs are customers of higher-tier ISPs. Recently, major content providers (Google) have also created their own networks and connect directly into lower-tier ISPs where possible.\n1.4 Delay, Loss and Throughput in Packet-Switched Networks Computer networks necessarily constrain throughput (the amount of data per second that can be transferred) between end system, introduce delays between end systems and can actually lose packets.\n1.4.1 Overview of Delay in Packet-Switched networks As a packet travels from one node (host or router) to the subsequent host along his path, it suffers from several types of delays at each node along the path.\nTypes of Delay Processing Delay The processing delay consists of the time required to examine the packet\u0026rsquo;s header and determine where to direct the packet. It may also include other factors, such as the time needed to check for bit-level errors occurred during transmission. They typically are of the order of microseconds or less. After processing the packet is sent to the queue preceding the link to the next router\nQueuing Delay At the queue, the packet experiences a queuing delay as it waits to be transmitted onto the link. It depends on the number of earlier-arriving packets, therefore if the queue is empty, then the packet\u0026rsquo;s queuing delay will be 0. Typically of the order of microseconds or milliseconds.\nTransmission delays If the length of the packet is L bits, and the transmission rate of the link is R bits/sec, then the transmission delay is L/R. This is the amount of time required to push (transmit) all of the packet\u0026rsquo;s bits into the link. Typically on the order of microseconds to milliseconds.\nPropagation Delay The time required to propagate a bit from the beginning of the link to the next router is the propagation delay. The bit propagates at the propagation speed of the link, which depends on the physical medium of the link. The propagation delay is the distance between two routers divided by the propagation speed of the link.\nTotal nodal delay it is the summation of the previous delays\n1.4.2 Queuing Delay and Packet Loss The queuing delay depends can vary from packet to packet, therefore when characterizing queuing delay, one typically uses statistical measures, such as average queuing delay, variance of queuing delay, and the probability that the queuing delay exceeds some specified value.\nPacket Loss A queue preceding a link has finite capacity. If a packet finds a full queue, then the router will drop it, the packet will be lost. The fraction of lost packets increases as the traffic intensity increases.\n1.4.3 End-to-End Delay Let\u0026rsquo;s now consider the total delay, from source to destination (not only the nodal delay). Let\u0026rsquo;s suppose there are N-1 routers between the source host and the destination host, then the nodal delays accumulate and give an end-to-end delay:\nd(end_end) = N * [d(proc) + d(queue) + d(trans) + d(prop)] 1.4.4 Throughput in Computer Networks Another critical performance measure in computer networks is end-to-end throughput. The instantaneous throughput at any instant of time is the rate (in bits/sec) at which host B is receiving a file. If the file consists of F bits and the transfers takes T seconds to transfer the whole file, then the average throughput of the file is F/T bits/sec. For a simple two-link network, the throughput is the min of all the throughputs, that is the transmission rate of the bottleneck link. Therefore, the constraining factor for throughput in today\u0026rsquo;s Internet is typically the access network.\n1.5 Protocol Layers and Their Service Models 1.5.1 Layered Architecture A layered architecture allows us to discuss a well-defined, specific part of a large and complex system. This simplification itself is of considerable value by providing modularity, making it much easier to change the implementation of the service provided by the layer: as long as the layer provides the same service to the layer above it, and uses the same services from the layer below it, the remainder of the system remains unchanged when a layer\u0026rsquo;s implementation is changed.\nProtocol Layering To provide structure to the design of network protocols, the network designers organize protocols in layers. Each protocol belongs to one of the layers. We are interested in the services that a layer offers to the layer above, service model of a layer. When taken together, the protocols of the various layers are called the protocol stack. The Internet protocol stack consists of five layers:\nApplication Transport Network Link Physical Application Layer Where network applications and their applications-layer protocols reside. The Internet\u0026rsquo;s application layer includes many protocols: HTTP, SMTP, FTP, DNS. An application-layer protocol is distributed over multiple end systems, with the application in one end system using the protocol to exchange packets of information with the application in another end system. This packet of information at the application layer is called message.\nTransport Layer It transports application-layer messages between application endpoints. In the Internet there are two transport protocols: TCP and UDP. TCP provides a connection-oriented service to its application: the service includes guaranteed delivery of application-layer messages to the destination and flow control unit. TCP also breaks long messages into shorter segments and provides a congestion-control mechanism, so that a source throttles its transmission rate when the network is congested. HTTP and SMTP use TCP\nUDP provides a connectionless service to its applications: it\u0026rsquo;s a no-frills service that provides no guarantees, no reliability, no flow control and no congestion control. A transport-layer packet is called segment Skype uses UDP (speed required)\nNetwork Layer It is responsible for moving network-layer packets known as datagrams from one host to another. The Internet\u0026rsquo;s network layer includes the IP Protocol. There is only one IP Protocol and all the Internet components that have a network layer must run it. The Internet\u0026rsquo;s network layer also contains routing protocols that determine the routes that datagrams take between sources and destinations. The Internet has many routing protocols. Often it is simply referred to as the IP protocols, forgetting that it includes routing too.\nLink Layer To move a packet from one node to the next, the network layer relies on the services of the link layer. The services provided by the link layer depend on the specific link-layer protocol that is employed over the link. Examples are Ethernet, WiFi. We will refer to the link-layer packets as frames\nPhysical Layer The job of the physical layer is to move the individual bits within the frame from one node to the next. The protocols are link dependent and further depend of the actual transmission medium of the link.\n1.5.2 Encapsulation Routers and link-layer switches are both packet switches but routers and link-layer switches do not implement all of the layers in the protocol stack: link-layer switches implement Physical and Link while router add the Network Layer too.\nFrom the Application Layer, the message passes to the transport layer, which appends additional information to it (the Header) that will be used by the receiver-side transport layer. The transport layer then adds its own header and passes the datagram to the link layer which adds it own link-layer header information. Thus, we see that at each layer, a packet has two types of fields: header fields and a payload field, the payload typically being the packet from the layer above.\nThe process of encapsulation can be more complex: for example a large message may be divided into multiple transport-layer segments, which will be divided into multiple datagrams\u0026hellip;.\n1.6 Networks Under Attack Malware Along with all the good files we exchange on the Internet, come malicious software, collectively known as malware that can also enter and infect our devices. Once a device infected, the malware can do all kinds of evil things: deleting files, install spyware\u0026hellip; A compromised host may also be enrolled in a network of thousands of similarly compromised devices, known as botnet which can be used for spam or distributed denial-of-service. Much of the malware is self-replicating: it seeks entry into other hosts from the infected machines. Malware can spread in the from of a virus or a worm.\nViruses are malware that requires some form of user interaction to infect the user\u0026rsquo;s device. Worms are malware that can enter a device without any explicit user interaction. DoS Denial-of-Service attacks render a network, host, or other piece of infrastructure unusable by legittimate users. Most of them fall into one of the three categories:\nVulnerability Attack: a few well-crafted messages are sent to a vulnerable application or operating system running on the targeted host. The service might stop or the host might crash. Bandwidth flooding: a deluge of packets is sent to the targeted host, so many packets that the target\u0026rsquo;s access link becomes clogged preventing legitimate packets from reaching the server Connection flooding: a large number of half-open or fully open TCP connections are established at the targeted host, which can become so bogged down that it stops accepting legitimate connections. In a distributed DoS (DDoS) attack the attacker controls multiple sources and has each source blast traffic at the target.\nSniffing A passive receiver can record a copy of every packet that passes through the network. It is then called a packet sniffer. Because packet sniffers are passive (they do not inject packets into the channel), they are difficult to detect. Some of the best defenses against packet sniffing involve cryptography.\nSpoofing The ability to inject packets into the Internet with a false source address is known as IP Spoofing and is but one of many ways in which one user can masquerade as another user. To solve this problem we will need end-point authentication.\nThe history of the Internet shaped is structure The Internet was originally designed to be based on the model of a group of mutually trusting users attached to a transparent network, a model in which there is no need for security. Many aspects of the original Internet architecture deeply reflect this notion of mutual trust, such as the ability for one to send a packet to any other user is the default rather than a requested/granted capability. However today\u0026rsquo;s Internet certainly does not involve \u0026ldquo;mutually trusted users\u0026rdquo;: communication among mutually trusted users is the exception rather the rule.\nHistory of Computer Networking and the Internet Chapter 2: Application Layer Network applications are the raison d\u0026rsquo;être of a computer network. They include text email, remote access to computers, file transfers, the WorldWideWeb (mid 90s), web searching, e-commerce, Twitter/Facebook, Amazon, Netflix, Youtube, WoW\u0026hellip;\n2.1 Principles of Network Applications At the core of network application development is writing programs that run on different end systems and communicate with each over the network. The programs running on end systems might be different (server-client architecture) or identical (Peer-to-Peer architecture). Importantly we write programs that run on end systems/hosts, not on network-core devices (routers/link-layer switches).\n2.1.1 Network Application Architectures From the application developer\u0026rsquo;s perspective, the network architecture is fixed and provides a specific set of services to applications. The application architecture, on the other hand, is chosen by him. In choosing the application architecture, a developer will likely draw one of the two predominant architectural paradigms used in modern network applications:\nClient-server architecture: there is an always on host, called the server which serves requests from many other hosts, called clients: [Web Browser and Web Server]. Clients do not communicate directly with each other. The server has a fixed, well-known address, called an IP address that clients use to connect to him. Often, a single server host is incapable of keeping up with all the requests from clients, for this reason, a data center, housing a large number of hosts, is often used to create a powerful virtual server (via proxyin). P2P architecture: there is minimal or no reliance on dedicated servers in data centers, the application exploits direct communication between pairs of intermittently connected bots, called peers. They are end systems owned and controlled by users. [Bittorrent, Skype]. P2P applications provide self-scalability (the network load is distributed) They are also cost-effective since they don\u0026rsquo;t require significant infrastructure and server bandwidth. P2P face challenges: ISP Friendly (asymmetric nature of residential ISPs) Security Incentives (convincing users to participate) Some applications have hybrid architectures, such as for many instant messaging applications: a server keeps track of the IP addresses of users, but user-to-user messages are sent directly between users.\n2.1.2 Processes Communicating In the jargon of operating systems, it\u0026rsquo;s not programs but processes that communicate. A process can be thought of as a program that is running within an end system. Processes on two different end systems communicate with each other by exchanging messages across the computer network: a sending process creates and sends messages into the network, a receiving process receives these messages and possibly responds by sending messages back.\nClient and Server Processes A network application consists of pairs of processes that send messages to each other over a network. For each pair of communicating processes we label:\nthe process that initiates the communication as the client [web browser] the process that waits to be contacted to begin the session as the server [web server] This labels stand even for P2P applications in the context of a communication session.\nThe Interface Between the Process and the Computer Network A process sends messages into, and receives messages from, the network through a software interface called a socket. A socket is the interface between the application layer and the transport layer within a host, it is also referred to as the Application Programming Interface (API) between the application and the network. The application developer has control of everything on the application-layer of the socket but has little control of the transport-layer side of the socket. The only control that he has over the transport-layer is:\nThe choice of the transport protocol Perhaps the ability to fix a few transport-layer parameters such as maximum buffer and maximum segment sizes Addressing Processes In order for a process running on one host to send packets to a process running on another host, the receiving process needs to have an address. To identify the receiving processes, two pieces of information need to be specified:\nThe address of the host. In the Internet, the host is identified by its IP Address, a 32-bit (or 64) quantity that identifies the host uniquely. An identifier that specifies the receiving process in the destination host: the destination port number. Popular applications have been assigned specific port numbers (web server -\u0026gt; 80) 2.1.3 Transport Services Available to Applications What are the services that a transport-layer protocol can offer to applications invoking it?\nReliable Data Transfer For many applications, such as email, file transfer, web document transfers and financial applications, packet\u0026rsquo;s drops and data loss can have devastating consequences. If a protocol provides guarantees that the data sent is delivered completely and correctly, it is said to provide reliable data transfer. The sending process can just pass its data into the socket and know with complete confidence that the data will arrive without errors at the receiving process.\nThroughput A transport-layer protocol could provide guaranteed available throughput at some specific rate. Applications that have throughput requirements are said to be bandwidth-sensitive applications.\nTiming A transport-layer protocol can also provide timing guarantees. Example: guarantees that every bit the sender pumps into the socket arrives at the receiver\u0026rsquo;s socket no more than 100 msec later, interesting for real-time applications such as telephony, virtual environments\u0026hellip;\nSecurity A transport-layer protocol can provide an application with one or more security services. It could encrypt all data transmitted by sending process and in the receiving host decrypt it.\n2.1.4 Transport Services Provided by the Internet The Internet makes two transport protocols available to applications: TCP and UDP.\nTCP Services TCP includes a connection-oriented service and a reliable data transfer service:\nConnection-oriented service: client and server exchange transport-layer control information before the application-level messages begin to flow. This so-called handshaking procedure alerts the client and server, allowing them to prepare for an onslaught of packets. Then a TCP connection is said to exist between the sockets of the two processes. When the application finishes sending messages, it must tear down the connection SECURING TCP Nether TCP nor UDP provide encryption. Therefore the Internet community has developed an enhancement for TCP called Secure Sockets Layer (SSL), which not only does everything that traditional TCP does but also provides critical process-to-process security services including encryption, data integrity and end-point authentication. It is not a third protocol, but an enhancement of TCP, the enhancement being implemented in the application layer in both the client and the server side of the application (highly optimized libraries exist). SSL has its own socket API, similar to the traditional one. Sending processes passes cleartext data to the SSL socket which encrypts it.\nReliable data transfer service The communicating processes can rely on TCP to deliver all data sent without error and in the proper order. TCP also includes a congestion-control mechanism, a service for the general welfare of the Internet rather than for the direct benefit of the communicating processes. It throttles a sending process when the network is congested between sender and receiver.\nUDP Services UDP is a no-frills, lightweight transport protocol, providing minimal services. It is connectionless, there\u0026rsquo;s no handshaking. The data transfer is unreliable: there are no guarantees that the message sent will ever reach the receiving process. Furthermore messages may arrive out of order. UDP does not provide a congestion-control mechanism neither.\nServices Not Provided by Internet Transport Protocols These two protocols do not provide timing or throughput guarantees, services not provided by today\u0026rsquo;s Internet transport protocols. We therefore design applications to cope, to the greatest extent possible, with this lack of guarantees.\n2.1.5 Application-Layer Protocols An application-layer protocol defines how an application\u0026rsquo;s processes, running on different end systems, pass messages to each other. It defines:\nThe type of the messages exchanged (request/response) The syntax of the various message types The semantics of the fields (meaning of the information in fields) The rules for determining whem and how a process sends messages and responds to messages 2.2 The Web and HTTP In the early 1990s, a major new application arrived on the scene: the World Wide Web (Berners-Lee 1994), the first application that caught the general public\u0026rsquo;s eye. The Web operates on demand: users receives what they want, when they want it. It is enormously easy for an individual to make information available over the web, hyperlinks and search engines help us navigate through the ocean of web sites\u0026hellip;\n2.2.1 Overview of HTTP The HyperText Transfer Protocol (HTTP), the Web\u0026rsquo;s application-layer protocol is a the heart of the Web. It is implemented in two programs: a client program and a server program. The two programs talk to each other by exchanging HTTP messages. A Web page (or document) consists of objects. An object is simply a file (HTML file, jpeg image\u0026hellip;) that is addressable by a single URL. Most Web pages consist of a base HTML file and several referenced objects. The HTML file references the other objects in the page with the objects\u0026rsquo; URLs. Each URL has two components: the hostname of the server that houses the object and the object\u0026rsquo;s path name. Web Browsers implement the client side of HTTP. HTTP uses TCP as its underlying transport protocol. The server sends requested files to clients without storing any state information about the client: it is a stateless protocol\n2.2.2 Non-Persistent and Persistent Connections In many Internet applications, the client and server communicate for an extended period of time, depending on the application and on how the application is being used, the series of requests may be back-to-back, periodically at regular intervals or intermittently. When this is happening over TCP, the developer must take an important decision: should each request/response pair be sent over a separate TCP connection or should all of the requests and their corresponding responses be sent over the same TCP connection? In the former approach, the application is said to use non-persistent connections and in the latter it is said to use persistent connections By default HTTP uses non-persistent connections but can be configured to be use persistent connections. To estimate the amount of time that elapses when a client requests the base HTML file until the entire file is received by the client we define the round-trip time (RTT) which is the time it takes for a small packet to travel from client to server and then back to the client.\nHTTP with Non-Persistent Connections For the page and each object it contains, a TCP connection must be opened (handshake request, handshake answer), we therefore observe an addition RTT, and for each object we will have a request followed by the reply This model can be expensive on the server side: a new connection needs to be established for each requested object, for each connection a TCP buffer must be allocated along some memory to store TCP variables.\nHTTP with Persistent Connections The server leaves the TCP connection open after sending a response, subsequent requests and responses between the same client and server will be sent over the same connection. In particular an entire web page (text + objects) ca be sent over a single persistent TCP connection, multiple web pages residing on the same server can be sent from the server to the same client over a single persistent TCP connection. These requests can be make back-to-back without waiting for replies to pending requests (pipelining). When the server receives back-to-back requests, it sends the objects back-to-back. If connection isn\u0026rsquo;t used for a pre-decided amount of time, it will be closed.\n2.2.3 HTTP Message Format Two types of HTTP messages:\nHTTP Request Message GET /somedir/page.html HTTP/1.1 Host: www.someschool.edu Connection: close User-agent: Mozilla/5.0 Accept-language: fr Ordinary ASCII text First line: request line Other lines: header lines the first lines has 3 fields: method field, URL field, HTTP version field: method field possible values: GET, POST, HEAD, PUT, DELETE The majority of HTTP requests use the GET method, used to request an object.\nThe entity body (empty with GET) is used by the POST method, for example for filling out forms. The user is still requesting a Web page but the specific contents of the page depend on what the user entered into the form fields. When POST is used, the entity body contains what the user entered into the form fields. Requests can also be made with GET including the inputted data in the requested URL. The HEAD method is similar to GET, when a server receives it, it responds with an HTTP message but it leaves out the requested object. It is often used for debugging. PUT is often used in conjunction with web publishing tools, to allow users to upload an object to a specific path on the web servers. Finally, DELETE allows a user or application to delete an object on a web server.\nHTTP Response Message A typical HTTP response message:\nHTTP/1.1 200 OK Connection: close Date: ... Server: ... Last-Modified: ... Content-Length: ... Content-Type: text/html (data data data data data ...) Status line: protocol version, status code, corresponding status message six header lines: the connection will be closed after sending the message date and time when the response was created (when the server retrieves the object from the file system, insert object in the message, sends the response message) Type of the server / software Last modified: useful for object caching Content-Length: number of bytes in the object Content-Type entity body: contains the requested object itself (data) Some common status codes:\n200 OK: request succeeded, information returned 301 Moved Permanently: the object has moved, the new location is specified in the header of the response 400 Bad Request: generic error code, request not understood 404 Not Found: The requested document doesn\u0026rsquo;t exist on the server 505 HTTP Version Not Supported: The requested HTTP protocol version is not supported by the server 2.2.4 User-Server Interaction: Cookies An HTTP server is stateless in order to simplify server design and improves performances. A website can identify users using cookies. Cookie technology has 4 components:\nCookie header in HTTP response message Cookie header in HTTP request message Cookie file on the user\u0026rsquo;s end-system managed by the browser Back-end database at the Website User connects to website using cookies:\nServer creates a unique identification number and creates an entry in its back-end database indexed by the identification number -server responds to user\u0026rsquo;s browser including in the header: Set-cookie: identification number The browser will append to the cookie file the hostname of the server and the identification number header Each time the browser will request a page, it will consult the cookie file, extract the identification number for the site and put a cookie header line including the identification number The server can track the user\u0026rsquo;s activity: it knows exactly what pages, in which order and at what times that identification number has visited. This is also why cookies are controversial: a website can learn a lot about a user and sell this information to a third party.\nTherefore cookies can be used to create a user session layer on top of stateless HTTP.\n2.2.5 Web Caching A Web cache, also called proxy server is a network entity that satisfies HTTP requests on behalf of an origin Web server. It has its own disk storage and keeps copies of recently requested objects in this storage.\nThe browser establishes a TCP connection to the web cache, sending an HTTP request for the object to the Web cache. The web cache checks to see if it has a copy of the object stored locally. If yes, it will return it within an HTTP response message to the browser. If not, the Web cache opens a TCP connection to the origin server, which responds with the requested object. The Web caches receives the object, stores a copy in its storage and sends a copy, within an HTTP response message, to the browser over the existing TCP connection. Therefore a cache is both a server and a client at the same time. Usually caches are purchased and installed by ISPs. They can substantially reduce the response time for a client request and substantially reduce traffic on an institution\u0026rsquo;s access link to the Internet.\nThrough the use of Content Distribution Networks (CDNs) web caches are increasingly playing an important role in the Internet. A CDN installs many geographically distributed caches throughout the Internet, localizing much of the traffic.\n2.2.6 The Conditional GET Caches introduce a new problem: what if the copy of an object residing in the cache is stale? The conditional GET is used to verify that an object is up to date. An HTTP request message is a conditional get if\nthe request message uses the GET method the request message includes an If-modified-since: header line. A conditional get message is sent from the cache to server which responds only if the object has been modified.\n2.5 DNS - The Internet\u0026rsquo;s Directory Service One identifier for a host is its hostname [cnn.com, www.yahoo.com]. Hostnames are mnemonic and therefore used by humans. Hosts are also identified by IP addresses.\n2.5.1 Services provided by DNS Routers and use IP addresses. The Internet\u0026rsquo;s domain name system (DNS) translates hostnames to IP addresses. The DNS is:\nA distributed database implemented in a hierarchy of DNS Servers An application-layer protocol that allows hosts to query the distributed database. DNS servers are often UNIX machines running the Berkeley Internet Name Domaine (BIND) software.\nDNS runs over UDP and uses port 53 It is often employed by other application-layer protocols (HTTP, FTP\u0026hellip;) to translate user-supplied hostnames to IP addresses.\nHow it works:\nThe user machine runs the client side of the DNS application The browser extracts www. xxxxx . xxx from the URL and passes the hostname to the client side of the DNS application The DNS sends a query containing the hostname to a DNS server The DNS client eventually receives a reply including the IP address for the hostname The browser can initiate a TCP connection. DNS adds an additional delay\nDNS provides other services in addition to translating hostnames to IP addresses:\nhost aliasing: a host with a complicated hostname can have more alias names. The original one is said to be a canonical hostname. mail server aliasing: to make email servers\u0026rsquo; hostnames more mnemonic. This also allows for an e-mail server and an Web server to have the same hostname. load distribution: replicated servers can have the same hostname. In this case, a set of IP addresses is associated with one canonical hostname. When a client make a DNS query for a name mapped to a set of addresses, the server responds with the entire set, but rotates the ordering within each reply. 2.5.2 Overview of How DNS Works From the perspective of the invoking application in the user\u0026rsquo;s host, DNS is a black box providing a simple, straightforward translation service. Having one single global DNS server would be simple, but it\u0026rsquo;s not realistic because it would a single point of failure, it would have an impossible traffic volume, it would be geographically too distant from some querying clients, its maintenance would be impossible.\nA Distributed, Hierarchical Database The DNS uses a large number of servers, organized in a hierarchical fashion and distributed around the world.\nThe three classes of DNS servers:\nRoot DNS servers: In the Internet there are 13 root DNS servers, most hosted in North America, each of these is in reality a network of replicated servers, for both security and reliability purposes (total: 247) Top-level domain (TLD) servers: responsible for top-level domains such as com org net edu and govand all of the country top-level domains uk fr jp Authoritative DNS servers: every organization with publicly accessible hosts must provide publicly accessible DNS records that map the names of those hosts to IP addresses. An organization can choose to implement its own authoritative DNS server or to pay to have the records stored in an authoritative DNS of some service provider. Finally there are local DNS servers which is central to the DNS architecture. They are hosted by ISPs. When a hosts connects to one of these, the local DNS server provides the host with the IP addresses of one or more of its local DNS servers. Requests can ho up to the root DNS servers and back down.\nWe can have both recursive and iterative queries. In recursive queries the user sends the request its nearest DNS which will ask to a higher-tier server, which will ask to lower order\u0026hellip; the chain goes on until it reaches a DNS that can reply, the reply will follow the inverse path that the request had. In iterative queries the same machine sends requests and receives replies. Any DNS can be iterative or recursive or both.\nDNS Caching DNS extensively exploits DNS caching in order to improve the delay performance and to reduce the number of DNS messages ricocheting around the Internet. In a query chain, when a DNS receives a DNS reply it can cache the mapping in its local memory.\n2.5.3 DNS Records and Messages The DNS servers that implement the DNS distributed database store resource records (RRs) including RRs that provide hostname-to-IP address mappings. Each DNS reply messages carries one or more resource records.\nA resource record is a four-tuple that contains the fields: (Name, Value, Type, TTL) TTL is the time to live of the resource record (when a resource should be removed from a cache). The meaning of Name and Value depend on Type:\nType Name Value A a hostname IP address NS a domain (foo.com) hostname of an authoritative DNS server which knows how to obtain the IP addresses for hosts in the domain. Used to route queries further along in the query chain CNAME a alias name canonical hostname for the name in Name MX alias hostname canonical hostname of a mail server that has an alias hostname Name DNS Messages The only types of DNS messages are DNS queries and reply messages. They have the same format:\nfirst 12 bytes in the header section: 16-bit number identifying the query, which will be copied into the reply query so that the client can match received replies with sent queries. 1 bit query/reply flag (0 query, 1 reply). 1 bit flag authoritative flag set in reply messages when DNS server is an authoritative for a queried name. 1 bit recursion flag if the client desires that the server performs recursion when it doesn\u0026rsquo;t have a record, 1 bit recursion-available field is set in the reply if the DNS server supports recursion question section: information about the query: name field containing the name being queried, type field answer section: resource records for the name originally queried: Type, Value, TTL. Multiple RRs can be returned if the server has multiple IP addresses authority section: records for other authoritative servers. additional section: other helpful records: canonical hostnames\u0026hellip; Inserting Records into the DNS Database We created a new company. Next we register th domain name newcompany.com at a registrar. A registrar is a commercial entity that verifies the uniqueness of the domain name, enters it into the DNS database and collects a small fee for these services. When we register the address, we need the provide the registrar with the IP address of our primary and secondary authoritative DNS servers, that will make sure that a Type NS and a Type A records are entered into the TLD com servers for our two DNS servers.\nFocus on security: DNS vulnerabilities DDoS bandwidth-flooding attack MITM: the mitm answers queries with false replies tricking the user into connecting to another server. The DNS infrastructure can be used to launch a DDoS attack against a targeted host To date, there hasn\u0026rsquo;t been an attack that that has successfully impeded the DNS service, DNS has demonstrated itself to be surprisingly robust against attacks. However there have been successful reflector attacks, these can be addressed by appropriate configuration of DNS servers.\n2.6 Peer-to-Peer Applications 2.6.1 File Distribution In P2P file distribution, each peer can redistribute any portion of the file it has received to any peers, thereby assisting the server in the distribution process. As of 2012 the most popular P2P file distribution protocol is BitTorrent, developed by Bram Cohen.\nScalability of P2P architectures Denote the upload rate of the server\u0026rsquo;s access link by $u_s$, the upload rate of the ith peer\u0026rsquo;s access link by $u_i$ and the download rate of the ith access link by $d_i$, tthe size of the to be distributed in bits () Comparison client-server and P2P.\nClient-Server The server must transmit one copy of the file to N peers, thus it transmits *NF *bits. The time to distribute the file is at least NF/u_s. Denote $d_min = min{ d_i }$ the link with the slowest download rate cannot obtain all F bits in less than $F/d_min$ seconds Therefore: $$ D_{cs} \\geq \\max \\left{ \\frac{NF}{u_s} , \\frac{F}{d_min} \\right} $$\nP2P When a peer receives some file data, it can use its own upload capacity to redistribute the data to other peers.\nAt the beginning of the distribution only the server has the file. It must send all the bits at least once. $D \\geq F/u_s$ The peer with the lowest download rate cannot obtain all F bits of the file in less than $F/d_min $ seconds. The total upload capacity of the system is equal to the summation of the upload rates of the server and of all the peers. The system must upload F bits to N peers, thus delivering a total of NF bits which can\u0026rsquo;t be done faster that $u_total$. We obtain: $$ D_{P2P} = \\max \\left{ \\frac{F}{u_s} , \\frac{F}{d_{min}} , \\frac{NF}{u_s + \\sum_{i=1}^N u_j} \\right} $$\nBitTorrent In BitTorrent the collection of all peers participating in the distribution of a particular file is called a torrent. Peers in a torrent download equal-size chunks of the file from one another with a typical chunk size of 256 KBytes. At the beginning a peer has no chunks, it accumulates more and more chunks over time. While it downloads chunks it also uploads chunks to other peers. Once a peer has acquired the entire file it may leave the torrent or remain in it and continue to upload chunks to other peers (becoming a seeder). Any peer can leave the torrent at any time and later rejoin it at anytime as well.\nEach torrent has an infrastructure node called a tracker: when a peer joins a torrent, it registers itself with the tracker and periodically informs it that it is still in the torrent. The tracker keeps track of the peers participating in the torrent. A torrent can have up to thousands of peers participating at any instant of time.\nUser joins the torrent, the tracker randomly selects a subset of peers from the set of participating peers. User establishes concurrent TCP connections with all of these peers, called neighboring peers. The neighboring peers can change over time. The user will ask each of his neighboring peers for the list of chunks they have (one list per neighbor). The user starts downloading the chunks that have the fewest repeated copies among the neighbors (rares first technique). In this manner the rarest chunks get more quickly redistributed, roughly equalizing the numbers of copies of each chunk in the torrent.\nEvery 10 seconds the user measures the rate at which she receives bits and determines the four peers that are sending to her at the highest rate. It then reciprocates by sending chunks to these same four peers. The four peers are called unchocked. Every 30 seconds it also choses one additional neighbor and sends it chunks. These peers are called optmistically unchocked.\n2.6.2 Distributed Hash Tables (DHTs) How to implement a simple database in a P2P network? In the P2P system each peer will only hold a small subset of the totality of the (key, value) pairs. Any peer can query the distributed database with a particular key, the database will locate the peers that have the corresponding pair and return the pair to querying peer. Any peer can also insert a new pair in the databse. Such a distributed database is referred to as a distributed hash table (DHT). In a P2P file sharing application a DHT can be used to store the chunks associated to the IP of the peer in possession of them.\nAn approach: Let\u0026rsquo;s assign an identifier to each peer, where the identifier is an integer in the range [0, 2^n -1] for some fixed n. Such an identifier can be expressed by a n-bit representation. A hash function is used to transform non-integer values into integer values. We suppose that this function is available to all peers. How to assign keys to peers? We assign each (key,value) pair to the peer whose identifier is the closest to key, which is the identifier defined as the closest successor of the key. To avoid having each peer keeping track of all other peers (scalability issue) we use\nCircular DHT If we organize peers into a circle, each peer only keeps track of its immediate successor and predecessor (modulo 2^n). This circular arrangement of peers is a special case of an overlay network: the peers form an abstract logical network which resides above the \u0026ldquo;underlay\u0026rdquo; computer network, the overlay links are not physical but virtual liaisons between pairs of peers. A single overlay link typically uses many physical links and physical routers in the underlying network.\nIn the circle a peer asks \u0026ldquo;who is responsible for key k?\u0026rdquo; and it sends the message clockwise around the circle. Whenever a peer receives such message, it knows the identifier of its predecessor and predecessor, it can determine whether it is responsible (closest to) for the key in question. If not, it passes the message to its successor. When the message reaches the peer responsible for the key, it can send a message back to the querying peer indicating that it is responsible for that key. Using this system N/2* messages are sent on average (N = number of peers). In designing a DHT there is always a tradeoff between the number of neighbors for each peer and the number of DHT messages needed to resolve a single query. (1 message if each peer keeps track of all other peers; N/2 messages if each knows only 2 neighbors). To improve our circular DHT we could add shortcuts so that each peer not only keeps track of its immediate successor and predecessor but also of relatively small number of shortcut peers scattered around the circle. How many shortcut neighbors? Studies show that DHT can be designed so that the number of neighbors per peer as well as the number of messages per query is O(log *N*) (N the number of peers).\nPeer Churn In a P2P system, a peer can come or go without warning. To keep the DHT overlay in place in presence of a such peer churn we require each peer to keep track (know to IP address) of its predecessor and successor, and to periodically verify that its two successors are alive. If a peer abruptly leaves, its successor and predecessor need to update their information. The predecessor replaces its first successor with its second successor and ask it for the identifier and IP address of its immediate successor.\nWhat if a peer joins? If it only knows one peer, it will ask him what will be his predecessor and successor. The message will reach the predecessor which will send the new arrived its predecessor and successor information. The new arrived can join the DHT making its predecessor successor its own successor and by notifying its predecessor to change its successor information.\n2.7 Socket Programming: Creating Network Applications Only code explication \u0026mdash;-\u0026gt; skipping\nChapter 3: Transport Layer 3.1 Introduction and Transport-Layer Services A transport-layer protocol provides for logical communication (as if the hosts running the processes were directly connected) between application processes running on different hosts. Application processes use the logical communication provided by the transport layer to send messages to each other, free from the worry of the details of the physical infrastructure used. Transport-layer protocols are implemented in the end systems but not in network routers. On the sending side, the transport layer converts the application messages into transport-layer packets, known as transport-layer segments. This is done by breaking them into smaller chunks and adding a transport-layer header to each chunk. The transport-layer then passes the segment to the network-layer packet at the sending end-system. On the receiving side, the network layer extracts the transport-layer segment from the datagram and passes the segment up to the transport-layer which then processes the received segment, making the data in the segment available to the received application.\n3.1.1 Relationship Between Transport and Network Layers A transport-layer protocol provides logical communication between processes running on different hosts. Whereas a network-layer protocol provides logical communication between hosts.\n3.1.2 Overview of the Transport Layer in the Internet A TCP/IP network (such as the Internet) makes two distinct transport-layer protocols available to the application layer:\nUDP [ User Datagram Protocol], which provides an unreliable, connectionless service to the invoking application TCP [Transmission Control Protocol] which provides a reliable, connection-oriented service to the invoking application. We need to spend a few words on the network-layer protocol: the Internet network-layer protocol is the IP (Internet Protocol). It provides a logical communication between hosts. The IP service model is a best-effort delivery service: it makes the best effort to deliver segments between hosts, but it doesnt provide guarantees:\nit doesn\u0026rsquo;t guarantee segment delivery it doesn\u0026rsquo;t guarantee orderly delivery of segments it doesn\u0026rsquo;t guarantee the integrity of the data in the segments Thus IP is said to be an unreliable service. Every host has at least one network-layer address a so-called IP address.\nUDP and TCP extend IP\u0026rsquo;s delivery service between 2 end systems to a delivery service between two processes running on the end systems. Extend host-to-host delivery to process-to-process delivery is called transport-layer multiplexing and demultiplexing. UDP provides process-to-process delivery and error checking services. Therefore it is an unreliable service. TCP provides reliable data transfer using flow control, sequence numbers, acknowledgements and timers. TCP thus converts IP\u0026rsquo;s unreliable service between end systems into a reliable data transport service between processes. TCP also provides congestion control, a service not really provided to the invoking application as it is to the Internet as a whole: it prevents any TCP connection from swamping the links and routers between communication hosts with an excessive amount of traffic giving each connection traversing a congested link an equal share of the bandwidth.\n3.2 Multiplexing and Demultiplexing Here we\u0026rsquo;ll cover multiplexing \u0026amp; demultiplexing in the context of the Internet but a multiplexing/demultiplexing service is needed for all computer networks.\nThe job of delivering the data in a transport-layer segment to the correct socket is called demultiplexing. The job of gathering data chunks at the source host from different sockets, encapsulating each data chunk with header information (which will be used in demultiplexing) to create segments and passing the segments to the networks layer is called multiplexing. Therefore sockets need to have unique identifiers and each segment needs to have special fields that indicate the socket to which the segment is delivered. These fields are the source port number field and the destination port number field. Each port number is a 16-bit number ranging from 0 to 65535. Port numbers ranging from 0 to 1023 are called well-known port numbers and are restricted, reserved for us by well-known application protocols such as HTTP (80) and FTP (21). Designing an application, we should assign it a port number.\nConnectionless Multiplexing and Demultiplexing A UDP socket is fully identified by the two-tuple: (destination IP address , destination port number) therefore if two UDP segments have different source IP address and/or source port numbers but have the same destination IP address and destination port number, than the two segments will be directed to the same destination process via the same destination socket. The source port number serves as part of the return address.\nConnection-oriented Multiplexing and Demultiplexing A TCP socket is identified by the four-tuple: (source IP address, source port number, destination IP address, destination port number) When a TCP segment arrives from the network to a host, the host uses all four values to demultiplex the segment to the appropriate socket. Two arriving TCP segments with different source IP addresses or source port numbers will (with the exception of a TCP carrying the original connection establishment request) be directed to two different sockets.\nRoutine:\nThe TCP server application always has a welcoming socket that waits for connection establishment requests from TCP clients on port number X The TCP client creates a socket and sends a connection establishment request (a TCP segment including destination port, source port number and a special connection-establishment bit set in the TCP header) The server OS receives the incoming connection-request segment on port X, it locates the server process that is waiting to accept a connection on port number X, then creates a new socket which will be identified by (source port number in the segment (cleint), IP address of source host (client), the destination port number in the segment (its own), its own IP address) With the TCP connection in place, client and server can now send data to each other The server may support many simultaneous TCP connection sockets, with each socket attached to a process and each socket identified by its own four-tuple. When a TCP segment arrives at the host, all the fours fields are used to demultiplex the segment to the appropriate socket.\nPort Scanning Can be used both by attackers and system administrator to find vulnerabilities in the target or to know network applications are running in the network. The most used port scanner is nmap free and open source. For TCP it scans port looking for port accepting connections, for UDP looking for UDP ports that respond to transmitted UDP segments. It then returns a list of open, closed or unreachable ports. A host running nmap can attempt to scan any target anywhere in the Internet\nWeb Servers and TCP In a web server, all segments have destination port 80 and both the initial connection-establishment segments and the segments carrying HTTP request messages will have destination port 80, the server will distinguish clients using the source IP addresses and port numbers. Moreover in today\u0026rsquo;s high-performing Web, servers often use only one process and create a new thread with a new connection soket for each new client connection.\nIf using persistent HTTP, client and server will exchange messages via the same server socket. If using non-persistent HTTP, a new TCP connection is created and closed for every request/response and hence a new socket is created and closed for every request/response.\n3.3 Connectionless Transport: UDP UDP does multiplexing/demultiplexing, light error checking, nothing more. If the developer chooses UDP, the application is almost directly talking with IP. Note that with UDP there is no handshaking between sending and receiving transport-layer entities before sending a segment. For this reason UDP is said to be connectionless. DNS is an example of an application layer protocol that typically uses UDP: there is no handshaking and when a client doesn\u0026rsquo;t receive a reply either it tries sending the query to another name server or it informs the invoking application that it can\u0026rsquo;t get a reply. Why should a developer choose UDP?\nFiner application-level controll over what data is sent and when: as soon as the application passes data to UDP, UDP will package the data inside a segment and immediately pass it to the network layer. TCP\u0026rsquo;s congestion control can delay the sending of the segment and will try sending the packet until this is received. In real time applications the sending rate is important, so we can trade off some data loss for some sending rate. No connection establishement UDP justs send data without any formal preliminaries without introducing any delay, probably the reason why DNS runs over UDP. No connection state: because a UDP application doesn\u0026rsquo;t need to keep track of the users or to keep connections alive, it can typically support many more active clients than a TCP application Small packet header overhead TCP has 20 bytes of header overhead in every segment versus the 8 of UDP It is possible for an application developer to have reliable data transfer when using UDP. This can be done if reliability is built into the application itself (eg adding acknowledgement and retransmission mechanisms) but it is a nontrivial task and may keep the developer busy for a long time.\n3.3.1 UDP Segment Structure The UDP header has only four fields, each consisting of two bytes:\nsource port number destination port number checksum (used for error detection.) length (which specifies the number of bytes in the UDP segment, header + data) This length field is needed since the size of the data field may differ from one UDP segment to the next.\n3.3.2 UDP Checksum Provides for error detection, to determine whether the bits in the segment have been altered as it moves from source to destination.\nAt the send side, UDP performs the 1s complement of the sum of all the 16-bit (max 64) words in the segment, with any overflow encountered during the sum being wrapped around. This result is put in the checksum field of the UDP segment header.\nUDP implements error detection according to the end-end principle: certain functionality (error detection in this case) must be implemented on an end-end basis: \u0026ldquo;functions placed at the lower levels may be redundant or of little value when compared to the cost of providing them at the higher level\u0026rdquo;.\n3.4 Principles of Reliable Data Transfer It is the responsibility of a realiable data transfer protocol to implement reliable data service: no transferred data bits are corrupted or lost and all are delivered in the order in which they were sent. We will consider the following actions:\nThe sending side of the data transfer protocol will be invoked from above by a call to rdt_send() On the receiving side rdt_rcv() will be called when a packet arrives while deliver_data() will be called when the rdt protocol wants to deliver data to the upper layer. We use the term packet rather than segment because the concepts explained here applies to computer networks in general. We will only consider the case of unidirectional data transfer that is data transfer from the sending to the receiving side. The case of reliable bidirectional (full-duplex) data transfer is not more difficult but more tedious to explain. Nonetheless sending and receiving side will need to transmit packets in both directions.\n3.4.1 Building a Reliable Data Transfer Protocol Finite-state machines (FSM) are boring! And unlikely to be asked at the exam, therefore I decided not to cover them here.\n3.4.2 Pipelined Reliable Data Transfer Protocols In today\u0026rsquo;s high-speed networks stop-and-wait protocols are simply not tolerable: we cannot send one packet and wait for the ACK and then send the second one, it is inefficient as we can see computing the utilization of the channel:\n$$ U = \\frac{L/R}{RTT+ L/R} $$\nThe solution is simple: rather than operate in a stop-and-wait manner, the sender is allowed to send multiple packets without waiting for acknowledgements. Since the many in-transit send-to-receiver packets can be visualized as filling a pipeline, this technique is known as pipelining.\nSome consequences:\nThe range of sequence numbers must be increased: each in-transit packet must have a unique sequence number Sender and receiver may have to buffer more than one packet. Two basic approaches toward pipelined error recovery can be identified: Go-Back-N and Selective Repeat\n3.4.3 Go-Back-N (GBN) The sender is allowed to send N packets (sender window size = N), the receiver has a window of size 1. If a segment from sender to receiver is lost, the receiver discards all the segments with sequence number greater than the sequence number of the dropped packet, answering with ACK with this sequence number. (no packet re-ordering) The sender will wait for ACK in order to move the window and send new packets. The wait is not infinite, after a certain time a timeout will occur and the sender will retransmit all the packets in the sending window. In a Go-Back-N protocol, acknowledgements are cumulative: if sender receives ACK3 he will know that all the packets from 0 to 3 have been received, even if hasn\u0026rsquo;t received ACK2.\n3.4.4 Selective Repeat When the window-size and bandwidth-delay product are both large, many packets can be in the pipeline and a single packet error can thus cause GBN to retransmit a large number of packets, many unnecessarily. Selective Repeat avoid unnecessary retransmissions by having the sender retransmit only those that packets it suspects were received in error at the receiver: individual acknowledgements (opposed to cumulative). sender window size = N and receiver window site = N. The sender has a timer for each packet in its window. When a timeout occurs, only the missing packet is resent. The receiver buffers out of order packets.\n3.5 Conncetion-Oriented Transport: TCP 3.5.1 The TCP Connection TCP is said to be connection-oriented because before one application process can begin to send data to another, the two processes must first \u0026ldquo;handshake\u0026rdquo; with each other. During the connection establishment, both sides of the connection will initialize many TCP state variables. TCP connection is not an end-to-end TDM or FDM circuit nor is it a virtual circuit as the connection state resides entirely in the two end systems and not in the intermediate network elements. A TCP connection provides a full-duplex service: when a connection between process A and process B, application layer data can flow from A to B and, at the same time, from B to A. TCP is also point-to-point: a connection is always between a single sender and a single receiver, no multicast possible.\nEstablishment of the connection: the client first sends a special TCP segment, the server responds with a second special TCP segment and the client answer again with a third special TCP segment. The first two cannot contain a payload while the third can. Three segments: three-way handshake. Both the sender and the receiver have buffers that are set up during the handshake. The maximum amount if data that can be grabbed and placed in a segment is limited by the maximum segment size (MSS). TCP therefore splits data into smaller chunks and pairs each chunk of client data with a TCP header thereby forming TCP segments which are passed down to the network layer. When TCP receives a segment at the other end, the segment\u0026rsquo;s data is placed in the TCP connection\u0026rsquo;s receive buffer. Each side of the connection has its own send buffer and its own receive buffer\n3.5.2 TCP Segment Structure 32 bit sequence number and acknowledgement number necessary for reliable data transmission 16 bit receive window used for flow control, indicates the number of bytes that a receiver is willing to accept 4 bit header length field. The TCP header can be of a variable length due to the TCP options field (usually empty therefore usual length is 20 bytes) options field used to negotiate MSS or as a window scaling factor for use in high speed networks. flag field: 6 bits: ACK used to indicate that the value carried in the acknowledgement field is valid, that is the segment contains an acknowledgement for a segment that has been successfully received. , 3. and 4. RST, SYN, FIN for connection setup and teardown PSH indicates that the receiver should pass the data to upper layer immediately URG indicates that there is data in the segment that the sending side upper layer has marked as urgent. Sequence Numbers and Acknowledgment Numbers TCP views data as an unstructured, but ordered, stream of bytes and TCP\u0026rsquo;s use of sequence numbers reflects this view: sequence numbers are over the stream of bytes and not over the series of transmitted segments. The sequence number for a segment is the byte-stream number of the first byte in the segment. EX 500,000 bytes, MSS = 1,000 bytes =\u0026gt; 500 segments are created. First is numbered 0, second 1000, third 2000\u0026hellip;..\nThe acknowledgement number that Host A puts in its segment is the sequence number of the next byte Host A is expecting from Host B. TCP is said to provide cumulative acknowledgements: if sender receives ACK 536 he will know that all the bytes from 0 to 535 have been well received. What does a host do when it receives out-of-order segments? The receiver buffers the out-of-order bytes and waits for the missing bytes to fill in the gaps. Usually both sides of a TCP connection randomly choose an initial sequence number randomly both for security and for minimizing the possibility that a segment that is still present in the network from an earlier, already terminated connection between two hosts is mistaken for a valid segment in a later connection between these same two hosts.\n3.5.3 Round-Trip Time Estimation and Timeout TCP uses a timeout/retransmit mechanism to recover from lost segments. The question rises: How long should the timeout intervals be? Clearly the timeout should be larger than the connection\u0026rsquo;s round-trip time? How much larger? How can the RTT be evaluated?\nEstimating the Round-Trip Time The sample RTT, SampleRTT, for a segment is the amount of time between when the segment is sent (passed to network layer) and when an acknowledgement for the segment is received. Most TCP implementations take one SampleRTT at a time: at any point in time, the SampleRTT is being estimated for only one of the transmitted but currently unacknowledged segments, leading to a new value of SampleRTT for approximatively every RTT. TCP never computes a SampleRTT for a segment that has been retransmitted, only for segments transmitted once. In order to estimate a typical RTT, TCP keeps an average called EstimatedRTT of the SampleRTT values. Upon obtaining a new SampleRTT TCP updates this estimation according to the formula:\nEstimatedRTT = (1 - a) * EstimatedRTT + a * SampleRTT\nwhere usually a = 1/8 = 0.125\nWe note that this weighted average puts more weight on recent samples than on old samples. In statistics such an average is called an exponential weighted moving average (EWMA). It is also useful to having an estimate of the variability of the RTT. We can measure how much SampleRTT typically deviates from EstimatedRTT:\nDevRTT = (1 - b) * DevRTT + b* | SampleRTT - EstimatedRTT |\nWe note that this is an EWMA of the difference of estimated and last measured RTT. The recommended value for b is b = 0.25\nSetting and Managing the Retransmission Timeout Interval TimeoutInterval = EstimatedRTT + 4 * DevRTT\nAn initial TimeoutInterval value of 1 second is recommended. Also when a timeout occurs, the value of TimeoutInterval is doubled in order to avoid a premature timeout occurring for a subsequent segment that will soon be acknowledged. As soon as a segment is received and EstimatedRTT is updated, the TimeoutInterval is again computed using the formula above.\n3.5.4 Reliable Data Transfer TCP creates a reliable data transfer service on top of IP\u0026rsquo;s unreliable best-effort service. It ensures that the data stream that a process reads out of its TCP receive buffer is uncorrupted, without gaps, without duplication and in sequence. We supposed until now that an individual timer was associated with each transmitted segment. However timer management can require considerable overhead. Thus the recommended TCP timer management procedures (defined by RFC standards) use only a single retransmission timer (it is helpful to think of the timer as being associated with the oldest unacknowledged segment).\nUpon receiving data from the application layer, TCP encapsulates it in a segment and passes to the segment to IP. If the timer is not running for some other segment, TCP starts it when the segment is passed to IP, the timer expiration interval being TimeoutInterval If the timeout occurs, TCP responds by retransmitting the segment that caused the timeout and by restarting the timer An valid acknowledgement segment is received: TCP compares the ACK y value with its sendBase (the sequence number of the oldest unacknowledged byte). If y \u0026gt; sendBase then ACK is acknowledging one or more previously unacknowledged segments (cumulative acknowledgement). The sendBase variable is updated and the timer is restarted if there are not-yet-acknowledged segments. Doubling the Timeout Interval Each time TCP retransmits, it sets the next timeout interval to twice the prevous value. However when the timer is restarted after receiving data from the application layer or after receiving an ACK, the TimeoutInterval is recomputed as described previously\nFast Retransmit The problem with timeout-triggered retransmission is that the timeout period can be relatively long. The sender can however often detect packet loss before the timeout event occurs by noting duplicate ACKs. A duplicate ACK is an ACK that reacknowledges a segment for which the sender has already received an earlier acknowledgement. When the TCP sender receives three duplicate ACK for the same data it takes this as an indication that the segment following the segment that has been ACKed three times has been lost. In the case that three duplicate ACKs are received, the TCP sender performs a fast restransmit: it retransmits the missing segment before that segment\u0026rsquo;s timer expires.\nGo-Back-N or Selective Repeat? Acknowledgments are cumulative (GBN) but many TCP implementations will buffer correctly received but out-of-order segments. Also consider fast retransmit where only the missing packet is resent (SR) instead of all the window (GBN). We can see that TCP\u0026rsquo;s error recovery mechanism is categorized as a hybdrid of GB and SR protocols.\n3.5.5 Flow Control The host on each side of a TCP connection set aside a receive buffer for the connection. When TCP receives bytes that are correct and in sequence, it places the data in the receive buffer. The associated application process will read data from this buffer, but necessarily at the instant the data arrives (busy, not interested\u0026hellip;). Thus the the sender can easily overflow the connection\u0026rsquo;s receive bufffer by sending too much data too quickly. To avoid this event, TCP provides a flow-control service. Flow control is a speed-matching service: matching the rate at which the sender is sending against the rate at which the receiving application is reading.\nFlow control and congestion control are not the same!: the former preventing overflow at the receiver side and being actuated only by the two end points, the latter preventing congestion of the network.\nTCP provides flow control by having the sender maintain a variable called the receive window, used to give the sender an idea of how much free buffer space is available at the receiver.\nHost A sends a large file to Host B over TCP.\nB side B allocates a receive buffer to its connection, its size being RcvBuffer B also keeps the variables: LastByteRead (number of last byte in the data stream read by the application process) and LastByteRcvd (the number of the last byte arrived from the network and placed in the receive buffer) We have: LastByteRcvd - LastByteRead \u0026lt;= RcvBuffer (we don\u0026rsquo;t want overflow!)\nReceive window aka the amount of spare room in the buffer rwnd = RcvBuffer - [LastByteRcvd - LastByteRead] rwnd is dynamic\nA side A keeps track of two variables:\n1. `LastByteSent` 2. `LastByteAcked` Through the connection\u0026rsquo;s life A must make sure that LastByteSent - LastByteSent \u0026lt;= rwnd\nIf B\u0026rsquo;s buffer becomes full, he sends rwnd = 0. If B has nothing to send to A, when the application process empties B\u0026rsquo;s buffer, TCP does not send a new segment with the new value of rwnd to A (TCP sends to A only if it needs to send data or if it needs to send an ACK). Therefore A is never informed that B\u0026rsquo;s buffer has some free space and he is blocked and can trasmit no more data. To solve this problem, TCP requires A to continue to send segments with one data byte when B\u0026rsquo;s receive window is 0, these segments will be acknowledged by B. Eventually the buffer will begin to empty and the acknowledgements will contain à non-zero rwnd value.\nWe remember that UDP has no flow control service\n3.5.6 TCP Connection Management How is the connection established? Three-way handshake The client-side TCP sends a special TCP segment to server-side TCP. This segment doesn\u0026rsquo;t contain any application-layer data but the flag bit SYN is set to 1. The segment is referred to as a SYN segment. The client also randomly chooses an initial sequence number (client_isn) and puts this number in the sequence number field of the initial TCP SYN segment. (randomizing client_isn is interesting to avoid security attacks). The TCP SYN segment arrives at the server-side, it is extracted from the datagram. The server allocates the TCP buffers and variables to the connection and sends a connection-granted segment to the client. This segment also contains no application-layer data. The SYN flag is set to 1, the ACK field in the header is set to client_isn+1. The server chooses its own initial sequence number server_isn and puts this value in the sequence number field of the TCP segment header. This segment is referred to as SYNACK segment. Upon receiving the SYNACK segment, the client also allocates buffers and variables to the connection. The client then sends the server yet another segment which acknowledges the SYNACK (server_isn+1 is set the acknowledgement field of the TCP segment header) After this setup, all the segments will have the SYN bit set to 0 in their headers.\nTearing down a TCP connection The client decides to end the connection:\nThe client sends a special TCP segment to the server, this special segment having the FIN bit flag set to 1 in the header. The server receives the segment and sends an acknowledgement to the client. The server then sends its own shutdown segment which also has the FIN bit set to 1 The client acknowledges the server\u0026rsquo;s shutdown segment. The \u0026ldquo;resources\u0026rdquo; (buffers and variables) in the hosts are deallocated. What if the two ends are not ready for communication? A host receives a TCP segment whose port number or source IP address do not match with any of the ongoing sockets in the host -\u0026gt; the host sends a special reset segment to the source (RST flag bit set to 1) and drops the packet (UDP does responds with a special ICMP datagram)\n3.6 Principles of Congestion Control 3.6.1 The Causes and the Costs of Congestion Scenario 1: Two Senders, A Router with Infinite Buffers A -\u0026gt; D, B -\u0026gt; C, A and B connect to the Internet through the same router, B and C connect to the Internet through the same router (pas envie)\n3.7 TCP Congestion Control TCP limits the rate at which it sends traffic into its connection as a function of perceived network congestion. The TCP congestion-control mechanism operating at the sender keeps track of an additional variable: the congestion window, noted cwnd which imposes a constraint on the rate at which a TCP sender can send traffic into the network. Specifically: LastByteSent - LastByteAcked \u0026lt;= min{cwnd, rwnd}. Limiting the amount of unacknowledged data at the sender we can limit the sender\u0026rsquo;s send rate. At the beginning of each RTT the sender sends cwnd bytes of data and at the end of the RTT he acknowledges. Thus the sender\u0026rsquo;s send rate is roughly cwnd/RTT bytes/sec. Adjusting the value of cwnd the sender can adjust the rate at which it sends data into the connection. Let now consider a loss event (timeout OR three duplicate ACKs). When there is excessive congestion some router buffers along the path overflows, causing a loss event at the sender which is taken by the sender to be an indication of congestion on the sender-to-receiver path. If there is no congestion then all the acknowledgements will be received at the sender, which will take these arrivals as an indication that segments have been received and that he can increase the congestion window size and hence its transmission rate. If acknowledgements arrive at a slow rate then the congestion window will be increased at a relatively slow rate and, viceversa, it will be increased more quickly if ACKs arrive at a high rate. Because TCP uses acknowledgements to trigger (or clock) its increase in congestion window size, TCP is said to be self-clocking. TCP uses the principles:\nA lost segment implies congestion therefore the sender rate should be decreased. An acknowledged segment means the network\u0026rsquo;s working, therefore the sender\u0026rsquo;s rate can be increased (if ACK of unacknowledged segment) Bandwidth probing: the transmission rates increases with ACKs and decreases with loss events: TCP is continuously checking (probing) the congestion state of the network TCP Congestion-Control Algorithm Three components :\n1 - Slow Start When a TCP connection begins, cwnd is usually initialized to a small value of 1 MSS and only one segment is sent. Each acknowledged packet will cause the cwnd to be increased by 1 MSS and the sender will send now two segments (because the window is increased by one for each ack). Therefore the number of segments doubles at each RTT, therefore the sending rate also doubles every RTT. Thus TCP send rate starts slow but grows exponentially during the slow start phase. When does the growth end?\nTimeout: cwnd is set to 1 MSS and the slow start is started anew. Also the variable slow start threshold is initialized: ssthresh = cwnd / 2 - (half of value of cwnd when congestion is detected) When cwnd \u0026gt;= ssthresh slow starts is stopped -\u0026gt; congestion avoidance state Three duplicate ACKs: fast retransmit and fast recovery state 2 - Congestion Avoidance TCP suppose congestion is present, how to adapt? Instead of doubling cwnd every RTT, cwnd is increased by just a single MSS every RTT. When should this linear increase stop?\nTimeout: cwnd is set to 1 MSS, and ssthresh = cwnd (when loss happened) / 2 Three duplicate ACKs: cwnd = (cwnd / 2) + 3 MSS and ssthresh = cwnd (when 3 ACKs received) / 2 -\u0026gt; fast recovery state 3 - Fast Recovery cwnd is increased by 1 MSS for every duplicate ACK received for the missing state that caused TCP to enter this state. When the ACK arrives for the missing segment, TCP goes into Congestion Avoidance after reducing cwnd. If a timeout occurs cwnd is set to 1 MSS and ssthresh is set to half the value of cwnd when the loss event occurred. Fast recovery is recommended but not required in TCP, in fact only the newer version of TCP, TCP Reno incorporated fast recovery.\nMacroscopic Description of TCP Throughput What is the average throughput (average rate) of a long-lived TCP connection? Ignoring the slow start phase (usually very short as the rate grows exponentially). When the window size is w the transmission rate is roughly w/RTT. w is increased by 1 MSS each RTT until a loss event. Denote by W the value of w when a loss event occurs. Then we have\naverage throughput of a connection = (0.75 * W)/RTT\nTCP Over High-Bandwidth Paths Today\u0026rsquo;s high speed links allow to have huge windows. What happens if one of the segments in the window gets lost? What fraction of the transmitted segments could be lost that would allow the TCP congestion control to achieve the desired rate?\naverage throughput of a connection = (1.22 * MSS)/(RTT * sqrt(L))\nWhere L is the loss rate\nChapter 4: The Network Layer In the chapter, there is an important distinction between the routing and forwarding functions of the network layer. Forwarding involves the transfer of a packet from an incoming link to an outgoing link within a single router while routing involves all of a network\u0026rsquo;s routers whose collective interactions via routing protocols determine the paths that packets take on their trips from source to destination.\n4.1 Introduction The primary role of routers is to forward datagrams from input links to output links. Routers do not run nor the application-layer or the transport-layer, they go only up until the network layer.\n4.1.1 Forwarding and Routing The role of the network layer is deceptively simple: to move packets from a sending hosts to a receiving host. To do so it performs two important functions:\nForwarding: When a packet arrives to a router\u0026rsquo;s input link, the router must move the packet to the appropriate output link. It is an action local to the router Routing: The network layer must determine the route or path taken by packets as they flow from a sender to a receiver. The algorithms that calculate these paths are referred to as routing algorithsm. It is a network-wide action Every router has a forwarding table. When a router receives a packet, it extracts a value from a specific field in the header and searches for that value in in the forwarding table. The procedure used to set up and update the table depends on the protocol used. However a router receives and sends routing protocol messages to configure its forwarding table.\nWe also need to mark the distinction between routers and packet switches.\nPacket-switches: performs forwarding according to the value in a field in the header of the packet. Some packet switches called link-layer switches base their forwarding decisions on values in the fields of the link-layer frame (link-layer devices) Routers: base forwarding decisions on the value in the network-layer field. (network-layer devices) but also must implement link layer (no 3 without 2) Connection Setup in some computer networks there is a third really important networks-layer function: connection setup: a path-wide process that sets up connection state in routers.\n4.1.2 Network Service Models The network service model defines the characteristics of end-to-end transport of packets between sending and receiving end systems. Some possible service for a network layer:\nSending side: Guaranteed delivery Guaranteed delivery with bounded delay Flow and receiving side: In-order packet delivery Guaranteed minimal bandwidth Guaranteed maximum jitter (amount of time between transmission of two successive packets at the sender side is equal to the amount of time between their receipt at the destination, or that this spacing changes by no more than some specified value) Security services: encryption for confidentiality, data integrity and source authentication The Internet\u0026rsquo;s network layer doesn\u0026rsquo;t provide any of these: it provides a best-effort service there are no timing or bandwidth guarantees, no loss or order guarantees and congestion control indications.\n4.2 Virtual Circuit and Datagrams Networks As in transport layer, the network layer can use connection or connection-less protocols. There however some differences with the transport layer:\nIn the network layer these services are host-to-host services (not the case for the TL, just look at TCP) The network layer provides either a host-to-host connectionless service or a host-to-host connection service but no both. Connection service -\u0026gt; Virtual-Circuit (VC) networks, Connectionless service -\u0026gt; datagram networks 4.2.1 Virtual-Circuit Networks The Internet is a datagram network but many alternative network architectures (ATM) are virtual-circuit networks. The connections in VC are called *virtual circuits (VCs)3. A VC consists of\nA source-to-destination path VC numbers, one for each link along the path Entries in the forwarding table in each router along the path A packet belonging to a virtual circuit will carry a VC number in its header. Because a VC may have different VC numbers on each link, each router must replace the VC number of traversing packets with a new VC number, which is obtained from the forwarding table. How is this determined? Whenever a VC is established across a router, an entry is added to the forwarding table, and one (corresponding to the terminating VC) is removed whenever a VC ends. The routers must maintain connection state information for the ongoing connections (using the tablea). There are 3 phases in a VC:\nVC Setup: sending side contacts networks layer specifying the IP address of the destination. The network sets up the VC. The path is determined as well as the VC number for each link along the path. The forwarding tables are updated and resources reserved. Data transfer: the packets flow VC teardown: The call termination propagates along the path, the forwarding tables are updated During network-layer setup all the routers along the path between source and destination are involved in VC setup, and each router is fully aware of all VCs passing through it (not in TCP: setup and connection only between source and destination). The messages used by end ssystems to initiate or terminate a VC are called signaling messages and the protocols used to exchange them are called signaling protocols.\n4.2.2 Datagram Networks Each time an end system wants to send a packet, it sampts the packet with the address of the destination end system and pops the packet into the network. The routers along the path will use this address to forward it. The router has a forward table that maps destination addresses to link interfaces. When a packet arrives, it reads the destination address, uses the table to determine what link to use, and forwards the packet to that output link interface.\nIf we consider IPv4, addresses are 32 bits long. To avoid having tables with 2^32 entries, routers use prefixes. When there are multiple mathces to one address, the router uses the longest prefix matching rule.\nAlthough routers in datagram networks maintain no connection state information, they nevertheless maintain forwarding state information in their forwarding tables.\n4.2.3 Origins of VC and Datagram Networks VC has its roots in the telephony world, which uses circuits switching too. The datagram model instead comes from the need to simplify as much as possible the network to bring computers together.\n4.3 What\u0026rsquo;s Inside a Router? Input ports: performs the physical layer functions of incoming link at the router. It is also here that the forwarding table is consulted to determine the output port to which the arriving packet will be forwarded via the switching fabric. Control packets (protocol info) are forwarded to the routing processor. Switching fabric: connects input prots to output ports. Output ports: stores packets received from the switching fabric and performs the necessary link layer and physical layer functions. Routing processor: executes the routing protocols (algorithms), maintains routing tables and attached link state information and computes the forwarding table for the router. Input ports, switching fabric and output ports implement the forwarding function and are almost always implemented in hardware (routing forwarding plane hardware) while the routing processor implements the routing function and is usually implemented in software running on traditional CPU (router control plane)\n4.3.1 Input Processing The packet arrives and the link and phyisical layer unpacking functions are performed. The forwarding table is computed and updated by the routing processor with a shadow copy typically stored at each input port so that forwarding decision can be made locallly without invoking the centralized routing processor on a per packet basis and thus avoiding a centralized processing bottleneck. The table is transferred to ports through separated bus. The lookup is then just a search (implemented in hardware and using high performance algorithms), speed also depends on the memory technology (DRAM, SRAM\u0026hellip;). Lookup is important but input processing also consists of\nphysical and link layer processing chekcing the packet\u0026rsquo;s version number, checksum, time to live\u0026hellip;. updating counters for network management. Input ports than moves the packet to the switching fabric (eventually queuing them if this is busy)\n4.3.2 Switching Can be performed in different ways:\nSwitching via memory Switching under the control of the CPU and input and output ports functioned as traditional I/O devices in a traditional operating system. The packet arrives, is copied into the processor memory, the processor determines the output port and copies the packet to the output port\u0026rsquo;s buffer. No parallel forwarding (only 1 memory read/write bus)\nSwitching via a bus An input port transfers a packet directly to the output port over a shared bus without intervention by the routing processor. The input port pre-pends an internal header to the packet. All the output ports receive the packet but only the one that matches the label in the internal header will keep the packet. The output port will remove this internal header. The switching speed is limited to the bus speed as one packet at a time can cross the bus (multiple arriving packets will have to wait). Sufficient only for small area and enterprise networks\nSwitching via an interconnected network To overcome the bandwidth limitation of a single shared bus a more sophisticated interconnection network can be used. A crossbar switch is an interconnection network consisting of 2N buses that connect N input ports to N output ports. Each vertical bus intersects each horizontal bus at a crosspoint which can be opened or closed at any time by the switch fabric controller. If a packet has to go from input X to output Y, only the crosspoint between the horizontal bus from X and the vertical bus to Y will be closed. Thus packets directed to different output ports can be forwarded simultaneously, but not multiple packets directed to the same output port.\n4.3.3 Output Processing takes packets stored in the output\u0026rsquo;s port\u0026rsquo;s memory and transmits them over the output link, thus selecting de dequeuing packets for transmission and performing the necessary link and physical layer transmission functions.\n4.3.4 Where Does Queuing Occur? Queues may form at both the input ports and the output ports. The location and the extent of queuing will depend on traffic load, speed of the switching fabric, and line speed. As the queues grow large, the router\u0026rsquo;s memory can eventually be exhausted and packet loss will occur. IS THIS USEFUL ? NOT COVERED BY TEACHER\n4.4 The Internet Protocol (IP): Forwarding and Addressing in the Internet We know move to study of the network layer in the Internet. There are two versions of the IP (Internet Protocol) in use today: IPv4 and IPv6. There 3 main components in the Internet: the IP Protocol (addressing, datagram format and packet handling conventions), the routing protocol (path selection), the Internet Control Message Protocol (ICMP) (error reporting and network information).\n4.4.1 Datagram Format A network layer packet is referred to as a datagram.\nSome fields:\nVersion number: 4 bits specifying the IP protocol version of the datagram (IPv4 or IPv6 ) Header length: the length of the packet is variable therefore this field tells where the header ends and the data begins. Usually datagrams contain no option so that the typical IP datagram has 20-byte header Type of service (TOS): allows different types of datagrams to be distinguished from each other. (eg real time vs non real time) Datagram length: 16 bits specifying the total length, that is header + data measured in bytes. 16 bits -\u0026gt; max header length = 65535 bytes, but usually datagrams are rarely larger than 1500 bytes. Identifier, flags, fragmentation offset: used for IP fragmentation. (NB: IPv6 doesn\u0026rsquo;t allow fragmentation at routers) Time-to-live (TTL): used to avoid that datagrams circulate forever. It is decreased by one each time the datagram is processed by a router. When TTL = 0, the datagram is dropped Protocol: only used when datagram reaches its final destination, it specifies what transport protocol to which the data of the datagram should be passed. EX: 6 -\u0026gt; TCP, 17 -\u0026gt; UDP Header checksum: helps the router to detect bit errors in a received IP datagram. Computation: each two bytes in the header are considered as numbers, summed up using the 1s complement arithmetic. The 1s complement of this sum is then put in the checksum field. A router computes the checksum for each datagram. If the computed one doesn\u0026rsquo;t equal the one in the field then the router has detected an error. Usually the datagram is discarded. As it is recomputed at each router, it may change. Source and destination IP addresses Options: rarely used, dropped by IPv6 Data (payload): usually contains the transport layer segment but can also contain ICMP messages IP Datagram Fragmentation The maximum amount of data that a link layer can carry is called the Maximum Transmission Unit (MTU). As each datagram is encapsulated in a link layer frame, the MTU imposes a hard limit on the length of the datagram. Each of the links along the route can use different link-layer protocols and therefore can have different MTU. We therefore have to break the IP datagram into smaller datagrams, each of which will go in different link layer frames. Each of these smaller datagrams is referred to as a fragment. A fragment must be reassembled before it can be passed to the transport layer. To reduce the workload on routers, the designers of IPv4 decided that reassembling should only be done at the destination end system.\nIn IPv4, to comply with fragmentation, the header contains the fields:\nIdentifiers: identifies the unfragmented datagram (same for all fragments) flags: in particular there is one flag set to 0 if the fragment is the last or to 1 if there are more to come fragmentation offset: an integer x, the data in the fragment should be inserted beginning at byte x * 8 If one fragment contains error or is lost, all the others are dropped and TCP will have the sender retransmit all the data. Fragmentation complicates the network and end systems and can be used in lethal DoS attacks such as the Jolt2 attack\n4.4.2 IPv4 Addressing The boundary between the host and the physical link is called an interface. A router has multiple links connected to it, therefore multiple interfaces and therefore a router has multiple IP addresses and an IP address is technically associated with an interface rather than with a host or router. IPv4 addresses are 32 bits long (4 bytes) -\u0026gt; max 2^32 possible addresses. They are typically writen in dotted decimal notation where each byte of the address is written in deciaml from and separated by a period from the others. EX 193.32.216.9 === 11000001 00100000 11011000 00001001 Each interface on every host (except host behind NATs) must have a unique IP address. How are these computed? A portion is determined by the subnet to which the host is connected. A subnet is the portion of the network interconnected end systems and one one router. (also called IP network or network). IP assigns an address to a subnet x.x.x.x/y where /y notation, sometimes known as a subnet mask indicates that the leftmost y bits of the 32 bit quantity define the subnet address. If y is 24, then any host attached to the a.a.a.0/24 subnet would be required to have an address of the form a.a.a.xxx.\nFIGURE 4.17 WTF\nThe Internet\u0026rsquo;s address assignment strategy is known as Classless Interdomain Routing (CIDR). It generalizes the notion of subnet addressing. Consider a.b.c.d/x : the x most significant bits constitute the network portion of the IP address and are often referred to as the prefix (or network prefix). EX an organization is assigned a block of contiguous addresses, that is, a range of addresses with a common prefix. When someone outside the organization want to send a datagram to someone inside, he will only need this x bits. The remaining 32-x bits can be thought of as distinguishing among the devices within the organization. These bits may have an additional subnetting structure. There is yet another type of IP address, the IP broadcast address 255.255.255.255. When a datagram is sent to this address, the datagram is delivered to all hosts on the same subnet.\nObtaining a Block of Addresses A network administrator contacts an ISP which would provide a partition of the addresses that had already been allocated to him. EX ISP has 200.23.16.0/20, it splits in 8 equal sized blocks: 200.23.16.0/23, 200.23.18.0/23, 200.23.20.0/23, \u0026hellip;, 200.23.30.0/23 Who assigns set of addresses to ISPs? The Internet Corporation for Assigned Names and Numbers (ICANN) which allocates IP addresses, manages DNS root servers, assigns domain names and solves domain name disputes.\nObtaining a Host Address: The Dynamic Host Configuration Protocol Once an organization has obtained a block of addresses, it can assign individual IP addresses to the hosts and router interfaces which are part of it. This can be done either manually (by the network administrator) or automatically by the Dynamic Host Configuration Protocol (DHCP). It can be configured so that a host receives the same IP each time it connects to the network or a temporary IP addresses that will change upon each connection. DHCP also transmits to hosts additional information (subnet mask, address of first-hop = default gateway, address of local DNS server). As it automates the connection of a host into the network, DHCP is often referred to as a plug-and-play protocol. It is also popular in *wireless LANs where hosts join and leave frequently and in residential ISP access networks. DHCP is a client-server protocol, the client being a newly arriving host needing network configuration information and the server being a router or a DHCP relay agent that know the address of a DHCP server for that network. For a new client there is a 4 step process i nthe DHCP protcol:\nDHCP server discovery c (client) looks for a server sending DHCP discover message a UDP packet directed to port 67. This segment is encapsulated in datagram sent to 255.255.255.255 (broadcast address) from address 0.0.0.0 DHCP server offer(s) s (server) replies with a DHCP offer message broadcast to all nodes on the subnet using (sent to 255.255.255.0). c may receiver many of these (more servers) containing the transaction ID, proposed IP address and an address lease time (amout of time for which the address will be valid) DHCP request: c chooses one offer and responds to s with a DHCP request message echoing back the configuration parameters DHCP ACK s responds with DHCP ACK message confirming DHCP also provides a mechanism for renewing the lease on an address.\nNetwork Address Translation (NAT) Every IP-capable device needs an IP address. The number of connected devices grows fast, how to deal with IPv4 address space exhaustion? Network Address Translation (NAT) The NAT-enabled router defines a realm (or private network) (a network whose addresses only have meaning to devices within that network) and it can use the whole 32 bit address space for devices connected to it, it will also have a public address used to communicate with the exterior. The picture is explicative.\nFrom the outside the router looks like a single device with a single IP address. It hides the details of the internal network from the outside world. Internal addresses can be assigned using DHCP.\nProblems with NAT:\nPort number should be used for addressingi processes not hosts Routers shouldn\u0026rsquo;t have access to the transport layer (ports) NAT violates end-to-end argument (any host should be able to contact any other host) NAT interferes with P2P applications (peers hidden by NAT), therefore the need of connection reversal for NAT traversal UPnP NAT traversal is increasingly provided by Universal Plug and Play. It requires both the host and the NAT to be compatible. Host requests a NAT mapping_ (private IP address, private port number) -\u0026gt; (public IP address, public port number) If the NAT accepts and creates the mapping, then outsiders can create connections to (public IP address, public port number).\n4.4.3 Internet Control Message Protocol ICMP is used to communicate network-layer information between hosts and routers, usually for error reporting (ex Destination network unreachable). ICMP is considered part of IP but architecturally lies just above IP as ICMP messages are carried inside IP datagrams as payloads. ICMP have a type and a code field and carry the header and the first 8 bytes of the datagram that caused the message to be generated in the first place. Ping and traceroute are implemented using ICMP messages\nInspecting datagrams: firewalls and intrusion detection systems Firewalls inspect the datagram and segment header fields denying suspicious datagrams entry into the internal network. Firewalls can block ICMP packages or packets based on port numbers, addresses. Additional protection can be provided by IDS, placed at the boundary of the network, performs deep packet inspection examining not only headers but also payloads (including application layer data). IDS have databases of packet signatures that are know to be dangerous. As packets flow through the IDS, it tries to match them to signatures in its database, if a match is found, an alert is created. IPS (intrusion prevention system) in addition to detecting, also blocks packets raising alerts.\n4.4.4 IPv6 Developed because of IPv4 address space exhaustion\nDatagram format the size of the source and destination addresses is increased from 32 to 128 bits: every grain of sand on the planet can be addressable. Unicast and multicast addresses are joind by the anycast address which allow a datagram to be delivered to any one of a group of hosts. A number of IPv4 fields have been dropped or made optional resulting in a 40-byte fixed-length header which allows faster datagram processing. Flow label not clear definition. 20-bit Version: 4-bit for IPv6 or 4. If ipv6 -\u0026gt; 0110 Traffic class: 8 bit similar to TOS Payload length: 16 bit unsigned integer indicating number of bytes following the 40-byte datagram header Next header: transport layer protocol Hop limit: decremented by one by each router forwarding the datagram, when 0, the datagram is discarded Fragmentation and reassembly cannot be done by intermediate routers, only by source and destination. If a router cannot trasmit a datagram because too big, it drops it and sends back an ICMP error message \u0026ldquo;Packet too big\u0026rdquo;. This reduces a lot the workload on the network.\nAs the transport layer and the link layer already perform check-summing, this functionality has been removed from the network layer for faster datagram processing.\nAn option field is no longer part of the header, instead it is one of the possible next headers pointed to from the header. A new version of ICMP has been defined for IPv6 which includes messages adapted to IPv6 (\u0026ldquo;packet too big\u0026rdquo;) and replaces IGMP (Internet Group Management Protocol), used to manage a host\u0026rsquo;s joining and leaving of multicast groups.\nTransitioning from IPv4 to IPv6 IPv6 is back compatible with IPv4 but not viceversa.\nIt\u0026rsquo;s not humanable possible to decide a date on which all machines would change their protocol. The most straightfoward way is a dual stack approach where IPv6 nodes also have a complete IPv4 implementation. To determine whether anotehr node is IPv6 or IPv4-only DNS can be used, just checking whether the node has a IPv6 address or an IPv4 one. However this will bring about the loss of data in specific IPv6 header fields. Another approach would be tunneling : when two IPv6 nodes are connected by intervening IPv4 routers, we call the IPv4 nodes tunnel, the entire IPv6 datagram is put in the payload field of a IPv4 datagram which will be propagated by the tunnel unaware of the details and received by the destination IPv6 node which is able to extract the IPv6 datagram and to route it. This migration shows the difficulty in changing network-layer protocols.\n4.5 Routing Algorithms A host is attached directly to one router, the default router for the host (also called first hop router). Whenever a host sends a packet, the packet is transferred to its default router, which we\u0026rsquo;ll call source router, we\u0026rsquo;ll call the default router for the destination host as the destination router. Routing a packet from source to destination boils down to routing the packet from source router to destination router.\nThe purpose of a routing algorithm is simple: given a set of routers connected by links, it finds a \u0026ldquo;good\u0026rdquo; path from source to destination router. A good path is the least expensive one.\nGraphs (see Algorithms course) are used to formulate routing problems, the node representing routers and the edges the links connecting them. Each edge also has a value representing its cost. For any nodes x and y in the G(raph) we denote c(x,y) the cost of the edge between them. If (x,y) doesn\u0026rsquo;t belong to G, we set c(x,y) = infinity. We only consider undirected graphs. We just have to find the least costly paths between sources and destinations. We can classify routing algorithms in two groups:\nGlobal routing algorithms: compute the least-cost path between a source and a destination using complete, global knowledge about the network. They are often referred to as link-state (LS) algorithms since the algorithm must be aware of the cost of each link in the network Decentralized routing algorthms: compute the least-cost path in an iterative, distributed manner: no node has complete information about the cost of all network links. Instead, each node begins with only the knowledge of the costs of its own directly attached links. We could also make another classification separating static routing algorithms (routes change very slowly, eg after human intervention) and dynamic routing algorithms( routing change as the load or topology change). Finally another distinction could be made between load-sensitive or load-insensitive algorithms according to whether link costs vary reflecting the level of congestion.\n4.5.1 The Link-State (LS) Routing Algorithm All link costs are known. In practice this is accomplished by having each node broadcast link-state packets to all other nodes in the network, each packet containing the identities and costs of its attached links resulting in all nodes having an identical and complete view of the network (each node could run the algorithm). A link-state algorithm can be Dijkstra\u0026rsquo;s algorithm or Prim\u0026rsquo;s algorithm. Code and example page 394\n4.5.2 The Distance-Vector (DV) Routing Algorithm The distance-vector algorithm is iterative, asynchronous and distributed.\nDistributed because each node receives some information from one or more of its directly attached neighbors, performs a calculation and then distributes the results back to its neighbors. iterative: the process continues on until no more information is exchanged between neighbors (self terminating) asynchronous: the nodes are not required to operate in lockstep with each other The least cost between x and y d(x,y) can be determined using the Bellman-Ford equation :\nd(x,y) = min_v {c(x,y) + d(v,y)}\n\u0026hellip; to be continued\n4.5.3 Hierarchical Routing In practice it is not possible to have a network of interconnected routers running the same routing algorithm because of two reasons:\nScale if the number of routers is large, running LS or DV algorithms for the whole network becomes prohibitive for memory, processing, storing and timing costs. Administrative autonomoy an organization should be able to organize its network as it wishes, while still being able to connect its network to the outside world. Therefore routers are organized into autonomous systems (ASs), each of which being under the same administrative control. Routers in the same AS run the same routing algorithm and have information about each other. The routing algorithm running within an AS is called an intra-autonomous system routing protocol. In an AS, one or more routers will have the task of being responsible for forwarding packets outside the AS, these routers are called gateway routers. To obtain reachability information from neighboring ASs and propagating the reachability information to all routers interal to its AS, gateway routers use inter-AS routing protocols. Two communicating ASs must run the same inter-AS routing protocol.\nWhen a router needs to forward a packet outside its AS and there are multiple gateway routers, the router has to make a choice. One often employed practice is to use hot-potato routing: the AS gets rid of the packet as quickly as possible (as inexpensively as possible), the router sends the packet to the gateway router that has the smallest router-to-gateway cost among all gateways with a path to the destination. An AS can decide what (internal) destinations to advertise to neighboring ASs: this a policy decision.\n4.6 Routing in the Internet 4.6.1 Intra-AS Routing in the Internet: RIP Intra-AS routing protocols are also known as interior gateway protocols. Historically two of these have been used extensively in the Internet: Routing Information Protocol (RIP) and Open Shortest Path First (OSPF).\nRIP was started for the Xerox Network Systems (XNS) architecture and was was widely deployed after being included in BSD. It is a distance-vector protocol working very similarly to what studied before. RIP uses hop count as a cost metric (each link has cost 1). Costs are from source router a destination subnet (not router-to-router as previously seen). hop = number of subnets traversed along the shortest path from source to destination subnet, including the destination subnet.\nRouting updates [messages] are exchanged between neighbors approximately every 30 seconds using a RIP response message, which contains a list of up to 25 destination subnets within the AS as well as the sender\u0026rsquo;s distance to each of those subnets. Response messages are also known as RIP advertisements. Each router maintains a RIP table known as a routing table which includes both the router\u0026rsquo;s distance vector and the router\u0026rsquo;s forwarding table. There are three columns in it: the destination subnet, the identity of next router along shortest path to reach destination and the number of hops to get to the destination along the shortest path.\nIf a router doesn\u0026rsquo;t hear from its neighbor for at least once every 180 seconds, that neighbor is considered to be no longer reachable (died or link down). Routers can also request information about its neighbor\u0026rsquo;s cost to a given destination using RIP\u0026rsquo;s request messages, which are transmitted over UDP using port 520. RIP is implemented in software but has access to the routing tables through the UNIX kernel.\n4.6.2 Intra-AS Routing in the Internet: OSPF OSPF and the related IS-IS are typically deployed in upper-tier ISPs whereas RIP is deployed in lower-tier ISPs and enterprise networks. Open indicates that the routing protocol speficication is publicly available. It was conceived as the successor to RIP. It is however a link state protocol that uses flooding of link-state information and a Dijkstra least-cost path algorithm: routers construct a complete topological map (graph) of the AS, then run Dijkstra\u0026rsquo;s algorithm to determine a shortest-path tree to all subnets with itself as the root node. Link costs are individually configured by the networks administrator who might choose to set all the link costs to 1, thus achieving minimum hop routuing or might choose to set the link weights to be inversely proportional to link capacity in order to discourage traffic from using low-bandwidth links. A router broadcasts routing information to all other routers in the AS, not just the neighbors. The broadcast happens whenever there is a change in a link\u0026rsquo;s state or every 30 minutes if the link\u0026rsquo;s state doesn\u0026rsquo;t change. OSPF advertisements are contained in OSPF messages that are carried by IP with an upper-lyerprotocol of 89 for OSPF, therefore OSPF must implement reliable message transfer and link-state broadcast; OSP also checks that links are operational using HELLO messages to attached neighbors. OSPF offers some services:\nsecurity: OSPF messages can be authenticated (not active by default). multiple same-cost paths: two paths having same cost can be used at the same time. integrated support for unicast and multicast routing support for hierarchy within a single routing domain: ability to structure an autonomous system hierarchically. A OSPF AS can be configured hierarchically into areas, each running its own OSPF algorithm, with each router broadcasting its link state to all other routers in that area. Area border routers are responsible for routing packets outside the area and one area is configured to be the backbone area, which routes traffic between other areas in the AS, it contains area border routers but also normal routers. 4.6.3 Inter-AS Routing: BGP The Border Gateway Protocol (BGP) is the de facto standard inter-AS routing protocol in today\u0026rsquo;s Internet. It provides each AS means to:\nobtain reachability information from neighboring ASs propagate reachability information to all internal routers determine good routes to subnets using reachability information and AS policy. it allows each subnet to advertise its existence to the rest of the Internet Basics It is a very complex algorithm. Routers exchange information over semipermanent TCP connections using port 179. There is typically one such BGP TCP connection for each link directly connecting two routers in two different ASs but there are also semipermanent TCP connections between routers in the same AS. For each connection, the two routers at the end of it are called BGP peers and the connection is called a BGP session. A session spanning two ASs is an external BGP (eBGP) session and BGP sessions between routers within an AS is called an internal BGP (iBGP) session. Destinations are not hosts, but CIDRized prefixes, each representing a subnet or collection of subnets.\nPath Attributes and BGP Routes In BGP an AS is identified by its globally unique AS number (ASN) which is assigned by ICANN regional registries. When a router advertises a prefix across a BGP session, it includes with the prefix a number of BGP attributes, a prefix with its attributes is called a route. Two other important attributes are:\nAS-PATH: contains the ASs through which the advertisement for the prefix has passed. When a prefix is passed into an AS, the AS adds its ASN to the AS-PATH. This attribute is used to detect and prevent looping advertisements (if router sees that its AS is already in AS-PATH, it rejects the ad) and to choose among multiple paths to the same prefix. -NEXT-HOP: the router interface that begins the AS-PATH. BGP also includes attributes allowing routers to assign preferences metrics to the routes and indicating how to prefix was inserted into BGP at the origins. When a router receives a route advertisement, it uses its import policy to decide whether to accept or filter the route and whether to set certain attributes such as the router preference metrics.\nBGP Route Selection The input of the selection is the set of all routes that have been learned and accepted by the router. If two or more routes exist for the same prefix, elimination rules are applied until only one remains.\nChapter 8: Security in Computer Networks 8.1 What is Network Security? Desirable properties of secure communication:\nConfindentiality: only sender and receiver should be able to understand the contents of the transmitted message -\u0026gt; encryption Message integrity: make sure the content of the communication is not altered -\u0026gt; checksum End-point authentication: sender and receiver should be able to confirm the identity of the other party involved in the communication. Operation security: ability to counter attacks to internal networks -\u0026gt; firewalls, IPS, IDS Possible attacks:\neavesdropping: sniffing and recording messages flowing in a channel modification, inserion, deletion of messages or message content These two allow to mount many other types of attacks\n8.2 Principle of Cryptography See Information Science, BA2 ADDITION:\nBlock Ciphers Today there are two broad classes of symmetric encryption techniques: stream ciphers and block ciphers(used for PGP, SSL, IPssec) In a block cipher, the message to be encrypted is processed into blocks of k bis and each block is encrypted independently. To encode a bloc, the cipher uses a on-to-one mapping to map the k-bit block of cleartext to a k-bit block of ciphertext. To avoid bruteforce attacks, cipher blocks usually employ large blocks (k=64) but longer blocks implies longer tables to store the mappings. Block ciphers typically use functions that simulate randomly permuted tables. EX 64 bit input split into 8 8-bit chunks, each of which is processed by a 8-bit to 8-bit table, each chunk having its table. The encrypted chunks are reassembled into a 64 bits message which is fed again to the input. After n such cycles, the function provides a 64-bit block of ciphertext. The key for this block would be the eight permutation tables, assuming that the scramble function is publicly known. Popular block ciphers: DES (Data Encryption Standard), 3DES, AES (Advanced Encryption Standard). These use functions instead of predetermined tables. Each of them uses a string of bits for a key (64-bit blocks with 56-bit key in DES, 128-bits blocks and 128/192/256 bits-long keys)\nCipher-Block Chaining We need to avoid long messages avoiding that two or more identical ciphertexts (produced for identical cleartexts by a symmetric encryption). (I DON\u0026rsquo;T FINISH THIS PART, IT GOES TOO DEEP INTO ENCRYPTION TECHNIQUES WHICH IS NOT WHAT WE ARE INTERESTED IN)\n8.3 Message Integrity and Digital Signatures We want to provide message integrity (aka message authentication). Message integrity is verified when:\nThe message received indeed originated from the sender The message was not tampered with on its way to the receiver 8.3.1 Cryptographic Hash Functions A hash function takes an input m and computes a fixed length size string H(m) known as a hash. A cryptographic hash function is required to have an additional property:\nit is computationally infeasible to find any two different messages x and y such that H(x) = H(y) Some used cryptographic hashing functions are md5, SHA\u0026hellip;\n8.3.2 Message Authentication Code To perform message integrity we also need a shared secret s, called the authentication key. The procedure is then:\nAlice creates message m, concatenates m+s and computes the hash H(m+s) to create the message authentication code (MAC) Alice appends the MAC to the message m creating (m+H(m+s)) Bob receives the message and knowing the hash function and the secret, computes the hash. He creates H(m+s) and compares it with what he received. MAC is nice because it doesn\u0026rsquo;t require any encryption algorithm The most popular standard of mac today is HMAC which can be used with either MD5 or SHA-1. The problem then is: how to distribute the secret? Physically?\n8.3.3 Digital Signatures A digital signature is a cryptographic technique to indicate the owner or creator of a document or to signify one\u0026rsquo;s agreement with a document\u0026rsquo;s content. Just as with handwritten signatures, digital signatures should be created in a way that they are verifiable (prove that the the author of a signature is indeed the author) and nonforgeable** (prove that only that individual could have signed the document). We can use the public and private keys we already created for asymmetric confidentiality. To sign a message m Bob can encrypt the message with the private key (only the matching public key will be able to decrypt). However encryption and decryption and computationally expensive therefore:\nBob computes the hash of the message Bob uses his private key to encrypt the hash Bob contants the encrypted hash and the message Alice can decrypt, find the hash, compute a hash herself check for identity We saw that both digital signatures and MACs involve using a hash function but digital signatures, requiring encryption, need heavier operations and also need a Public Key infrastructure (PKI) with certification authorities.\nPublic Key Certification An important application of digital signatures is public key certification, that is, certifying that a public key belongs to a specific entity. It is used in IPsec and SSL. A Certification Authority binds a public key to a particular entity. It has the follow roles:\nA CA verifies that an entity (person, router, \u0026hellip;) is who it says it is. The method depends on the authority The CA creates a cerificate that binds the public key of the entity to the identity. The certificate contains the public key and globally unique identifying information about the owner of the public key. The certificate is digitally signed by the CA 8.4 End-Point Authentication End-point authentication is the process of one entity proving its identity to another entity over a computer network. Authentication must be done solely on the basis of messages and data exchanged as part of an authentication protocol. Typically this would run before the two communicating parties run some other protocol.\nWe can analyze authentication developing a simple algorithm step by step:\nVersion 1.0 Alice simply sends a message to Bob saying \u0026ldquo;I\u0026rsquo;m Alice\u0026rdquo;\nVersion 2.0 Alice and Bob always communicate using the same addresses. Bob can simply check that the message has the source IP of Alice. However is fairly easy to spoof an IP address: crafting a special datagram is feasible using a custom kernel e.g Linux.\nVersion 3.0 Alice and Bob could share a secret password, a secrete between the authenticator and the person being authenticated. Alice: I\u0026rsquo;m Alice, Password. However password can be eavesdropped, sniffed (read and stored).\nVersion 3.1 We could encrypt the password using a shared symmetric cryptographic key. However this protocol is subject to playback attacks an eavesdropper could sniff the encrypted secret and, without having to decrypt, could send it to impersonate Alice.\nVersion 4.0 To avoid playback attacks we could use the same principle behind TCP\u0026rsquo;s three way handshake. A nonce is a number that a protocol will use only once in a lifetime. The procedure is then:\nAlice sends: I am Alice bob chooses a nonce and sends it to Alice Alice encrypts it using Alice and Bob\u0026rsquo;s symmetric secret key and sends the encrypted nonce. Bob decrypts the received nonce and checks for equality with the one he generated. 8.5 Securing e-mail Security functionalities are provided by many layers of the network stack. Why? There is a need for security at higher layers as well as blanket coverage at lower layers and it easier to provide security at higher layers.\n8.5.1 Secure E-Mail What features do we want? Confindentiality, Sender authentication, Receiver authentication.\nConfidentiality: to overcome the problem of sharing a symmetric secret, Alice and Bob use asymmetric cryptography. Bob makes his public key publicly available (key server or web page) and Alice encrypts her message with Bob\u0026rsquo;s public key. Bob can decrypt using his private key. However asymmetric crypto is quite inefficient. A session key can be used: Alice selects a random symmetric key. She uses it to encrypt the message. She the encrypts this key using Bob\u0026rsquo;s public key and concatenates the symmetricly encrypted message and the asymmetricly encrypted key. Sender authentication and message integrity: we suppose that Alice and Bob don\u0026rsquo;t care for confidentiality. They will use digital signatures and message digests. Alice applies a hash function H to her message m, obtain a message digest, signs the digest with her private key to create a digital signature, concatenates the original message with the signature to create a package and sends the package to Bob\u0026rsquo;s e-mail address. Bob uses Alice\u0026rsquo;s public key to the digest and compares the result fo this operation with his own hash H of the message. Confidentiality, sender authentication and message integrity: the two procedures above can be combined, message and digest are concatenated and the treated as a new message which is encrypted using the first technique. These techniques suppose however that Alice and Bob are able to exchange their public keys. An intruder could in fact send a public key to Bob pretending to be Alice. Certification is needed.\nPhil Zimmermann and PGP PZ was the creator of PGP. For that he was legally attacked by the US Government, he distributed PGP while it should have stayed a secret weapon in the heads of the defense. The US dropped the case and PGP became the most widely used e-mail encryption software in the world despite the lack of funding, paid staff.\n8.5.2 PGP Pretty Good Privacy (PGP) is an e-mail encryption scheme that has become the De Facto standard. It uses the same design shown above, giving the option of signing, encrypting or both. When PGP is installed, it creates a public key pair for the user, the public key can be posted online while the private key is protected by a password which has to be entered every time the user accesses the private key. A PGP message appears after the MIME header. PGP also provides a mechanism for public key certification. PGP public keys are certified by Web of Trust: Alice can certify any key/username pair when she believes the pair really belong together and, in addition, PGP permits Alice to say that she trusts another user to vouch for the authenticity of more keys. Some PGP users sign each other\u0026rsquo;s key by holding key-signing parties.\n8.6 Securing TCP Connections: SSL We now move to the transport layer. The enhanced version of TCP is called Secure Socket Layer (SSL), a slightly modified version of SSL v3 called Transport Layer Security (TLS) has been standardized by the IETF. Originally developed by Netscape, SSL has enjoyed broad deployment since its origins, providing secure communication between all recent browsers and online services. SSL provides TCP with confidentiality, data integrity, server authentication and client authentication. SSL is often used over HTTP, however, as it secures TCP, it can be employed by any application that runs over TCP. SSL provides a simple Application Programming Interface with sockets, similar to TCP\u0026rsquo;s API. When an application wants to use SSL, it must include SSL classes/libraries. Technically SSL resides in the application layer but from the developer\u0026rsquo;s perspective it is a transport layer protocol that provides TCP\u0026rsquo;s services enhanced with security services.\n8.6.1 The Big Picture (primitive almost-SSL) Three phases:\nHandshake: Bob initiates a TCP connection is established (TCP SYN, SYNACK, ACK). Bob sends SSL Hello, Alice responds with her certificate containing her public key (the certificate being certified by a CA, Bob is sure that the key belongs to Alice). Bob generates a master secrect (MS), encrypts it with Alice\u0026rsquo;s public key to create the Encrypted Master Secret (EMS) and sends it to Alice who will decrypt it with her private key to get the MS which can be used for confidentiality and integrity as seen before. Key Derivation instead of using the MS for integrity and confidentiality, it is safer to use different keys for different functions. Therefore both Alice and Bob use the MS to generate: Eb = session encryption key for data Bob -\u0026gt; Alice Mb = session MAC key for data Bob -\u0026gt; Alice Ea = session encryption key for data Alice -\u0026gt; Bob Ma = session MAC key for data Alice -\u0026gt; Bob The MS could simply be split in four chunks, but real SSL does it differently. Data Transfer TCP is a byte-stream protocol, so where would we put the MAC for the integrity check? SSL breaks the data stream into records, appends a MAC to each record and then encrypts record+MAC. However, in a MITM attack, the order of packets could be reversed as TCP sequence numbers are not encrypted. SSL therefore uses sequence numbers. Bob keeps a sequence number counter which begins at zero and is incremented at each record transmission. He includesthe sequence number in the MAC calculation: MAC = hash(data+Mb+SeqNum). Alice tracks Bob\u0026rsquo;s sequence numbers so that she can verify the MAC. SSL Record The real SSL record:\nType: handshake message, data message, connection teardown message Length: used to extract the records out of the TCP byte stream 8.6.2 A More Complete Picture SSL allows Alice and Bob to agree on the cryptographic algorithms at the beginning of the SSL session, during handshake. Steps:\nThe client sends a list of cryptographic algorithms it supports, along with a client nonce The server chooses a symmetric algorithm (ex: AES), a public key algorithm (ex RSA) and a MAC algorithm. It sends back to the client its choices as well as a certificate and a server nonce. The client verifies the certificate, extracts the server\u0026rsquo;s public key, generates a Pre-Master Secret (PMS), encrypts it with the server\u0026rsquo;s public key and sends the encrypted PMS to server. Using the same key derivation function (specified by SSL standard), client and server independently compute the Master Secret (MS) from the PMS and the nonces. The MS is sliced up to create the two encryption and the two MAC keys. Furthemore when the symmetric cipher employs CBC (ex 3DES or AES) the two Initialization Vectors (IVs), one for each side of the connection, are also obtained from hte MS. Henceforth all messages sent between client and server are encrypted and authenticated (using MAC) The client sends a MAC of all the handshake messages The server sends a MAC of the handshake messages. 5 and 6 protect the handshake from tampering: if in the end MAC are not coherent with the previously sent messages, the connection is stopped. (prevents an attacker from impersonating the server and imposing weak algorithms). Nonces are used to avoid connection replay attacks (resending packets sniffed during a previous connection again, using nonces allows to have different MACs and therefore messages at each connection, even if the content of the communication is the same).\nConnection Closure TCP FIN segments can be crafted by an attacker (truncation attack), therefore they cannot be used. The type field of SSL records is used for these purpose, even if it sent in the clear, it is authenticated at the receivers using record\u0026rsquo;s MAC.\n8.7 Network-Layer Security: IPsec and Virtual Private Networks The IP security protocol is called IPsec, it secures IP datagrams between any two network-layer entities (host, routers)\n8.7.1 IPsec and Virtual Private Networks (VPNs) An institution extending overt multiple geographical regions might want its own IP network so that the machines in it can communicate securely. Such a disjoint network is a private network. A physical private network can be expensive. VPN can be used to deploy and maintain a private network over the existing public Internet. The traffic is sent over the Internet but encrypted before entering the public net. Not all traffic sent into the Internet by the gateway routers or laptops will be IPsec secured (only the portion accessing internal resources)\n8.7.2 The AH and ESP Protocols In the IPsec protocol suite, there are two principal protocols: the Authentication Header (AH) protocol and the Encapsulation Security Payload (ESP) protocol. When a source IPsec entity (router or host) sends secure datagrams to a destination entity it does so with either ESP or AH. AH provides source authentication and data integrity while ESP provides source authentication, data integrity and confidentiality. Because the latter is often critical for VPNs, ESP is much more widely used AH. We will only study ESP.\n8.7.3 Security Associations Before sending IPsec datagrams from source entity to destination entity, source and destination create a network-layer logical connection called security association (SA). SA is a simplex (unidirectional from source to destination) logical connection. If both entities want to send datagrams to each other, then two SAs need to be established, one in each direction. The VPN server (headquarters gateway router) will maintain state information about the SA, which will include:\n32-bit identifier for the SA, called Security Parameter Index (SPI) The origin interface (client outside) of the SA and its destination (its out facing interface) [IP addresses] Type of the encryption used Encryption key Type of the integrity check Authentication key An IPsec entity often maintains state information for many SAs (all outside clients) using its Security Association Database (SAD) which is a data structure in the entity\u0026rsquo;s OS kernel.\n8.7.4 The IPsec Datagram IPsec has two different packet forms, one for tunnel mode and one for transport mode, the first one, being more appropriate for VPNs, is more widely deployed than the transport mode, we will therefore only focus on it.\nThe headquarters\u0026rsquo;s gateway receives an IPv4 datagram from inside the network directed to a VPN client outside. Here is what happens:\nIt appends to the back of the original datagram (which includes the original header fields) in the ESP trailer field It encrypts the result using the algorithm and key specified in the SA Appends to the front of the result a ESP Header creating the \u0026ldquo;enchilada\u0026rdquo; Creates an authentication MAC over the whole enchilada using algorithm and key specified in the SA Appends the MAC to the back of the enchilada forming the payload Creates a brand new IP header with all the classic IPv4 header fields which it appends before the payload. The protocol number field is set to 50, designating IPsec. The routers along the path will treat the datagram as a normal one, oblivious that it is an IPsec datagram. To decide whether outgoing packets should be treated as above or simply let through, the gateway maintains a Security Policy Database (SPD) which indicates what types of datagrams (as a function of the source and destination IPs and of the protocol) are to be IPsec processed and, for those that are, which SA should be used. IPsec provides confidentiality, source authentication, data integrity, replay-attack prevention.\n8.7.5 IKE: Key Management in IPsec Who/What should populate the SAD? For small VPNs this can be done manually. For larger ones there is the Internet Key Exchange (IKE) protocol. IKE is similar to the handshake in SSL. Here are the steps:\nDuring the first exchange of messages, the two sides use Diffie-Hellman to create a Bi-Directional IKE SA between the routers, which is entirely different form the IPsec SA discussed above. This IKESA provides an authenticated and encrypted channel between the two routers. Keys are established for encryption and authentication for IKESA. Also established is a master secret. During the second exchange of messages, both sides reveal their identity to each other by signing their messages. However the identities are not revealed to an eventual sniffer, since the messages are sent over the IKE sa channel. The two sides also negotiate the IPsec encryption and authentication algorithms to be employed by the IPsec SA. Finally the two sides create an SA n each direction. We have two phases to reduce computational costs: we don\u0026rsquo;t need asymmetric cryptography during second phase, allowing IKE to generate many SAs with relatively little computational cost.\n8.9 Operational Security: Firewalls and Intrusion Detection Systems 8.9.1 Firewalls A firewall is a combination of hardware and software that isolates an organization\u0026rsquo;s internal network from the Internet at large, allowing some packets to pass and blocking others. It has three goals\nAll traffic from outside to inside, and vice versa, passes through the firewall Only authorized traffic, as defined by the local security by the local policy, will be allowed to pass. The firewall itself is immune to penetration Firewalls can be classified in three categories:\n1: Traditional Packet Filters Packet filters examine each datagram in isolation determining whether the datagram should be allowed to pass or should be dropped based on administrator-specific rules. Filtering decisions can be based on IP source/destination, protocol type, TCP/UDP, TCP flags/ ICMP message type, rules for leaving/entering, rules for different router interfaces. The parameters are based on the policy of the organization taking account of user productivity and bandwidth usage as well as security concerns.\n2: Stateful Packet Filters Decisions are made on each packet in isolation. Stateful filters track TCP connecions and use this knowledge to make filtering decisions.\n3: Application Gateways Application Gateways look beyond the IP/TCP/UDP headers and make policy decisions based on application data. An Application Gateway is an application-specific server through which all application data must pass. Multiple AG can run on the same host, but each gateway is a separate server with its own processes.\n8.9.2 Intrusion Detection Systems An intrusion detection system (IDS) is a device that alerts when it observes potentially malicious traffic. An intrusion prevention system (IPS) is a device that filters out suspicious traffic. Both types of device perform deep packet inspection: they look beyond the header fields and into the actual application data that the packets carry.\nAn IDS can detect a wide range of attacks, including network mapping, port scans, TCP stack scans, DoS, worms, viruses, OS vulnerability attacks and application vulnerability attacks. An organization can deploy one more IDS sensors in its network. When many are used, they work together, usually coordinated by a central server. More than one is often a good solution as each one compare each passing packet with tens of thousands of signatures. They are usually classified as either signature-based systems or anomaly-based systems. A signature based IDS maintains an extensive database of attack signature, each of which being a set of rules pertaining to an intrusion activity. A signature can be a list of packet characteristics or may relate to a series of packets. They are created by network security engineers researching attacks. The ids sniffs every packet passing by it, comparing it with signatures. Signature based IDS, although widely deployed, have a number of limitations: they require a previous knowledge of the attack to generate an accurate signature, false alarms may be generated, they can be slow and fail to detect attacks if overwhelmed. Anomaly-based packets study normal traffic and looks for statistically unusual events. They don\u0026rsquo;t rely on previous knowledge of attacks.\nChapter 5: The Link Layer: Links, Access Networks and LANs 5.1 Introduction to the Link Layer Some terminology:\nnode = any device running a link-layer protocol (hosts, routers, switches\u0026hellip;) link = communication channels connecting adjacent nodes along the path. Over a given link, a transmitting node encapsulates the datagram in a link-layer frame and transmits the frame into the link. 5.1.1 The Services Provided by The Link Layer Possible services offered by a link-layer protocol include:\nFraming: all link layer protocols encapsulate each network layer datagram within a link-layer frame before transmission. A frame consists of a data field, containing the datagram, and a number of header fields, whose structure is determined by the protocol. Link access: A Medium Access Control (MAC) protocol specifies the rules by which a frame is transmitted onto the link. Reliable delivery: the protocol guarantees to move each datagram across the link without loss or errors. A reliable delivery protocol is often used for links highly prone to errors (WiFi) so that the error can be corrected locally, where it happens, rather than forcing an end-to-end retransmission. However it can represent a significant overhead for low bit-error links (cable) and therefore many wired link-layer protocols do not provide a reliable delivery service. Error detection and correction: signal attenuation and electromagnetic noise can introduce errors. Because there is no need to forward a datagram that has an error, may link-layer protocols provide a mechanism to detect such bit errors so that they can drop the frames. This can be accomplished transmitting error-detection bits in the frame. Link layer error detection is usually more sophisticated and implemented in hardware. 5.1.2 Where Is the Link Layer Implemented? In routers, the link layer is implemented in the line card. Is a host\u0026rsquo;s link layer implemented in hardware or software? For the most part, the link layer is implemented in a network adapter, sometimes known as network interface card (NIC). At the heart of the NIC is the link-layer controller, usually a single, special purpose chip that implements many of the link-layer services. Thus, much of a link-layer controller\u0026rsquo;s functionality is implemented in hardware. Part of the link layer is implemented in software that runs on the host\u0026rsquo;s CPU, this part implement higher-level functionalities. Link-Layer is a combination of hardware and software, the place in the protocol stack where software meets hardware.\n5.2 Error-Detection and -Correction Techniques Error detection and correction allow the receiver to sometimes, but not always, detect that bit errors have occurred. Even with the use of error-detection bits, there still may be undetected bit errors (the receiver is unaware of the presence of corrupted bits). We want to keep the probability of such an event small. Let\u0026rsquo;s now consider three techniques for detecting errors in the transmitted data: parity checks, checksumming methods and cyclic redundancy checks\n5.2.1 Parity Checks Perhaps the simplest form of error detection is the use of a single parity bit. Suppose that the information to be sent, D, has d bits. In an even parity scheme, the sender simply includes one additional bit and chooses its value such that the total number of 1s in the d+1 bits (original + parity bit) is even. (odd parity scheme, parity bit to one if #1s % 2 != 0). The receiver only needs to count the number of 1s in the d+1 bits. If an odd number of 1 valued bits are found with an even parity scheme, the receiver knows that some odd number of bit error has occurred. If an even number of bit errors occur, this would result in an undetected error. Another approach is to use a two dimensional even parity: the d bits are divided into i rows and j columns. A parity value is computed for each row and for each column. The result i + j + 1 parity bits comprise the error-detection bits. A single bit error in the original d bits will cause the parity of both the column and the row containing the flipped bit to to be in error. The receiver can not only detect the error, but also use the column and row indices of the column and row with parity errors to actually identify the bit that was corrupted and correct the error. This technique also allows to detect an error in the parity bits. The ability of the receiver to both detect and correct errors is known as forward error correction (FEC)\n5.2.2 Checksumming Methods The d bits of data are treated as a sequence of k-bit integers for example the Internet checksum already studied: bytes of data are treated as integers and summed, the 1s complement of this sum forms the Internet checksum carried in the header. The receiver checks the checksum by taking the 1s complement of the sum of the received data (including checksum) and checking whether the result is all 1 bits, if there are any 0, an error is indicated. In TCP and UDP the checksum is computed over all fields (header and data). Checksumming methods require little packet overhead but they provide relatively weak protection against errors. Why is checksumming used in transport layer and cyclic redundancy check used at the link layer? Transport layer is implemented in software (OS) and therefore needs a simple and fast error detection scheme while error detection at link layer is implemented in hardware which can perform the more complex CRC operations.\n5.2.3 Cyclic Redundancy Check (CRC) Cyclic Redundancy Check (CRC) codes are also known as polynomial codes since it is possible to view the string to be sent as a polynomial whose coefficients are the 0 and 1 values in the bit string with operation interpreted as polynomial arithmetic. Sender and receiver must agree on a r+1 bit pattern know as generator which we\u0026rsquo;ll denote as G. We require the leftmost bit of G to be a 1. For a given piece of data D the sender will choose r additional bits, R, and append them to D such that the resulting d + r bit pattern, interpreted as a binary number, is exactly divisible by G using modulo-2 arithmetic. Checking is therefore easy: the receiver divides the d + r received by bits by G, if the remainder is nonzero, an error has occurred, otherwise the data is accepted as being correct. All CRC calculations are done in modulo 2 without carries in addition or borrows in subtraction (+ = - = xor).\n5.3 Multiple Access Links and Protocols There are two types of network links: point-to-point and broadcast links. A point-to-point link consists of a single sender at one end of the link and a single receiver at the other end of the link. A broadcast link can have multiple sending and receiving nodes all connected to the same, single, shared broadcast channel. The term broadcast is used because when any node transmits a frame, the channel broadcasts the frame and each other node receives a copy (ex: ethernet, wireless).\nThe multiple access problem: How to coordinate the access of multiple sending and receiving nodes to a shared broadcast channel? Computer networks have multiple access protocols by which nodes regulate their transmission into the shared broadcast channel. More than two nodes can transmit frames at the same time, which will result in all of the nodes receiving multiple frames at the same time: the frames collide at all of the receivers. Typically in case of collision, none of the receiving nodes can make any sense of any of the frames, they become inextricably tangled together and are therefore lost, the channel being wasted during collision. Thus it is necessary to coordinate the transmission of the active nodes. We can classify multiple access protocols in three categories: channel partitioning protocols, random access protocols, taking-turns protocols.\n5.3.1 Channel Partitioning Protocols TDM and FDM (from circuit switching) are in this category. A third channel partitioning tool is code division multiple access (CDMA) which assigns a different code to each node. Each node then uses its unique code to encode the data bits it sends. If the codes are chosen carefully, then all nodes can transmit simultaneously and yet have their respective receivers correctly receive a sender\u0026rsquo;s encoded data bits. Originally used in military systems, it\u0026rsquo;s now widely used for civilian use, particularly in cellular telephony.\n5.3.2 Random Access Protocols A transmitting node always transmits at the full rate of the channel, R bps. When there is a collision, each node involved in the collision repeatedly retransmits its frame until the frame gets through without a collision. But when a node experiences a collision, it waits a random dely before retransmitting the frame. The delay is chosen independently. Here a few of the most commonly used random access protocols:\nSlotted ALOHA All frames consist of L bits, time is divided into slots of size L/R seconds, nodes start to transmit frames only at the beginning of slots. Moreover nodes are synchronized so that each node when the slot begins. If two or more frames collide in a slot, then all the nodes detect the collision event before the slot ends.\nIf p is a probability then the operation of slotted ALOHA in each node is simple:\neach node waits the beginning of the next slot to transmit the entire frame in a slot If no collision occurs, the frame is considered delivered If collision, this is detect before the end of the slot. The node retransmits its frame in each subsequent slot with probability p (probability of retransmission) until the frame is transmitted without a collision. Slotted ALOHA allows transmission at full rate R, is highly decentralized, and is extremely simple. The computed maximal efficiency (successfully used slots in transmission / total slots) of Slotted ALOHA) is 37% thus the effective transmission rate is 0.37R bps.\nAloha all nodes synchronize their transmissions to start at the beginning of a slot. The node immediately transmits a frame in its entirety in the channel. In case of collision, the node will then immediately retransmit the frame with probability p otherwise the node waits for a frame transmission time, after which it transmits the frame with probability p or wait for another frame with probability 1-p. The maximum efficiency is 1/(2e) but the protocol is fully decentralized.\nCarrier Sense Multiple Access (CSMA) CSMA and CSMA/CD (collision detection) embody two rules:\ncarrier sensing: if a node is transmitting, the others wait until they detect no transmission for a short amount of time and begin transmission. collision detection: a transmitting node listens to the channel while it\u0026rsquo;s transmitting, if it detects that another node is transmitting, it stops transmitting and waits for a random amount of time before repeating the sense-and-transmit-when-idle-cycle. It is evident that the propagation delay of the channel plays a crucial role: the longer, the larger the chance that a carrier sensing node is not yet able to sense a transmission that has already begun.\nCarrier Sense Multiple Access with Collision Detection (CSMA/CD) When a node detects a collision, it ceases transmission immediately in Collision Detection. A link layer frame is prepared, if the node senses that the channel is idle (no energy is entering the adapter from the channel), it starts to transmit the frame, else it waits until it detects idle. While transmitting, the node monitors the channel for usage from other nodes, if the entire frame is transmitted without detecting usage, then the adapter is finished. If energy is detected from other adapters while transmitting, the node aborts transmission (stops), waits for a random amount of time and then returns to checking for idle.\nThe wait for random amount of time is required in order to avoid the nodes to keep colliding.\nCSMA/CD Efficiency Is the long run fraction of time during which frames are being transmitted without collision. If the propagation delay approaches 0, the efficiency approaches 1. Also if the propagation delay becomes very large, efficiency approaches 1.\n5.3.3 Taking-Turns Protocols There are a lot of them, we\u0026rsquo;ll cover two of the more important, the first one being the polling protocol. It requires one of the nodes to be designated as a master node which polls each of the nodes in a round-robin fashion. The master tells node 1 that it can transmit up to some maximum number of frames, when node 1 is finished (the master checks for energy in the channel) the master tells the same to node 2 and so on. The polling protocol eliminates the collisions and empty slots that plague random access protocols, resulting in a much higher efficiency. However it introduces a polling delay (the amount of time required to notify a node that it can transmit) [if only one is transmitting, it will have to wait for the master to poll all the others]. Moreover the master node represents a single point of failure.\nThe second protocol is the token-passing protocol in which there is no master method. A small, special purpose frame known as token is exchanged among the nodes in some fixed order. When a node receives a toke, it holds it only if it has some frames to transmit otherwise it immediately forwards it to the next node.If a node has frames to transmit when it receives the token, it sends up to a maximum number of frames and then passes the token. Token passing is decentralized and highly efficient but the failure of one node could crash the entire channel, or a node could neglect to release the token\u0026hellip;.\n5.3.4 DOCSIS: The Link-Layer Protocol for Cable Internet Access The Data-Over-Cable-Service-Interface-Specifications specifies the cable data network architecture and its protocols. DOCSIS uses FDM to divide the downstream and upstream network segments into multiple frequency channels. Each upstream and downstream channel is a broadcast channel. Several cable modems share the same upstream channel (frequency) to the CMTS and thus collision can potentially occur. Each upstream channel is divided into intervals of time (TDM-like) each containing a sequence of mini-slots during which cable modems can transmit to the CMTS, which explicitly grants permission to individual modems to transmit during specific mini-slots. This is done sending a special control message known as a MAP message on a downstream channel to specify which cable modem can transmit during which mini-slot. Modems send mini-slot-request frames to the CMTS during a special set of interval mini-slots dedicated for this purpose. The requests are transmitted in a random access manner and may collide with each other. The modem cannot detect activity nor collisions: it simply infers that its request experienced collision if it does not receive a response in the next downstream control message. When a collision is inferred, a modem uses binary exponential backoff to defer the transmission to a future slot.\n5.4 Switched Local Area Networks Switched local networks connect hosts using link-layer switches which do not run networks-layer protocols.\n5.4.1 Link-Layer Addressing and ARP MAC Addresses Network interfaces in hosts and routers have link-layer addresses, however link-layer switches do not have link-layer addresses associated with their interfaces so that they can carry datagrams without having routers or hosts having to explicitly address the frame to the intervening switch. A link-layer address is called LAN address, physical address or MAC address, the last name being the most popular. This address is 6 bytes long, typically expressed in hexadecimal notation. They are supposed to be permanent but can be changed via software. No two adapters have the same address: the IEEE manages the MAC address space, usually assigning a 24 prefix to each manufacturer and letting him choose the content of the remaining 24 bits. MAC address have a flat structure (no hierarchy such as in IP) and do not change. When an adapter wants to send a frame to some destination adapter, it inserts the destination adapter\u0026rsquo;s MAC address into the frame and then sends the frame into the LAN. An adapter might receive a frame that isn\u0026rsquo;t addressed to it, when this happens, the adapter checks whether the frame\u0026rsquo;s destination address matches its own, if not it discards the frame. When a sending adapter want to broadcast to the whole network, it inserts a special MAC broadcast address into the destination address field, for 6bytes addresses that is FF-FF-FF-FF-FF-FF\nAddress Resolution Protocol (ARP) The Address Resolution Protocol (ARP) translates network-layer addresses into link-layer addresses, analogously to DNS, but ARP resolves IP addresses only for hosts and router interfaces on the same subnet. Each host and router has an ARP table which contain mappings of IP addresses to MAC addresses and a time-to-live TTL value which indicates when each mapping will be deleted from the table. A typical TTL is 20 minutes from when an entry is placed in the ARP table. The table does not necessarily contain an entry for every host and router on the subnet. What if a frame has to be sent to an address which does not appear in the table? The sender creates a special packet, an ARP packet, containing the sending and receiving IP and MAC addresses. Both ARP query and response have the same format: the sending forwards the ARP request to the broadcast address (destination address) the frame containing the query is received by all the other adapters in the subnet. Each adapter passes the frame to the ARP module which checks if its IP address matches the destination IP address in the query. The one with a match sends back the response with the desired mapping. The querying can update its table and send the IP datagram encapsulated in a link-layer frame. ARP is plug and play: the table gets build automatically. ARP stands in the boundary between the link and network layers.\nSending a Datagram off the Subnet A datagram that has to be sent out of the subnet is first sent to the first-hop router on the path to the final destination (which is outside the subnet). How is its MAC acquired? Using ARP. When the frame reaches the next-hop router of the destination subnet, it has to be moved inside, the router having to decide what interface to use. This is done using the forwarding table: the router extracts the datagram and checks the destination IP. The datagram is encapsulated again and sent into the subnet, this time the MAC address of the frame is indeed the destination MAC address of the ultimate destination, which the router acquire via ARP.\n5.4.2 Ethernet It has pretty much taken over the wired LAN market. Since its invention in the 70\u0026rsquo;s, it has grown and become faster. At the beginning the original Ethernet LAN used a coaxial bus to interconnect the nodes, creating a broadcast LAN. By the late 90s, most companies and universities had replaces their LANs with Ethernet installation using a hub-based star topology: hosts and routers are directly connected to a hub with twisted-pair copper wire. A hub is a physical layer device that acts on individual bits rather than frames. When a hub receives a bit, it simply recreates it boosting its energy strength and transmits the bit onto all the other interfaces (it\u0026rsquo;s still a broadcast LAN). In the early 2000s, the star topology evolved: the hub was replaced with a switch, allowing a collision-less LAN.\nEthernet Frame Structure Data fields (46 to 1,500 bytes): carries the IP datagram (or other network-layer datagram). The MTU (maximum transmission unit) is 1500 bytes, compensated with fragmentation. The minimum is 46, is less, the data is \u0026ldquo;stuffed\u0026rdquo; and the receiving network layer uses the length field to eliminate the stuffing Destination address (6 bytes) destination MAC address. Source address (6 bytes) Type field (2 bytes) allows to multiplex network layer protocols (if not only IP is used, also ARP has its own type number 0x0806) Cyclic redundant check (CRC) (4 bytes): used for bit error detection Preamble (8 bytes): the first seven have value 10101010, the last has value 10101011. The first seven serve as \u0026ldquo;wake up\u0026rdquo; the receiving side and to synchronize their clocks to that of the sender\u0026rsquo;s clock the two 1s at the end of byte 8 alerts the receiver that the important stuff is about to come. All of the Ethernet technologies provide connectionless service (no handshaking, similar to UDP) and unrealiable service to the network layer (no ACK, drop in case of errors) which help to make Ethernet simple and cheap. If there are gaps due to discarded Ethernet frames, the fact that the application sees the gaps or not depends on the transport layer protocol used: not with TCP (reliable data transfer), yes with UDP.\nEthernet Technologies There are many variants and flavors of Ethernet which have been standardized over the years by the IEEE. They vary in speed: 10 Megabit, 100 Megabit, 1000 Megabit, 10 Gigabit\u0026hellip; They can also vary in the type of traffic they can transport\u0026hellip;.\n5.4.3 Link-Layer Switches Switch receive and forward frames. They are transparent: adapters address each other, without knowing that the switch is sitting in the middle. As they\u0026rsquo;re output rate might be smaller than the input rate, they also have buffers to queue frames.\nForwarding and Filtering Filtering is the switch function that determines whether a frame should be forwarded to some interface or should just be dropped. Forwarding is the switch function that the determines the interfaces to which a frame should be directed and then moves the frame to those interfaces. Switch filtering and forwarding are done with a switch table which contains entries for some (not necessarily all) of the hosts and routers on a LAN. Each entry contains: (MAC address, interface leading toward that MAC, time at which the entry was placed in the table) Switches forward frames based on the MAC addresses rather than on IP addresses.\nWhen a switch receives a frame:\nThere is no entry in the table associated with the destination address -\u0026gt; the packet is broadcast through all the interfaces (except the one through which the frame was received) There is an entry in the table that point to the same interface through which the frame was received -\u0026gt; The frame is discarded (filtering) There is an entry in the table that point to an interface different from the one through which the frame was received -\u0026gt; the frame is put in the output buffer preceding the interface discovered thanks to the table (forwarding) Self-Learning The switch table is build automatically, dynamically and autonomously without any intervention from a network administrator: switches are self learning.\nThe switch table is initially empty For each incoming frame, the switch stores in its table the MAC address in the frame\u0026rsquo;s source address field the interface from which the frame arrived the current time The switch deletes an address in the table if no frame are received with that address as the source after some period (aging time) so that to eliminate unused entries from the table Thus switches are plug-and-play devices: they require no human intervention. Switches are also full-duplex, meaning any interface can send and receive at the same time.\nProperties of Link-Layer Switching Advantages over buses or hubs:\nElimination of collisions: the switch buffers frames and never transmit more than one frame on a segment at any one time. The maximum aggregated throughput is the sum of all the switch interface rates Heterogeneous links: The switch providing isolation, different links can operate at different speeds and run over different media. Therefore switches are ideal for mixing legacy equipment with new equipment. Management: A switch can disconnect a malfunctioning adapter and a cut cable isolates only one host. Switches can gather statistics useful for debugging and planning the evolution of the network. Switches Versus Routers They are both packet switches but switches are layer-2 packet switches while routers are layer-3 packet switches. Switches are plug-and-play, have relatively high filtering and forwarding rates. However to prevent the cycling of broadcast frames, the active topology of a swtiched network is restricted to a spanning tree. A large network requires large ARP tables in hosts and routers and would generate substantial ARP traffic and processing. Switches are also susceptible to broadcast storms: if one goes crazy and send an endless stream of broadcast frames, the others will forward all of the frames resulting in a network collapse. Routers network addressing is hierarchical, packets do not normally cycle and the topology is not limited to a spanning tree even when the network has redundant paths. Therefore packets can use the best path between source and destination. But routers are not plug-and-play (a host need the IP to connect) and often have a larger per-packet processing time than switches. Finally two pronunciation cause a lot of disputes.\nPDF Note👇 ","permalink":"https://samirpaul1.github.io/blog/posts/computer-networks-notes/","summary":"Computer Networks Notes","title":"Computer Networks Notes"},{"content":"Project Based Learning A list of programming tutorials in which aspiring software developers learn how to build an application from scratch. These tutorials are divided into different primary programming languages. Tutorials may involve multiple technologies and languages.\nTable of Contents: C# C/C++ Clojure Dart Elixir Erlang F# Go Haskell HTML/CSS Java JavaScript Kotlin Lua OCaml PHP Python R Ruby Rust Scala Swift Additional resources C/C++: Build an Interpreter (Chapter 14 on is written in C) Memory Allocators 101 - Write a simple memory allocator Write a Shell in C Write a FUSE Filesystem Build Your Own Text Editor Build Your Own Lisp How to Program an NES Game in C Write an OS from scratch How to create an OS from scratch Building a CHIP-8 Emulator Beginning Game Programming with C++ and SDL Implementing a Key-Value Store Tiny 3D graphics projects Tiny Renderer or how OpenGL works: software rendering in 500 lines of code Understandable RayTracing in 256 lines of bare C++ KABOOM! in 180 lines of bare C++ 486 lines of C++: old-school FPS in a weekend Writing a minimal x86-64 JIT compiler in C++ Part 1 Part 2 Build a Live Code-reloader Library for C++ Write a hash table in C Let\u0026rsquo;s Build a Simple Database Let\u0026rsquo;s Write a Kernel Write a Bootloader in C Linux Container in 500 Lines of Code Write Your Own Virtual Machine Learning KVM - Implement Your Own Linux Kernel Write a C compiler Part 1: Integers, Lexing and Code Generation Part 2: Unary Operators Part 3: Binary Operators Part 4: Even More Binary Operators Part 5: Local Variables Part 6: Conditionals Part 7: Compound Statements Part 8: Loops Part 9: Functions Part 10: Global Variables Implementing a Language with LLVM Meta Crush Saga: a C++17 compile-time game High-Performance Matrix Multiplication Space Invaders from Scratch Part 1 Part 2 Part 3 Part 4 Part 5 Tetris Tutorial in C++ Platform Independent Writing a Linux Debugger Part 1: Setup Part 2: Breakpoints Part 3: Registers and memory Part 4: Elves and dwarves Part 5: Source and signals Part 6: Source-level stepping Part 7: Source-level breakpoints Part 8: Stack unwinding Part 9: Handling variables Part 10: Advanced topics Let\u0026rsquo;s write a compiler Part 1: Introduction, selecting a language, and doing some planning Part 2: A lexer Part 3: A parser Part 4: Testing Part 5: A code generator Part 6: Input and output Part 7: Arrays Part 8: Strings, forward references, and conclusion Network programming Let\u0026rsquo;s Code a TCP/IP Stack\nPart 1: Ethernet \u0026amp; ARP Part 2: IPv4 \u0026amp; ICMPv4 Part 3: TCP Basics \u0026amp; Handshake Part 4: TCP Data Flow \u0026amp; Socket API Part 5: TCP Retransmission Programming concurrent servers\nPart 1 - Introduction Part 2 - Threads Part 3 - Event-driven Part 4 - libuv Part 5 - Redis case study Part 6 - Callbacks, Promises and async/await MQTT Broker from scratch\nPart 1 - The protocol Part 2 - Networking Part 3 - Server Part 4 - Data structures Part 5 - Topic abstraction Part 6 - Handlers Bonus - Multithreading OpenGL: Creating 2D Breakout game clone in C++ with OpenGL Breakout Setting up Rendering Sprites Levels Collisions Ball Collision detection Collision resolution Particles Postprocessing Powerups Audio Render text Final thoughts Handmade Hero How to Make Minecraft in C++/OpenGL (video) C#: Learn C# By Building a Simple RPG Game Create a Rogue-like game in C# Create a Blank App with C# and Xamarin (work in progress) Build iOS Photo Library App with Xamarin and Visual Studio Building the CoreWiki This is a Wiki-style content management system that has been completely written in C# with ASP.NET Core and Razor Pages. You can find the source code here. Clojure: Build a Twitter Bot with Clojure Building a Spell-Checker Building a JIRA integration with Clojure \u0026amp; Atlassian Connect Prototyping with Clojure Tetris in ClojureScript Dart: Flutter: Amazon Clone with Admin Panel Food Delivery App Google Docs Clone Instagram Clone Multiplayer TicTacToe Game TikTok Clone Ticket Booking App Travel App Twitch Clone WhatsApp Clone Wordle Clone Zoom Clone Elixir Building a Simple Chat App With Elixir and Phoenix How to write a super fast link shortener with Elixir, Phoenix, and Mnesia Erlang ChatBus : build your first multi-user chat room app with Erlang/OTP Making a Chat App with Erlang, Rebar, Cowboy and Bullet F#: Write your own Excel in 100 lines of F# Java: Build an Interpreter (Chapter 4-13 is written in Java) Build a Simple HTTP Server with Java Build an Android Flashlight App (video) Build a Spring Boot App with User Authentication JavaScript: Build 30 things in 30 days with 30 tutorials Build an App in Pure JS Build a Jupyter Notebook Extension Build a TicTacToe Game with JavaScript Build a Simple Weather App With Vanilla JavaScript Build a Todo List App in JavaScript HTML and CSS: Build A Loading Screen Build an HTML Calculator with JS Build Snake using only JavaScript, HTML \u0026amp; CSS Mobile Application: Build a React Native Todo Application Build a React Native Application with Redux Thunk Web Applications: React: Create Serverless React.js Apps Create a Trello Clone Create a Character Voting App with React, Node, MongoDB and SocketIO React Tutorial: Cloning Yelp Build a Full Stack Movie Voting App with Test-First Development using Mocha, React, Redux and Immutable Build a Twitter Stream with React and Node Build A Simple Medium Clone using React.js and Node.js Integrate MailChimp in JS Build A Chrome Extension with React + Parcel Build A ToDo App With React Native Make a Chat Application Create a News App with React Native Learn Webpack For React Testing React App With Puppeteer and Jest Build Your Own React Boilerplate Code The Game Of Life With React A Basic React+Redux Introductory Tutorial Build an Appointment Scheduler Build A Chat App with Sentiment Analysis Build A Full Stack Web Application Setup Create Todoist clone with React and Firebase Build A Random Quote Machine Part 1 Part 2 Part 3 Part 4 Part 5 Part 6 Part 7 React Phone E-Commerce Project(video) Angular: Build an Instagram Clone with Angular 1.x\nBuild an offline-capable Hacker News client with Angular 2+\nPart 1 Part 2 Build a Google+ clone with Django and AngularJS (Angular 1.x)\nBuild A Beautiful Real World App with Angular 8 :\nPart I Part II Build Responsive layout with BootStrap 4 and Angular 6\nToDo App with Angular 5\nIntroduction to Angular Part 1 Node: Build a real-time Markdown Editor with NodeJS Test-Driven Development with Node, Postgres and Knex Write a Twitter Bot in Node.js Part 1 Part 2 Build A Simple Search Bot in 30 minutes Build A Job Scraping Web App Building a GitHub App How to build your own Uber-for-X App using JavaScript, Node.JS, MongoDB and Web Sockets Part 1 Part 2 Vue Vue 2 + Firebase: How to build a Vue app with Firebase authentication system in 15 minutes Vue.js Application Tutorial – Creating a Simple Budgeting App with Vue Build a Blog with Vue, GraphQL and Apollo Build a full stack web application using MEVN (MongoDB, Express, Vue, Node) stack Part 1 Part 2 Vue.js To-Do List Tutorial (video) Vue 2 + Pub/Sub: Build a peer to peer multi-user platform for games Others (Hapi, Express\u0026hellip;): Build a Progressive Web Application (PWA) Part 1 Part 2 Part 3 Build A Native Desktop App with JS Build a Powerful API with NodeJs,GraphQL and Hapi Part I D3.js Learn D3 using examples Learn To Make A Line Chart Game Development: Make 2D Breakout Game using Phaser Make Flappy Bird in HTML5 and JavaScript with Phaser Part 1 Part 2 Desktop Application: Build A Desktop Chat App with React and Electron Miscellaneous: How to Build a Web Framework in Less Than 20 Lines of Code Build Yourself a Redux How to write your own Virtual DOM Build A Realtime Serverless GraphQL API with WebSockets on AWS Kotlin: Keddit - Learn Kotlin While Developing an Android Application Lua: LÖVE: BYTEPATH: Creation of a Complete Game with Lua and LÖVE Part 0: Introduction Part 1: Game Loop Part 2: Libraries Part 3: Rooms and Areas Part 4: Exercises Part 5: Game Basics Part 6: Player Basics Part 7: Player Stats and Attacks Part 8: Enemies Part 9: Director and Gameplay Loop Part 10: Coding Practices Part 11: Passives Part 12: More Passives Part 13: Skill Tree Part 14: Console Part 15: Final Python: Web Scraping: Mining Twitter Data with Python Scrape a Website with Scrapy and MongoDB How To Scrape With Python and Selenium WebDriver Which Movie Should I Watch using BeautifulSoup Web Applications: Build a Microblog with Flask Create a Blog Web App In Django Choose Your Own Adventure Presentations Build a Todo List with Flask and RethinkDB Build a Todo List with Django and Test-Driven Development Build a RESTful Microservice in Python Microservices with Docker, Flask, and React Build A Simple Web App With Flask Create A Django API in under 20 minutes Build a Community-driven delivery application with Django, Postgres and JavaScript Part 1 Part 2 Realtime Chat application with Vue, django-notifs, RabbitMQ and uWSGI Part 1 Part 2 Part 3 Part 4 Part 5 Part 6 Bots: Build a Reddit Bot How to Make a Reddit Bot - YouTube (video) Build a Facebook Messenger Bot Making a Reddit + Facebook Messenger Bot How To Create a Telegram Bot Using Python Part 1 Part 2 Create a Twitter Bot In Python Data Science: Learn Python For Data Science by Doing Several Projects (video): Part 1: Introduction Part 2: Twitter Sentiment Analysis Part 3: Recommendation Systems Part 4: Predicting Stock Prices Part 5: Deep Dream in TensorFlow Part 6: Genetic Algorithms Machine Learning: Write Linear Regression From Scratch in Python (video) Step-By-Step Machine Learning In Python Predict Quality Of Wine Solving A Fruits Classification Problem Learn Unsupervised Learning with Python Build Your Own Neural Net from Scratch in Python Linear Regression in Python without sklearn Multivariate Linear Regression without sklearn Music Recommender using KNN Find Similar Quora Questions- Using BOW, TFIDF and Xgboost Using Word2Vec and Xgboost Detecting Fake News with Python and Machine Learning OpenCV: Build A Document Scanner Build A Face Detector using OpenCV and Deep Learning Build fastest custom object Detection system yusing YOLOv3(video playlist) Build a Face Recognition System using OpenCV, Python and Deep Learning Detect The Salient Features in an Image Build A Barcode Scanner Learn Face Clustering with Python Object Tracking with Camshift Semantic Segmentation with OpenCV and Deep Learning Text Detection in Images and Videos People Counter using OpenCV Tracking Multiple Objects with OpenCV Neural Style Transfer with OpenCV OpenCV OCR and Text Recognition Text Skew Correction Tutorial Facial Landmark Detection Tutorial Object Detection using Mask-R-CNN Automatic Target Detection Tutorial EigenFaces using OpenCV Faster(5-point) Facial Landmark Detection Tutorial Hand Keypoint Detection Dlib Correlation Object Tracking - Single Object Tracker Mutiple Object Tracker Image Stitching with OpenCV and Python Instance Segmentation with OpenCV Face mask detector Deep Learning: Using Convolutional Neural Nets to Detect Facial Keypoints Generate an Average Face using Python and OpenCV Break A Captcha System using CNNs Use pre-trained Inception model to provide image predictions Create your first CNN Build A Facial Recognition Pipeline Build An Image Caption Generator Make your Own Face Recognition System Train a Language Detection AI in 20 minutes Object Detection With Neural Networks Learn Twitter Sentiment Analysis - Part I - Data Cleaning Part II - EDA, Data Visualisation Part III - Zipf\u0026rsquo;s Law, Data Visualisation Part IV - Feature Extraction(count vectoriser) Part V - Feature Extraction(Tfidf vectoriser) Part VI - Doc2Vec Part VII - Phrase Modeling + Doc2Vec Part VIII - Dimensionality Reduction Part IX - Neural Nets with Tfdif vectors Part X - Neural Nets with word2vec/doc2vec Part XI - CNN with Word2Vec Use Transfer Learning for custom image classification Learn to Code a simple Neural Network in 11 lines of Python Build a Neural Network using Gradient Descent Approach Train a Keras Model To Generate Colors Get Started with Keras on a Custom Dataset Use EigenFaces and FisherFaces on Faces94 dataset Kaggle MNIST Digit Recognizer Tutorial Fashion MNIST tutorial with tf.keras CNN using Keras to automatically classify root health Keras vs Tensorflow Deep Learning and Medical Image Analysis for Malaria Detection Transfer Learning for Image Classification using Keras Code a Smile Classifier using CNNS in Python Natural Language Processing using scikit-learn Code a Taylor Swift Lyrics Generator Mask detection using PyTorch Lightning Miscellaneous: Build a Simple Interpreter Build a Simple Blockchain in Python Write a NoSQL Database in Python Building a Gas Pump Scanner with OpenCV/Python/iOS Build a Distributed Streaming System with Python and Kafka Writing a basic x86-64 JIT compiler from scratch in stock Python Making a low level (Linux) debugger Part 1 Part 2: C Implementing a Search Engine Part 1 Part 2 Part 3 Build the Game of Life Create terminal ASCII art Write a Tic-Tac-Toe AI Create photomosaic art Build the game \u0026ldquo;Snake\u0026rdquo; in the terminal Write yourself a Git A Python implementation of a Python bytecode runner Create a Voice assistant using Python Go: Create a Real Time Chat App with Golang, Angular 2, and WebSocket Building Go Web Applications and Microservices Using Gin How to Use Godog for Behavior-driven Development in Go Building Blockchain in Go Part 1: Basic Prototype Part 2: Proof of Work Part 3: Persistence and CLI Part 4: Transactions 1 Part 5: Address Part 6: Transactions 2 Part 7: Network Building a container from scratch in Go - Liz Rice (Microscaling Systems)(video) Build Web Application with GoLang Building a Chat Application in Go with ReactJS Part 1: Initial Setup Part 2: Simple Communication Part 3: Designing our Frontend Part 4: Handling Multiple Clients Part 5: Improving the Frontend Part 6: Dockerizing your Backend Go WebAssembly Tutorial - Building a Calculator Tutorial REST Servers in Go Part 1 - standard library Part 2 - using a router package Part 3 - using a web framework Part 4 - using OpenAPI and Swagger Part 5 - middleware Part 6 - authentication Part 7 - GraphQL Let\u0026rsquo;s build a URL shortener in Go - with Gin \u0026amp; Redis Part 1 - Project setup Part 2 - Storage Layer Part 3 - Short Link Generator Part 4 - Forwarding Building a TCP Chat in Go(video) Building a BitTorrent client from the ground up in Go REST API masterclass with Go, PostgreSQL and Docker(video playlist)in progress PHP: How To Build A Blog With Laravel (video) Make Your Own Blog (in Pure PHP) Build A Real Estate Website Example with SilverStripe Building Realtime Chat App with Laravel 5.4 and VueJS (video) Build A Social Network: Laravel 5 - Youtube (video) Build a full-featured multi-tenant app with Laravel Part 0: Introduction Part 1: Setup Part 2: Roles and Permissinos Part 3: Invitation Part 4: Authentication Part 5: Testing Part 6: User Profile Part 7: Deployment Build a Laravel CRUD Application From Scratch OCaml: Implement a Language with LLVM in OCaml Ruby: Build a Network Stack with Ruby Build your own Redis Part 0: Introduction Part 1: Barebones TCP Server Part 2: PING \u0026lt;-\u0026gt; PONG Part 3: Concurrent Clients Part 4: ECHO Rebuilding Git in Ruby Ruby on Rails: The Ruby on Rails Tutorial Build Instagram From Scratch with Ruby on Rails Build a Social Network using Rails How To Build a Ruby on Rails Application Haskell: Write You a Haskell - Build a modern functional compiler Write Yourself a Scheme in 48 hours Write You A Scheme, Version 2 Roll Your Own IRC Bot Making Movie Monad Making a Website with Haskell (outdated) R: Build Web Apps with Shiny Build A Cryptocurrency Bot Learn Associate Rule Mining in R Rust: A Simple Web App in Rust Part 1 Part 2a Part 2b Write an OS in pure Rust Build a browser engine in Rust Write a Microservice in Rust Learning Rust with Too Many Linked Lists Rust in Detail: Writing Scalable Chat Service from Scratch Part 1: Implementing WebSocket. Introduction. Part 2: Sending and Receiving Messages Writing a Rust Roguelike for the Desktop and the Web Single Page Applications using Rust Writing NES Emulator in Rust Create a simulation of evolution using neural network and genetic algorithm, and compile the application to WebAssembly Part 1 Part 2 Part 3 Part 4 Scala: Simple actor-based blockchain No Magic: Regular Expressions Swift: Hacking with Swift - Learn Swift by doing 39 projects Retro first-person shooter from scratch Additional Resources React Redux Links Udemy.com Full Stack Python Node School ScotchIO Exercism Egghead.io Michael Herman\u0026rsquo;s Blog Thinkster.io Enlight Hack Club Workshops CodeCrafters ","permalink":"https://samirpaul1.github.io/blog/posts/curated-list-of-project-based-tutorials/","summary":"Curated List of Project Based Tutorials","title":"Curated List of Project Based Tutorials"},{"content":" In this repository, I have stored solutions to various problems and concepts of Data Structures and Algorithms in Python3 in a structured manner.\nTopics Covered: Dynamic Programming Sorting Algorithms LinkedList Object-Oriented Programming Binary Trees Graph Algorithms Heap Matrix Trie Binary Search Backtracking Stack Queue Greedy String Bit Manipulation Array HashMap DFS BFS Two Pointers Math Recursion In various folders of the above topics, you can find questions and concepts related to that topic.\nIn the Dynamic Programming section, you can find all the questions covered and not covered in Aditya Verma\u0026rsquo;s dynamic programming playlist folder-wise with my handwritten notes.✍️\nIf you are preparing for an interview from Striver’s SDE Sheet then the 30-Days-SDE-Sheet-Practice will be helpful to you. Here I have stored solutions to questions of each day with short notes to each solution, as short notes about the approach are very helpful during revision.🎯\nIn the Questions-Sheet directory, you can find questions asked by top product-based companies.\nThere is a collection of books and pdfs on various important computer science fundamentals in the BOOKS-and-PDFs directory.📚\nI am continuously trying to improve this repository by adding new questions and concepts related to the respective topic. Please feel free to contribute to this repository.💻\nView this repository with improved user experience▶️https://samirpaul.in/DSAlgo🚀 Things you can contribute to:\nUpdate the existing solution with a better one (better complexity). Add new questions and solutions in Python3 to the respective directory. Add new resources to BOOKS-and-PDFs \u0026amp; Questions-Sheet. Solve issues raised by other people or yourself. Provide well-documented source code with detailed explanations. List of Important Questions:✨ The following list of questions was recommended by Love Babbar on this video. I have documented all those questions here.✌️\nTopic Important DSA Questions Link Topic: Problem: Related Link \u0026lt;-\u0026gt; Array Reverse the array \u0026lt;-\u0026gt; Array Find the maximum and minimum element in an array \u0026lt;-\u0026gt; Array Find the \u0026ldquo;Kth\u0026rdquo; max and min element of an array \u0026lt;-\u0026gt; Array Given an array which consists of only 0, 1 and 2. Sort the array without using any sorting algo \u0026lt;-\u0026gt; Array Move all the negative elements to one side of the array \u0026lt;-\u0026gt; Array Find the Union and Intersection of the two sorted arrays. \u0026lt;-\u0026gt; Array Write a program to cyclically rotate an array by one. https://leetcode.com/problems/rotate-array/ Array find Largest sum contiguous Subarray [V. IMP] https://leetcode.com/problems/maximum-subarray/ Array Minimise the maximum difference between heights [V.IMP] https://leetcode.com/problems/smallest-range-ii/ Array Minimum no. of Jumps to reach end of an array https://leetcode.com/problems/jump-game Array find duplicate in an array of N+1 Integers \u0026lt;-\u0026gt; Array Merge 2 sorted arrays without using Extra space. \u0026lt;-\u0026gt; Array Kadane\u0026rsquo;s Algorithm https://leetcode.com/problems/maximum-subarray/ Array Merge Intervals \u0026lt;-\u0026gt; Array Next Permutation \u0026lt;-\u0026gt; Array Count Inversion \u0026lt;-\u0026gt; Array Best time to buy and Sell stock \u0026lt;-\u0026gt; Array find all pairs on integer array whose sum is equal to given number \u0026lt;-\u0026gt; Array find common elements In 3 sorted arrays \u0026lt;-\u0026gt; Array Rearrange the array in alternating positive and negative items with O(1) extra space \u0026lt;-\u0026gt; Array Find if there is any subarray with sum equal to 0 https://leetcode.com/problems/subarray-sum-equals-k/ Array Find factorial of a large number \u0026lt;-\u0026gt; Array find maximum product subarray \u0026lt;-\u0026gt; Array Find longest coinsecutive subsequence \u0026lt;-\u0026gt; Array Given an array of size n and a number k, fin all elements that appear more than \u0026quot; n/k \u0026quot; times. \u0026lt;-\u0026gt; Array Maximum profit by buying and selling a share atmost twice \u0026lt;-\u0026gt; Array Find whether an array is a subset of another array \u0026lt;-\u0026gt; Array Find the triplet that sum to a given value \u0026lt;-\u0026gt; Array Trapping Rain water problem \u0026lt;-\u0026gt; Array Chocolate Distribution problem \u0026lt;-\u0026gt; Array Smallest Subarray with sum greater than a given value \u0026lt;-\u0026gt; Array Three way partitioning of an array around a given value \u0026lt;-\u0026gt; Array Minimum swaps required bring elements less equal K together \u0026lt;-\u0026gt; Array Minimum no. of operations required to make an array palindrome \u0026lt;-\u0026gt; Array Median of 2 sorted arrays of equal size \u0026lt;-\u0026gt; Array Median of 2 sorted arrays of different size \u0026lt;-\u0026gt; Array Subarray Sums Divisible by K Array Continuous Subarray Sum \u0026lt;-\u0026gt; \u0026lt;-\u0026gt; Matrix Spiral traversal on a Matrix \u0026lt;-\u0026gt; Matrix Search an element in a matriix \u0026lt;-\u0026gt; Matrix Find median in a row wise sorted matrix \u0026lt;-\u0026gt; Matrix Find row with maximum no. of 1\u0026rsquo;s \u0026lt;-\u0026gt; Matrix Print elements in sorted order using row-column wise sorted matrix \u0026lt;-\u0026gt; Matrix Largest Rectangle in Histogram Matrix Maximum size rectangle https://practice.geeksforgeeks.org/problems/max-rectangle/1 Matrix Find a specific pair in matrix \u0026lt;-\u0026gt; Matrix Rotate matrix by 90 degrees \u0026lt;-\u0026gt; Matrix Kth smallest element in a row-cpumn wise sorted matrix \u0026lt;-\u0026gt; Matrix Common elements in all rows of a given matrix \u0026lt;-\u0026gt; String Reverse a String \u0026lt;-\u0026gt; String Check whether a String is Palindrome or not \u0026lt;-\u0026gt; String Find Duplicate characters in a string \u0026lt;-\u0026gt; String Why strings are immutable in Java? \u0026lt;-\u0026gt; String Write a Code to check whether one string is a rotation of another \u0026lt;-\u0026gt; String Write a Program to check whether a string is a valid shuffle of two strings or not \u0026lt;-\u0026gt; String Count and Say problem \u0026lt;-\u0026gt; String Write a program to find the longest Palindrome in a string.[ Longest palindromic Substring] \u0026lt;-\u0026gt; String Find Longest Recurring Subsequence in String \u0026lt;-\u0026gt; String Print all Subsequences of a string. \u0026lt;-\u0026gt; String Print all the permutations of the given string \u0026lt;-\u0026gt; String Split the Binary string into two substring with equal 0’s and 1’s \u0026lt;-\u0026gt; String Word Wrap Problem [VERY IMP]. \u0026lt;-\u0026gt; String EDIT Distance [Very Imp] \u0026lt;-\u0026gt; String Find next greater number with same set of digits. [Very Very IMP] \u0026lt;-\u0026gt; String Balanced Parenthesis problem.[Imp] \u0026lt;-\u0026gt; String Word break Problem[ Very Imp] \u0026lt;-\u0026gt; String Rabin Karp Algo \u0026lt;-\u0026gt; String KMP Algo \u0026lt;-\u0026gt; String Convert a Sentence into its equivalent mobile numeric keypad sequence. \u0026lt;-\u0026gt; String Minimum number of bracket reversals needed to make an expression balanced. \u0026lt;-\u0026gt; String Count All Palindromic Subsequence in a given String. \u0026lt;-\u0026gt; String Count of number of given string in 2D character array \u0026lt;-\u0026gt; String Search a Word in a 2D Grid of characters. \u0026lt;-\u0026gt; String Boyer Moore Algorithm for Pattern Searching. \u0026lt;-\u0026gt; String Converting Roman Numerals to Decimal \u0026lt;-\u0026gt; String Longest Common Prefix \u0026lt;-\u0026gt; String Number of flips to make binary string alternate \u0026lt;-\u0026gt; String Find the first repeated word in string. \u0026lt;-\u0026gt; String Minimum number of swaps for bracket balancing. \u0026lt;-\u0026gt; String Find the longest common subsequence between two strings. \u0026lt;-\u0026gt; String Program to generate all possible valid IP addresses from given string. \u0026lt;-\u0026gt; String Write a program tofind the smallest window that contains all characters of string itself. \u0026lt;-\u0026gt; String Rearrange characters in a string such that no two adjacent are same \u0026lt;-\u0026gt; String Minimum characters to be added at front to make string palindrome \u0026lt;-\u0026gt; String Given a sequence of words, print all anagrams together \u0026lt;-\u0026gt; String Find the smallest window in a string containing all characters of another string \u0026lt;-\u0026gt; String Recursively remove all adjacent duplicates \u0026lt;-\u0026gt; String String matching where one string contains wildcard characters \u0026lt;-\u0026gt; String Function to find Number of customers who could not get a computer \u0026lt;-\u0026gt; String Transform One String to Another using Minimum Number of Given Operation \u0026lt;-\u0026gt; String Check if two given strings are isomorphic to each other \u0026lt;-\u0026gt; String Recursively print all sentences that can be formed from list of word lists \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Find first and last positions of an element in a sorted array \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Find a Fixed Point (Value equal to index) in a given array https://leetcode.com/problems/find-pivot-index/ Searching \u0026amp; Sorting Search in a rotated sorted array https://leetcode.com/problems/search-in-rotated-sorted-array/ Searching \u0026amp; Sorting square root of an integer \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Maximum and minimum of an array using minimum number of comparisons \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Optimum location of point to minimize total distance https://leetcode.com/problems/best-meeting-point/ Searching \u0026amp; Sorting Find the repeating and the missing \u0026lt;-\u0026gt; Searching \u0026amp; Sorting find majority element \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Searching in an array where adjacent differ by at most k \u0026lt;-\u0026gt; Searching \u0026amp; Sorting find a pair with a given difference \u0026lt;-\u0026gt; Searching \u0026amp; Sorting find four elements that sum to a given value \u0026lt;-\u0026gt; Searching \u0026amp; Sorting maximum sum such that no 2 elements are adjacent \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Count triplet with sum smaller than a given value \u0026lt;-\u0026gt; Searching \u0026amp; Sorting merge 2 sorted arrays \u0026lt;-\u0026gt; Searching \u0026amp; Sorting print all subarrays with 0 sum \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Product array Puzzle \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Sort array according to count of set bits \u0026lt;-\u0026gt; Searching \u0026amp; Sorting minimum no. of swaps required to sort the array \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Bishu and Soldiers \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Rasta and Kheshtak \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Kth smallest number again Using Min Heap Searching \u0026amp; Sorting Find pivot element in a sorted array \u0026lt;-\u0026gt; Searching \u0026amp; Sorting K-th Element of Two Sorted Arrays \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Aggressive cows \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Book Allocation Problem https://leetcode.com/problems/capacity-to-ship-packages-within-d-days/ Searching \u0026amp; Sorting EKOSPOJ: \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Job Scheduling Algo \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Missing Number in AP \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Smallest number with atleastn trailing zeroes infactorial \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Painters Partition Problem: \u0026lt;-\u0026gt; Searching \u0026amp; Sorting ROTI-Prata SPOJ \u0026lt;-\u0026gt; Searching \u0026amp; Sorting DoubleHelix SPOJ \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Subset Sums \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Findthe inversion count \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Implement Merge-sort in-place \u0026lt;-\u0026gt; Searching \u0026amp; Sorting Partitioning and Sorting Arrays with Many Repeated Entries \u0026lt;-\u0026gt; LinkedList Write a Program to reverse the Linked List. (Both Iterative and recursive) \u0026lt;-\u0026gt; LinkedList Reverse a Linked List in group of Given Size. [Very Imp] https://leetcode.com/problems/reverse-nodes-in-k-group/ LinkedList Write a program to Detect loop in a linked list. \u0026lt;-\u0026gt; LinkedList Write a program to Delete loop in a linked list. \u0026lt;-\u0026gt; LinkedList Find the starting point of the loop. \u0026lt;-\u0026gt; LinkedList Remove Duplicates in a sorted Linked List. LinkedList Remove Duplicates from Sorted List II LinkedList Remove Duplicates in a Un-sorted Linked List. LinkedList Write a Program to Move the last element to Front in a Linked List. \u0026lt;-\u0026gt; LinkedList Add “1” to a number represented as a Linked List. \u0026lt;-\u0026gt; LinkedList Add two numbers represented by linked lists. \u0026lt;-\u0026gt; LinkedList Intersection of two Sorted Linked List. \u0026lt;-\u0026gt; LinkedList Intersection Point of two Linked Lists. \u0026lt;-\u0026gt; LinkedList Merge Sort For Linked lists.[Very Important] \u0026lt;-\u0026gt; LinkedList Quicksort for Linked Lists.[Very Important] \u0026lt;-\u0026gt; LinkedList Find the middle Element of a linked list. \u0026lt;-\u0026gt; LinkedList Check if a linked list is a circular linked list. \u0026lt;-\u0026gt; LinkedList Split a Circular linked list into two halves. \u0026lt;-\u0026gt; LinkedList Write a Program to check whether the Singly Linked list is a palindrome or not. \u0026lt;-\u0026gt; LinkedList Deletion from a Circular Linked List. \u0026lt;-\u0026gt; LinkedList Reverse a Doubly Linked list. \u0026lt;-\u0026gt; LinkedList Find pairs with a given sum in a DLL. \u0026lt;-\u0026gt; LinkedList Count triplets in a sorted DLL whose sum is equal to given value “X”. \u0026lt;-\u0026gt; LinkedList Sort a “k”sorted Doubly Linked list.[Very IMP] \u0026lt;-\u0026gt; LinkedList Rotate DoublyLinked list by N nodes. \u0026lt;-\u0026gt; LinkedList Rotate a Doubly Linked list in group of Given Size.[Very IMP] \u0026lt;-\u0026gt; LinkedList Can we reverse a linked list in less than O(n) ? \u0026lt;-\u0026gt; LinkedList Why Quicksort is preferred for. Arrays and Merge Sort for LinkedLists ? \u0026lt;-\u0026gt; LinkedList Flatten a Linked List \u0026lt;-\u0026gt; LinkedList Sort a LL of 0\u0026rsquo;s, 1\u0026rsquo;s and 2\u0026rsquo;s \u0026lt;-\u0026gt; LinkedList Clone a linked list with next and random pointer \u0026lt;-\u0026gt; LinkedList Merge K sorted Linked list \u0026lt;-\u0026gt; LinkedList Multiply 2 no. represented by LL \u0026lt;-\u0026gt; LinkedList Delete nodes which have a greater value on right side \u0026lt;-\u0026gt; LinkedList Segregate even and odd nodes in a Linked List \u0026lt;-\u0026gt; LinkedList Program for n’th node from the end of a Linked List \u0026lt;-\u0026gt; LinkedList Find the first non-repeating character from a stream of characters \u0026lt;-\u0026gt; Binary Trees level order traversal \u0026lt;-\u0026gt; Binary Trees Reverse Level Order traversal \u0026lt;-\u0026gt; Binary Trees Height of a tree \u0026lt;-\u0026gt; Binary Trees Diameter of a tree \u0026lt;-\u0026gt; Binary Trees Mirror of a tree \u0026lt;-\u0026gt; Binary Trees Inorder Traversal of a tree both using recursion and Iteration \u0026lt;-\u0026gt; Binary Trees Preorder Traversal of a tree both using recursion and Iteration \u0026lt;-\u0026gt; Binary Trees Postorder Traversal of a tree both using recursion and Iteration \u0026lt;-\u0026gt; Binary Trees Left View of a tree \u0026lt;-\u0026gt; Binary Trees Right View of Tree https://leetcode.com/problems/binary-tree-right-side-view/ Binary Trees Top View of a tree https://leetcode.com/problems/vertical-order-traversal-of-a-binary-tree/ Binary Trees Bottom View of a tree \u0026lt;-\u0026gt; Binary Trees Zig-Zag traversal of a binary tree https://leetcode.com/problems/binary-tree-zigzag-level-order-traversal/ Binary Trees Check if a tree is balanced or not \u0026lt;-\u0026gt; Binary Trees Diagnol Traversal of a Binary tree https://www.youtube.com/watch?v=e9ZGxH1y_PE Binary Trees Boundary traversal of a Binary tree https://www.youtube.com/watch?v=0ca1nvR0be4 Binary Trees Construct Binary Tree from String with Bracket Representation \u0026lt;-\u0026gt; Binary Trees Convert Binary tree into Doubly Linked List \u0026lt;-\u0026gt; Binary Trees Convert Binary tree into Sum tree \u0026lt;-\u0026gt; Binary Trees Construct Binary tree from Inorder and preorder traversal \u0026lt;-\u0026gt; Binary Trees Find minimum swaps required to convert a Binary tree into BST \u0026lt;-\u0026gt; Binary Trees Check if Binary tree is Sum tree or not \u0026lt;-\u0026gt; Binary Trees Check if all leaf nodes are at same level or not \u0026lt;-\u0026gt; Binary Trees Check if a Binary Tree contains duplicate subtrees of size 2 or more [ IMP ] \u0026lt;-\u0026gt; Binary Trees Check if 2 trees are mirror or not \u0026lt;-\u0026gt; Binary Trees Sum of Nodes on the Longest path from root to leaf node \u0026lt;-\u0026gt; Binary Trees Check if given graph is tree or not. [ IMP ] \u0026lt;-\u0026gt; Binary Trees Find Largest subtree sum in a tree \u0026lt;-\u0026gt; Binary Trees Maximum Sum of nodes in Binary tree such that no two are adjacent \u0026lt;-\u0026gt; Binary Trees Print all \u0026ldquo;K\u0026rdquo; Sum paths in a Binary tree \u0026lt;-\u0026gt; Binary Trees Find LCA in a Binary tree \u0026lt;-\u0026gt; Binary Trees Find distance between 2 nodes in a Binary tree \u0026lt;-\u0026gt; Binary Trees Kth Ancestor of node in a Binary tree \u0026lt;-\u0026gt; Binary Trees Find all Duplicate subtrees in a Binary tree [ IMP ] \u0026lt;-\u0026gt; Binary Trees Tree Isomorphism Problem \u0026lt;-\u0026gt; Binary Trees Copy List with Random Pointer Binary Search Trees Fina a value in a BST \u0026lt;-\u0026gt; Binary Search Trees Deletion of a node in a BST \u0026lt;-\u0026gt; Binary Search Trees Find min and max value in a BST \u0026lt;-\u0026gt; Binary Search Trees Find inorder successor and inorder predecessor in a BST \u0026lt;-\u0026gt; Binary Search Trees Check if a tree is a BST or not \u0026lt;-\u0026gt; Binary Search Trees Populate Inorder successor of all nodes \u0026lt;-\u0026gt; Binary Search Trees Find LCA of 2 nodes in a BST \u0026lt;-\u0026gt; Binary Search Trees Construct BST from preorder traversal \u0026lt;-\u0026gt; Binary Search Trees Convert Binary tree into BST \u0026lt;-\u0026gt; Binary Search Trees Convert a normal BST into a Balanced BST \u0026lt;-\u0026gt; Binary Search Trees Merge two BST [ V.V.V\u0026gt;IMP ] \u0026lt;-\u0026gt; Binary Search Trees Find Kth largest element in a BST \u0026lt;-\u0026gt; Binary Search Trees Find Kth smallest element in a BST \u0026lt;-\u0026gt; Binary Search Trees Count pairs from 2 BST whose sum is equal to given value \u0026ldquo;X\u0026rdquo; \u0026lt;-\u0026gt; Binary Search Trees Find the median of BST in O(n) time and O(1) space \u0026lt;-\u0026gt; Binary Search Trees Count BST ndoes that lie in a given range \u0026lt;-\u0026gt; Binary Search Trees Replace every element with the least greater element on its right \u0026lt;-\u0026gt; Binary Search Trees Given \u0026ldquo;n\u0026rdquo; appointments, find the conflicting appointments \u0026lt;-\u0026gt; Binary Search Trees Check preorder is valid or not \u0026lt;-\u0026gt; Binary Search Trees Check whether BST contains Dead end \u0026lt;-\u0026gt; Binary Search Trees Largest BST in a Binary Tree [ V.V.V.V.V IMP ] \u0026lt;-\u0026gt; Binary Search Trees Flatten BST to sorted list \u0026lt;-\u0026gt; Binary Search Trees Check Completeness of a Binary Tree Binary Search Trees Non-overlapping Intervals Binary Search Trees Largest BST in Binary Tree https://leetcode.com/problems/maximum-sum-bst-in-binary-tree/ Greedy Activity Selection Problem \u0026lt;-\u0026gt; Greedy Job SequencingProblem \u0026lt;-\u0026gt; Greedy Huffman Coding \u0026lt;-\u0026gt; Greedy Water Connection Problem \u0026lt;-\u0026gt; Greedy Fractional Knapsack Problem \u0026lt;-\u0026gt; Greedy Greedy Algorithm to find Minimum number of Coins \u0026lt;-\u0026gt; Greedy Maximum trains for which stoppage can be provided \u0026lt;-\u0026gt; Greedy Minimum Platforms Problem \u0026lt;-\u0026gt; Greedy Buy Maximum Stocks if i stocks can be bought on i-th day \u0026lt;-\u0026gt; Greedy Find the minimum and maximum amount to buy all N candies \u0026lt;-\u0026gt; Greedy Minimize Cash Flow among a given set of friends who have borrowed money from each other Optimal Account Balancing Greedy Minimum Cost to cut a board into squares \u0026lt;-\u0026gt; Greedy Number of Islands \u0026lt;-\u0026gt; Greedy Find maximum meetings in one room https://www.lintcode.com/problem/919 Greedy Maximum product subset of an array \u0026lt;-\u0026gt; Greedy Maximize array sum after K negations \u0026lt;-\u0026gt; Greedy Maximize the sum of arr[i]*i \u0026lt;-\u0026gt; Greedy Maximum sum of absolute difference of an array \u0026lt;-\u0026gt; Greedy Maximize sum of consecutive differences in a circular array \u0026lt;-\u0026gt; Greedy Minimum sum of absolute difference of pairs of two arrays \u0026lt;-\u0026gt; Greedy Program for Shortest Job First (or SJF) CPU Scheduling \u0026lt;-\u0026gt; Greedy Program for Least Recently Used (LRU) Page Replacement algorithm \u0026lt;-\u0026gt; Greedy Smallest subset with sum greater than all other elements \u0026lt;-\u0026gt; Greedy Chocolate Distribution Problem \u0026lt;-\u0026gt; Greedy DEFKIN -Defense of a Kingdom \u0026lt;-\u0026gt; Greedy DIEHARD -DIE HARD \u0026lt;-\u0026gt; Greedy GERGOVIA -Wine trading in Gergovia \u0026lt;-\u0026gt; Greedy Picking Up Chicks \u0026lt;-\u0026gt; Greedy CHOCOLA –Chocolate \u0026lt;-\u0026gt; Greedy ARRANGE -Arranging Amplifiers \u0026lt;-\u0026gt; Greedy K Centers Problem \u0026lt;-\u0026gt; Greedy Minimum Cost of ropes \u0026lt;-\u0026gt; Greedy Find smallest number with given number of digits and sum of digits \u0026lt;-\u0026gt; Greedy Rearrange characters in a string such that no two adjacent are same \u0026lt;-\u0026gt; Greedy Find maximum sum possible equal sum of three stacks \u0026lt;-\u0026gt; Greedy Maximum Sub-String after at most K changes https://leetcode.com/problems/maximize-the-confusion-of-an-exam/ BackTracking Rat in a maze Problem \u0026lt;-\u0026gt; BackTracking Printing all solutions in N-Queen Problem \u0026lt;-\u0026gt; BackTracking Word Break Problem using Backtracking \u0026lt;-\u0026gt; BackTracking Remove Invalid Parentheses \u0026lt;-\u0026gt; BackTracking Sudoku Solver \u0026lt;-\u0026gt; BackTracking m Coloring Problem \u0026lt;-\u0026gt; BackTracking Print all palindromic partitions of a string \u0026lt;-\u0026gt; BackTracking Subset Sum Problem \u0026lt;-\u0026gt; BackTracking The Knight’s tour problem \u0026lt;-\u0026gt; BackTracking Tug of War \u0026lt;-\u0026gt; BackTracking Find shortest safe route in a path with landmines \u0026lt;-\u0026gt; BackTracking Combinational Sum \u0026lt;-\u0026gt; BackTracking Find Maximum number possible by doing at-most K swaps \u0026lt;-\u0026gt; BackTracking Print all permutations of a string \u0026lt;-\u0026gt; BackTracking Find if there is a path of more than k length from a source \u0026lt;-\u0026gt; BackTracking Longest Possible Route in a Matrix with Hurdles \u0026lt;-\u0026gt; BackTracking Print all possible paths from top left to bottom right of a mXn matrix \u0026lt;-\u0026gt; BackTracking Partition of a set intoK subsets with equal sum \u0026lt;-\u0026gt; BackTracking Find the K-th Permutation Sequence of first N natural numbers \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement Stack from Scratch \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement Queue from Scratch \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement 2 stack in an array \u0026lt;-\u0026gt; Stacks \u0026amp; Queues find the middle element of a stack \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement \u0026ldquo;N\u0026rdquo; stacks in an Array \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Check the expression has valid or Balanced parenthesis or not. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Reverse a String using Stack \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Design a Stack that supports getMin() in O(1) time and O(1) extra space. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Find the next Greater element \u0026lt;-\u0026gt; Stacks \u0026amp; Queues The celebrity Problem https://www.youtube.com/watch?v=CiiXBvrX-5A Stacks \u0026amp; Queues Arithmetic Expression evaluation https://leetcode.com/problems/evaluate-reverse-polish-notation/ Stacks \u0026amp; Queues Evaluation of Postfix expression https://www.youtube.com/watch?v=422Q_yx2yA8 Stacks \u0026amp; Queues Implement a method to insert an element at its bottom without using any other data structure. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Reverse a stack using recursion \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Sort a Stack using recursion \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Merge Overlapping Intervals \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Largest rectangular Area in Histogram \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Length of the Longest Valid Substring \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Expression contains redundant bracket or not \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement Stack using Queue \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement Stack using Deque \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Stack Permutations (Check if an array is stack permutation of other) \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement Queue using Stack \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement \u0026ldquo;n\u0026rdquo; queue in an array \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Implement a Circular queue \u0026lt;-\u0026gt; Stacks \u0026amp; Queues LRU Cache Implementationa \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Reverse a Queue using recursion \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Reverse the first “K” elements of a queue \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Interleave the first half of the queue with second half \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Find the first circular tour that visits all Petrol Pumps \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Minimum time required to rot all oranges \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Distance of nearest cell having 1 in a binary matrix \u0026lt;-\u0026gt; Stacks \u0026amp; Queues First negative integer in every window of size “k” \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Check if all levels of two trees are anagrams or not. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Sum of minimum and maximum elements of all subarrays of size “k”. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Minimum sum of squares of character counts in a given string after removing “k” characters. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Queue based approach or first non-repeating character in a stream. \u0026lt;-\u0026gt; Stacks \u0026amp; Queues Next Smaller Element \u0026lt;-\u0026gt; Heap Implement a Maxheap/MinHeap using arrays and recursion. \u0026lt;-\u0026gt; Heap Sort an Array using heap. (HeapSort) \u0026lt;-\u0026gt; Heap Maximum of all subarrays of size k. \u0026lt;-\u0026gt; Heap “k” largest element in an array \u0026lt;-\u0026gt; Heap Kth smallest and largest element in an unsorted array \u0026lt;-\u0026gt; Heap Merge “K” sorted arrays. [ IMP ] \u0026lt;-\u0026gt; Heap Merge 2 Binary Max Heaps \u0026lt;-\u0026gt; Heap Kth largest sum continuous subarrays \u0026lt;-\u0026gt; Heap Leetcode- reorganize strings \u0026lt;-\u0026gt; Heap Merge “K” Sorted Linked Lists [V.IMP] \u0026lt;-\u0026gt; Heap Smallest range in “K” Lists \u0026lt;-\u0026gt; Heap Median in a stream of Integers \u0026lt;-\u0026gt; Heap Check if a Binary Tree is Heap \u0026lt;-\u0026gt; Heap Connect “n” ropes with minimum cost \u0026lt;-\u0026gt; Heap Convert BST to Min Heap \u0026lt;-\u0026gt; Heap Convert min heap to max heap \u0026lt;-\u0026gt; Heap Rearrange characters in a string such that no two adjacent are same. \u0026lt;-\u0026gt; Heap Minimum sum of two numbers formed from digits of an array \u0026lt;-\u0026gt; Graph Create a Graph, print it \u0026lt;-\u0026gt; Graph Implement BFS algorithm \u0026lt;-\u0026gt; Graph Implement DFS Algo \u0026lt;-\u0026gt; Graph Detect Cycle in Directed Graph using BFS/DFS Algo \u0026lt;-\u0026gt; Graph Detect Cycle in UnDirected Graph using BFS/DFS Algo \u0026lt;-\u0026gt; Graph Search in a Maze \u0026lt;-\u0026gt; Graph Minimum Step by Knight \u0026lt;-\u0026gt; Graph flood fill algo \u0026lt;-\u0026gt; Graph Clone a graph \u0026lt;-\u0026gt; Graph Making wired Connections \u0026lt;-\u0026gt; Graph word Ladder \u0026lt;-\u0026gt; Graph Dijkstra algo \u0026lt;-\u0026gt; Graph Implement Topological Sort \u0026lt;-\u0026gt; Graph Minimum time taken by each job to be completed given by a Directed Acyclic Graph \u0026lt;-\u0026gt; Graph Find whether it is possible to finish all tasks or not from given dependencies \u0026lt;-\u0026gt; Graph Find the no. of Isalnds \u0026lt;-\u0026gt; Graph Given a sorted Dictionary of an Alien Language, find order of characters \u0026lt;-\u0026gt; Graph Implement Kruksal’sAlgorithm \u0026lt;-\u0026gt; Graph Implement Prim’s Algorithm \u0026lt;-\u0026gt; Graph Total no. of Spanning tree in a graph \u0026lt;-\u0026gt; Graph Implement Bellman Ford Algorithm \u0026lt;-\u0026gt; Graph Implement Floyd warshallAlgorithm \u0026lt;-\u0026gt; Graph Travelling Salesman Problem \u0026lt;-\u0026gt; Graph Graph ColouringProblem \u0026lt;-\u0026gt; Graph Snake and Ladders Problem \u0026lt;-\u0026gt; Graph Find bridge in a graph \u0026lt;-\u0026gt; Graph Count Strongly connected Components(Kosaraju Algo) \u0026lt;-\u0026gt; Graph Check whether a graph is Bipartite or Not \u0026lt;-\u0026gt; Graph Detect Negative cycle in a graph \u0026lt;-\u0026gt; Graph Longest path in a Directed Acyclic Graph \u0026lt;-\u0026gt; Graph Journey to the Moon \u0026lt;-\u0026gt; Graph Cheapest Flights Within K Stops \u0026lt;-\u0026gt; Graph Oliver and the Game \u0026lt;-\u0026gt; Graph Water Jug problem using BFS \u0026lt;-\u0026gt; Graph Water Jug problem using BFS \u0026lt;-\u0026gt; Graph Find if there is a path of more thank length from a source \u0026lt;-\u0026gt; Graph M-ColouringProblem \u0026lt;-\u0026gt; Graph Minimum edges to reverse o make path from source to destination \u0026lt;-\u0026gt; Graph Paths to travel each nodes using each edge(Seven Bridges) \u0026lt;-\u0026gt; Graph Vertex Cover Problem \u0026lt;-\u0026gt; Graph Chinese Postman or Route Inspection \u0026lt;-\u0026gt; Graph Number of Triangles in a Directed and Undirected Graph \u0026lt;-\u0026gt; Graph Minimise the cashflow among a given set of friends who have borrowed money from each other \u0026lt;-\u0026gt; Graph Two Clique Problem \u0026lt;-\u0026gt; Trie Construct a trie from scratch \u0026lt;-\u0026gt; Trie Find shortest unique prefix for every word in a given list \u0026lt;-\u0026gt; Trie Word Break Problem (Trie solution) Trie Given a sequence of words, print all anagrams together \u0026lt;-\u0026gt; Trie Implement a Phone Directory \u0026lt;-\u0026gt; Trie Print unique rows in a given boolean matrix \u0026lt;-\u0026gt; Dynamic Programming Coin ChangeProblem \u0026lt;-\u0026gt; Dynamic Programming Knapsack Problem \u0026lt;-\u0026gt; Dynamic Programming Binomial CoefficientProblem \u0026lt;-\u0026gt; Dynamic Programming Permutation CoefficientProblem \u0026lt;-\u0026gt; Dynamic Programming Program for nth Catalan Number \u0026lt;-\u0026gt; Dynamic Programming Matrix Chain Multiplication \u0026lt;-\u0026gt; Dynamic Programming Edit Distance \u0026lt;-\u0026gt; Dynamic Programming Subset Sum Problem \u0026lt;-\u0026gt; Dynamic Programming Friends Pairing Problem \u0026lt;-\u0026gt; Dynamic Programming Gold Mine Problem \u0026lt;-\u0026gt; Dynamic Programming Assembly Line SchedulingProblem \u0026lt;-\u0026gt; Dynamic Programming Painting the Fenceproblem \u0026lt;-\u0026gt; Dynamic Programming Maximize The Cut Segments \u0026lt;-\u0026gt; Dynamic Programming Longest Common Subsequence \u0026lt;-\u0026gt; Dynamic Programming Longest Repeated Subsequence \u0026lt;-\u0026gt; Dynamic Programming Longest Increasing Subsequence \u0026lt;-\u0026gt; Dynamic Programming Space Optimized Solution of LCS \u0026lt;-\u0026gt; Dynamic Programming LCS (Longest Common Subsequence) of three strings \u0026lt;-\u0026gt; Dynamic Programming Maximum Sum Increasing Subsequence \u0026lt;-\u0026gt; Dynamic Programming Count all subsequences having product less than K \u0026lt;-\u0026gt; Dynamic Programming Longest subsequence such that difference between adjacent is one \u0026lt;-\u0026gt; Dynamic Programming Maximum subsequence sum such that no three are consecutive \u0026lt;-\u0026gt; Dynamic Programming Egg Dropping Problem \u0026lt;-\u0026gt; Dynamic Programming Maximum Length Chain of Pairs \u0026lt;-\u0026gt; Dynamic Programming Maximum size square sub-matrix with all 1s \u0026lt;-\u0026gt; Dynamic Programming Maximum sum of pairs with specific difference \u0026lt;-\u0026gt; Dynamic Programming Min Cost PathProblem \u0026lt;-\u0026gt; Dynamic Programming Maximum difference of zeros and ones in binary string \u0026lt;-\u0026gt; Dynamic Programming Minimum number of jumps to reach end \u0026lt;-\u0026gt; Dynamic Programming Minimum cost to fill given weight in a bag \u0026lt;-\u0026gt; Dynamic Programming Minimum removals from array to make max –min \u0026lt;= K \u0026lt;-\u0026gt; Dynamic Programming Longest Common Substring \u0026lt;-\u0026gt; Dynamic Programming Count number of ways to reacha given score in a game \u0026lt;-\u0026gt; Dynamic Programming Count Balanced Binary Trees of Height h \u0026lt;-\u0026gt; Dynamic Programming LargestSum Contiguous Subarray [V\u0026gt;V\u0026gt;V\u0026gt;V IMP ] \u0026lt;-\u0026gt; Dynamic Programming Smallest sum contiguous subarray \u0026lt;-\u0026gt; Dynamic Programming Unbounded Knapsack (Repetition of items allowed) \u0026lt;-\u0026gt; Dynamic Programming Word Break Problem \u0026lt;-\u0026gt; Dynamic Programming Largest Independent Set Problem \u0026lt;-\u0026gt; Dynamic Programming Partition problem \u0026lt;-\u0026gt; Dynamic Programming Longest Palindromic Subsequence \u0026lt;-\u0026gt; Dynamic Programming Count All Palindromic Subsequence in a given String \u0026lt;-\u0026gt; Dynamic Programming Longest Palindromic Substring \u0026lt;-\u0026gt; Dynamic Programming Longest alternating subsequence \u0026lt;-\u0026gt; Dynamic Programming Weighted Job Scheduling \u0026lt;-\u0026gt; Dynamic Programming Coin game winner where every player has three choices \u0026lt;-\u0026gt; Dynamic Programming Count Derangements (Permutation such that no element appears in its original position) [ IMPORTANT ] \u0026lt;-\u0026gt; Dynamic Programming Maximum profit by buying and selling a share at most twice [ IMP ] \u0026lt;-\u0026gt; Dynamic Programming Optimal Strategy for a Game \u0026lt;-\u0026gt; Dynamic Programming Optimal Binary Search Tree \u0026lt;-\u0026gt; Dynamic Programming Palindrome PartitioningProblem \u0026lt;-\u0026gt; Dynamic Programming Word Wrap Problem \u0026lt;-\u0026gt; Dynamic Programming Mobile Numeric Keypad Problem [ IMP ] \u0026lt;-\u0026gt; Dynamic Programming Boolean Parenthesization Problem \u0026lt;-\u0026gt; Dynamic Programming Largest rectangular sub-matrix whose sum is 0 \u0026lt;-\u0026gt; Dynamic Programming Largest area rectangular sub-matrix with equal number of 1’s and 0’s [ IMP ] \u0026lt;-\u0026gt; Dynamic Programming Maximum sum rectangle in a 2D matrix \u0026lt;-\u0026gt; Dynamic Programming Maximum profit by buying and selling a share at most k times \u0026lt;-\u0026gt; Dynamic Programming Find if a string is interleaved of two other strings \u0026lt;-\u0026gt; Dynamic Programming Maximum Length of Pair Chain \u0026lt;-\u0026gt; Dynamic Programming Partition Equal Subset Sum https://leetcode.com/submissions/detail/561942165/ Dynamic Programming Target Sum Bit Manipulation Count set bits in an integer \u0026lt;-\u0026gt; Bit Manipulation Find the two non-repeating elements in an array of repeating elements \u0026lt;-\u0026gt; Bit Manipulation Count number of bits to be flipped to convert A to B \u0026lt;-\u0026gt; Bit Manipulation Count total set bits in all numbers from 1 to n \u0026lt;-\u0026gt; Bit Manipulation Program to find whether a no is power of two \u0026lt;-\u0026gt; Bit Manipulation Find position of the only set bit \u0026lt;-\u0026gt; Bit Manipulation Copy set bits in a range \u0026lt;-\u0026gt; Bit Manipulation Divide two integers without using multiplication, division and mod operator \u0026lt;-\u0026gt; Bit Manipulation Calculate square of a number without using *, / and pow() \u0026lt;-\u0026gt; Bit Manipulation Power Set \u0026lt;-\u0026gt; Moore voting algorithm Majority Element https://www.youtube.com/watch?v=n5QY3x_GNDg Moore voting algorithm Majority Element II https://www.youtube.com/watch?v=yDbkQd9t2ig 30 Days Interview Preparation Plan🎯 Originally the below sheet was prepared by Raj Vikramaditya A.K.A Striver. I have documented this sheet here in markdown.\nDay1: (Arrays)\nSort an array of 0’s 1’s 2’s without using extra space or sorting algo\nRepeat and Missing Number\nMerge two sorted Arrays without extra space\nKadane’s Algorithm\nMerge Overlapping Subintervals\nFind the duplicate in an array of N+1 integers.\nDay2: (Arrays)\nSet Matrix Zeros\nPascal Triangle\nNext Permutation\nInversion of Array (Using Merge Sort)\nStock Buy and Sell\nRo tate Matrix\nDay3: (Arrays/maths)\nSearch in a 2D matrix\nPow(X,n)\nMajority Element (\u0026gt;N/2 times)\nMajority Element (\u0026gt;N/3 times)\nGrid Unique Paths\nReverse Pairs (Leetcode)\nGo through Puzzles from GFG** (Search on own)\nDay4: (Hashing)\n2 Sum problem\n4 Sum problem\nLongest Consecutive Sequence\nLargest Subarray with 0 sum\nCount number of subarrays with given XOR (this clearsa lot of problems)\nLongest substring without repeat\nDay5: (LinkedList)\nReverse a LinkedList\nFind middle of LinkedList\nMerge two sorted Linked List\nRemove N-th node from back of LinkedList\nDelete a given Node when a node is given. (0(1) solution)\nAdd two numbers as LinkedList\nDay6:\nFind intersection point of Y LinkedList\nDetect a cycle in Linked List\nReverse a LinkedList in groups of size k\nCheck if a LinkedList is palindrome or not.\nFind the starting point of the Loop of LinkedList\nFlattening of a LinkedList**\nRotate a LinkedList\nDay7: (2-pointer)\nClone a Linked List with random and next pointer\n3 sum\nTrapping rainwater\nRemove Duplicate from Sorted array\nMax consecutive ones\nDay8: (Greedy)\nN meeting in one room\nMinimum number of platforms required for a railway\nJob sequencing Problem\nFractional Knapsack Problem\nGreedy algorithm to find minimum number of coins\nActivity Selection (it i\ns same as N meeting in one room)\nDay9 (Recursion):\nSubset Sums\nSubset-II\nCombination sum-\nCombination sum\nPalindrome Partitioning\nK-th permutation Sequence\nDay10: (Recursion and Backtracking)\nPrint all Permutations of a string/array\nN queens Problem\nSudokuSolver\nM coloring Problem\nRat in a Maze\n6.Word Break -\u0026gt; print all ways\nDay11 : (Binary Search)\nN-th root of an integer (use binary search) (square root, cube root, ..)\nMatrix Median\nFind the element that appears once in sorted array, and rest element appears twice (Binary search)\nSearch element in a sorted and rotated array/ find pivot where it is rotated**\nMedian of 2 sorted arrays\nK-th element of two sorted arrays\nAllocate Minimum Number of Pages\nAggressive Cows\nDay12: (Bits) (Optional, very rare topic in interviews, but if you have time left, someone might ask)\nCheck if a number if a power of 2 or not in O(1) Count total set bits Divide Integers without / operator Power Set (this is very important) Find MSB in o(1) Find square of a number without using multiplication or division operators. Day13: (Stack and Queue)\nImplement Stack Using Arrays\nImplement Queue Using Arrays\nImplement Stack using Queue (using single queue)\nImplement Queue using Stack (0(1) amortised method)\nCheck for balanced parentheses\nNext Greater Element\nSort a Stack\nDay14:\nNext Smaller Element Similar to previous question next greater element, just do pop the greater elements out ..\nLRU cache (vvvv. imp)\nLFU Cache (Hard, can be ignored)\n4.Largest rectangle in histogram (Do the one pass solution)\nTwo pass\nOne pass\nSliding Window maximum video Implement Min Stack Rotten Orange (Using BFS) Stock Span Problem Find maximum of minimums of every window size 10.The Celebrity Problem Day15: (String)\nReverse Words in a String Longest Palindrome in a string Roman Number to Integer and vice versa Implement ATOI/STRSTR Longest Common Prefix Rabin Karp Day16: (String)\nPrefix Function/Z-Function KMP algo / LPS(pi) array Minimum characters needed to be inserted in the beginning to make it palindromic. Check for Anagrams Count and Say Compare version numbers Day17: (Binary Tree)\nInorder Traversal (with recursion and without recursion) Preorder Traversal (with recursion and without recursion) Postorder Traversal (with recursion and without recursion) LeftView Of Binary Tree Bottom View of Binary Tree Top View of Binary Tree** Day18: (Binary Tree)\nLevel order Traversal / Level order traversal in spiral form Height of a Binary Tree Diameter of Binary Tree Check if Binary tree is height balanced or not LCA in Binary Tree Check if two trees are identical or not** Day 19: (Binary Tree)\nMaximum path sum Construct Binary Tree from inorder and preorder Construct Binary Tree from Inorder and Postorder Symmetric Binary Tree Flatten Binary Tree to LinkedList Check if Binary Tree is mirror of itself or not Day 20: (Binary Search Tree)\nPopulate Next Right pointers of Tree Search given Key in BST Construct BST from given keys. Check is a BT is BST or not Find LCA of two nodes in BST Find the inorder predecessor/successor of a given Key in BST.** Day21: (BinarySearchTree)\nFloor and Ceil in a BST Find K-th smallest and K-th largest element in BST (2 different Questions) Find a pair with a given sum in BST BST iterator Size of the largest BST in a Binary Tree Serialize and deserialize Binary Tree Day22: (Mixed Questions)\nBinary Tree to Double Linked List Find median in a stream of running integers. K-th largest element in a stream. Distinct numbers in Window. K-th largest element in an unsorted array. Flood-fill Algorithm Day23: (Graph) Theory\nClone a graph (Not that easy as it looks) DFS BFS Detect A cycle in Undirected Graph/Directed Graph Topo Sort Number of islands (Do in Grid and Graph both) Bipartite Check Day24: (Graph) Theory\nSCC(using KosaRaju’s algo) Djisktra’s Algorithm Bellman Ford Algo Floyd Warshall Algorithm MST using Prim’s Algo MST using Kruskal’s Algo Day25: (Dynamic Programming)\nMax Product Subarray Longest Increasing Subsequence Longest Common Subsequence 0-1 Knapsack Edit Distance Maximum sum increasing subsequence Matrix Chain Multiplication Day26: (DP)\nMaximum sum path in matrix, (count paths, and similar type do, also backtrack to find the maximum path) Coin change Subset Sum Rod Cutting Egg Dropping Word Break Palindrome Partitioning (MCM Variation) Maximum profit in Job scheduling For core revision\u0026lt;/\u0026gt; Day27:\nRevise OS notes that you would have made during your sem If not made notes, spend 2 or 3 days and make notes from Knowledge Gate. Day28:\nRevise DBMS notes that you would have made during your semesters. If not made notes, spend 2 or 3 days and make notes from Knowledge Gate. Day29:\nRevise CN notes, that you would have made during your sem. If not made notes, spend 2 or 3 days and make notes from Knowledge Gate. Day30:\nMake a note of how will your represent your projects, and prepare all questions related to tech which you have used in your projects. Prepare a note which you can say for 3-10 minutes when he asks you that say something about the project. System Design – Concepts📚 https://github.com/SamirPaul1/system-design-primer\nhttps://www.freecodecamp.org/news/systems-design-for-interviews/\nhttps://github.com/shashank88/system_design\n","permalink":"https://samirpaul1.github.io/blog/posts/data-structures-and-algorithms-for-coding-interview/","summary":"Data Structures and Algorithms for Coding Interview","title":"Data Structures and Algorithms for Coding Interview"},{"content":"Quick Access Links LeetCode LeetCode - CheatSheet Getting Started Prerequisites Built With Authors Acknowledgments Quick Access Links LeetCode 1-Two Sum Brute Force One Pass Hash Table 2-Add Two Numbers Elementary Math Solution 3-Substring No Repeat Brute Force Sliding Window Sliding Window Optimized 4-Median of Two Sorted Arrays Recursive Approach 5-Longest Palindromic Substring Longest Common Substring Brute Force Dynamic Programming Expand Around Center Manacher\u0026rsquo;s Algorithm 6-ZigZag Conversion Sort by Row Visit by Row 7-Reverse Integer Pop and Push Digits and Check Before Overflow 8-String to Integer (atoi) ASCII Conversion 9-Palindrome Number Revert Half of the Number 10-Regular Expression Matching Recursion Dynamic Programming Non-Recursive 11-Container with the Most Water Brute Force Two Pointer Approach 12-Integer To Roman String Array 13-Roman to Integer Character Array 14-Longest Common Prefix Horizontal Scanning Vertical Scanning Divide and Conquer Binary Search Further Thoughts 15-3Sum Sorted Array 16-3Sum Closest 3 Pointers 17-Letter Combinations of a Phone Number Backtracking First In First Out (FIFO) Queue 18-4Sum Sorted Array 19-Remove Nth Node From End of List Two Pass Algorithm One Pass Algorithm 20-Valid Parentheses Counting method Stacks 21-Merge Two Sorted Lists Recursive Non-Recursive 22-Generate Parentheses Brute Force Backtracking Closure Number 23-Merge k Sorted Lists Brute Force 146-LRU Cache 1-Two Sum Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice.\nExample:\nGiven nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9, return [0, 1]. Brute Force public int[] twoSum(int[] nums, int target) { for (int i=0; i\u0026lt;nums.size; i++){ for (int j=i+1;j\u0026lt;nums.length;j++){ if (nums[j]==target-nums[i]){ return new int[] {i,j}; } } } throw new IllegalArgumentException(\u0026#34;No two sum solution\u0026#34;); } Complexity Analysis\n* Time complexity: O(n^2) we have a nested loop * Space complexity: O(1) we do not allocate any additional memory One Pass Hash Table public int[] twoSum(int[] nums, int target) { Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for(int i=0; i\u0026lt;nums.length; i++){ int complement=target-nums[i]; if (map.containsKey(complement)){ return new int[] {map.get(complement),i}; } map.put(nums[i],i); } throw new IllegalArgumentException(\u0026#34;No two sum solution\u0026#34;); } Complexity Analysis\n* Time complexity: O(n)\teach lookup in the hash table only requires O(1) time * Space complexity: O(n)\twe require additional space for the hash table which stores at most n 2-Add Two Numbers Given two non-empty linked lists representing two non-negative integers with the digits stored in reverse order and each node containing a single digit, add the two numbers and return as a linked list\nExample:\nInput (2 -\u0026gt; 4 -\u0026gt; 3) + (5 -\u0026gt; 6 -\u0026gt; 4) Output 7 -\u0026gt; 0 -\u0026gt; 8 342 + 465 = 807 Elementary Math Solution /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummyHead= new ListNode(0); ListNode p=l1, q=l2, curr=dummyHead; int carry=0; while (p!=null||q!=null){ int x= (p!=null) ? p.val :0; //if (p!=null) then x contains p.val int y= (q!=null) ? q.val :0; int sum=carry+x+y; carry=sum/10; curr.next=new ListNode(sum%10); curr=curr.next; if (p!=null) p=p.next; if (q!=null) q=q.next; } if (carry\u0026gt;0){ curr.next= new ListNode(carry); } return dummyHead.next; } } Complexity analysis\n* Time Complexity: O(max(m,n)) depends on the lengths of the two linked lists * Space Complexity: O(max(m,n))\tthe maximum length of the new list is max(m,n)+1 3-Substring No Repeat Longest Substring Without Repeating Characters\nGiven a string find the length of the longest substring without repeating characters.\nExample Input: \u0026#34;abcabcbb\u0026#34; Output:\t3 Explanation:\tThe answer is \u0026#34;abc\u0026#34;, with the length of 3 Example 2 Input:\t\u0026#34;bbbbb\u0026#34; Output:\t1 Explanation:\tThe answer is \u0026#34;b\u0026#34;, with the length of 1 Example 3 Input:\t\u0026#34;pwwkew\u0026#34; Output:\t3 Explanation: The answer is \u0026#34;wke\u0026#34;, with the length of 3. Note that the answer must be a substring \u0026#34;pwke\u0026#34; is a subsequence and not a substring Brute Force Algorithm\nSuppose we have a function \u0026ldquo;boolean allUnique(String substring)\u0026rdquo; which returns true if all the characters in the substring are unique and false otherwise. We can iterate through all the possible substrings of the given string s and call the function allUnique. If it turns out to be true, then we update our answer of the maximum length of substring without duplicate characters.\nTo enumerate all substrings of a given string we enumerate the start and end indices of them. Suppose the start and end indices are i and j respectively. Then we have 0 \u0026lt;= i \u0026lt;= j \u0026lt;= n. Thus using two nested loops with i from 0 to n-1 and j from i+1 to n, we can enumerate all the substrings of s\nTo check if one string has duplicate characters we can use a set. We iterate through all the characters in the string and put them into the set one by one. Before putting one character, we check if the set already contains it. If so we return false and after the loop we return true.\npublic class Solution { public int lengthOfLongestSubstring(String s) { int n = s.length(); int ans = 0; for (int i = 0; i \u0026lt; n; i++) for (int j = i + 1; j \u0026lt;= n; j++) if (allUnique(s, i, j)) ans = Math.max(ans, j - i); return ans; } public boolean allUnique(String s, int start, int end) { Set\u0026lt;Character\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int i = start; i \u0026lt; end; i++) { Character ch = s.charAt(i); if (set.contains(ch)) return false; set.add(ch); } return true; } } Complexity Analysis\n* Time Complexity: O(n^3)\tVerifying if characters in [i,j) are unique requires us to scan all of them which would cost O(j-i) time. For a given i, the sum of time costed by each j -\u0026gt; [i+1,n] is \u0026#34;Summation from i+1 to n O(j-1)\u0026#34; Thus, the sum of all the time consumption is: O(summation from 0 to n-1(summation from j=i+1 to n (j-1))) O(summation from i=0 to n-1(1+n-i)(n-i)/2)) = O(n^3) *Note that the sum of all numbers up to n 1+2+3+...+n = n(n+1)/2 * Space Complexity: O(min(n,m))\tWe require O(k) space for checking a substring has no duplicate characters, where k is the size of the set. The size of the Set is upper bounded by the size of the string n amd the size of the charset or alphabet m Sliding Window A sliding window is an abstract concept commonly used in array/string problems. A window is a range of elements in the array/string which usually defined by the start and end indices\nEx. [i,j) left-closed, right-open A sliding window is a window that slides its two boundaries in a certain direction, for example if we slide [i,j) to the right by 1 element, then it becomes [i+1, j+1) - left closed, right open.\nSliding Window approach, whenever we are looking at a section on an array usual to perform calculations we don\u0026rsquo;t need to completely recalculate everything for every section of the array. Usually we can use the value obtained from another section of the array to determine something about this section of the array. For example if we are calculating the sum of sections of an array we can use the previously calculated value of a section to determine the sum of an adjacent section in the array.\nEx. 1 2 3 4 5 6 7 8 If we calculate the first section of four values we get 1+2+3+4 = 10 , then to calculate the next section 2+3+4+5 we can just take our first section (window_sum) and perform the operation:\nwindow_sum-first entry + last entry = 10-1+5= 14 So essentially for the window sliding technique we use what we know about an existing window to determine properties for another window.\nAlgorithm\nIn the brute force approach, we repeatedly check a substring to see if it has duplicate characters but this is unnecessary. If a substring from index i to j-1 is already checked to have no duplicate characters we only need to check if s[j] is already in the substring.\nTo check if a character is already in the substring we can scan the substring which leads to an O(n^2) algorithm but we can improve on this runtime using a HashSet as a sliding window to check if a character exists in the current set O(1).\nWe use a HashSet to store the characters in the current window [i,j) and then we slide the index j to the right, if it is not in the HashSet, we slide j further until s[j] is already in the HashSet. At this point we found the maximum size of substrings without duplicate characters starting with index i. If we do this for all i, then we obtain our answer.\npublic class Solution { public int lengthOfLongestSubstring(String s) { int n = s.length(); Set\u0026lt;Character\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); int ans = 0, i = 0, j = 0; while (i \u0026lt; n \u0026amp;\u0026amp; j \u0026lt; n) { // try to extend the range [i, j] if (!set.contains(s.charAt(j))){ set.add(s.charAt(j++)); ans = Math.max(ans, j - i); } else { set.remove(s.charAt(i++)); } } return ans; } } Complexity Analysis\nTime complexity:\tO(2n)=O(n)\tWorst case each character will be visited twice by i and j Space complexity: O(min(m,n))\tSame as the brute force method, we need O(k) space for the sliding window where k is the size of the set. The size of the set is bounded by the size of the string n and the size of the charset/alphabet m Sliding Window Optimized The previously discussed sliding window approach requires at most 2n steps and this could in fact be optimized even further to require only n steps. Instead of using a set to tell if a character exists or not, we could define a mapping of the characters to its index. Then we can skip the characters immediately when we found a repeated character\nIf s[j] has a duplicate in the range [i , j) with index j\u0026rsquo;, we don\u0026rsquo;t need to increase i little be little we can just skip all the elements in the range [i , j\u0026rsquo;] and let i be j\u0026rsquo;+1 directly\npublic class Solution { public int lengthOfLongestSubstring(String s) { int n = s.length(), ans = 0; Map\u0026lt;Character, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // current index of character // try to extend the range [i, j] for (int j = 0, i = 0; j \u0026lt; n; j++) { if (map.containsKey(s.charAt(j))) { i = Math.max(map.get(s.charAt(j)), i); } ans = Math.max(ans, j - i + 1); map.put(s.charAt(j), j + 1); } return ans; } } 4-Median of Two Sorted Arrays There are two sorted arrays num1 and num2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). You may assume nums1 and nums2 cannot be both empty.\nExample nums1 = [1, 3] nums2 = [2] The median is 2.0 Example 2 nums1= [1, 2] nums2= [3, 4] The median is (2+3)/2 = 2.5 Recursive Approach In statistics the median is used for dividing a set into two equal length subsets with one set being always greater than the other set. To approach this problem first we cut A into two parts at a random position i:\nleft_A | right_A A[0], A[1], ... , A[i-1] A[i], A[i+1], ... , A[m-1] Since A has m elements, there are m+1 kinds of cutting as i can range from 0-m. We can also see that left_A is empty when i is zero and right_A is empty when i=m\nlen(left_A) = i and len(right_A)= m-i We can similarly cut B into two parts at a random position j:\nleft_B\t|\tright_B B[0], B[1], ... , B[j-1]\tB[j], B[j+1], ... , B[n-1] Now if we put left_A and left_B into one set and put right_A and right_B into another set and name them left_part and right_part, then we get\nleft_part\t|\tright_part A[0], A[1], ... , A[i-1]\tA[i], A[i+1], ... , A[m-1] B[0], B[1], ... , B[j-1]\tB[j], B[j+1], ... , B[n-1] If we can ensure that\nthe len(left_part) = len(right_part) max(left_part) \u0026lt;= min(right_part) then we divide all the elements in {A,B} into two parts with equal length and one part is always greater than the other. Then\nmedian= (max(left_part)+min(right_part))/2 To ensure these two conditions, we need to ensure:\ni+j= m-i+n-j (or: m-i+n-j+1) if n\u0026gt;m, we just need to set i=0~m, j= (m+n+1)/2 - i B[j-1]\u0026lt;=A[i] and A[i-1]\u0026lt;=B[j] So, all we need to do is search for i in [0,m] to find an object i such that B[j-1]\u0026lt;=A[i] and A[i-1]\u0026lt;=B[j] where j=(m+n+1)/2 -i\nThen we perform a binary search following the steps described below:\nSet imin=0, imax=0, then start searching in [imin, imax] Set i=(imin+imax)/2 , j=(m+n+1)/2 - i Now we have len(left_part) = len(right_part) and there are only 3 more situations which we may encounter: - B[j-1] \u0026lt;= A[i] and A[i-1]\u0026lt;=B[j] This means that we have found the object i, so we can stop searching - B[j-1] \u0026gt; A[i] Means A[i] is too small, we must adjust i to get B[j-1]\u0026lt;=A[i] so we increase i because this will cuase j to be decreased. We cannot decrease i because when i is decreased, j will be increased so B[j-1] is increased and A[i] is decreased (B[j-1]\u0026lt;= A[i] will never be satisfied) - A[i-1] \u0026gt; B[j] Means A[i-1] is too big and thus we must decrease i to get A[i-1]\u0026lt;=B[j]. In order to do that we must adjust the searching range to [imin, i-1] so we set imax=i-1 and go back to step 2 When the object i is found, then the media is:\nmax(A[i-1],B[j-1]), when m+n is odd (max(A[i-1],B[j-1])+min(A[i],B[j]))/2, when m+n is even\nNext is to consider the edge values i=0, i=m, j=0, j=n where A[i-1], B[j-1], A[i], B[j] may not exist\nclass Solution { public double findMedianSortedArrays(int[] A, int[] B) { int m=A.length; int n=B.length; if (m\u0026gt;n) { //ensuring that m\u0026lt;=n int[] temp=A; A=B; B=temp; int tmp=m; m=n; n=tmp; } int iMin=0, iMax=m, halfLen=(m+n+1)/2; while (iMin\u0026lt;=iMax) { int i=(iMin+iMax)/2 int j= halfLen - i; if (i\u0026lt;iMax \u0026amp;\u0026amp; B[j-1] \u0026gt; A[i]){ iMin=i+1; //i is too small } else if (i\u0026gt;iMin \u0026amp;\u0026amp; A[i-1]\u0026gt;B[j]) { iMax=i-1; //i is too big } else{ //we have found the object i int maxLeft=0; if (i==0) { maxLeft=B[j-1]; } else if (j==0){ maxLeft=A[i-1]; } else{ maxLeft=Math.max(A[i-1], B[j-1]); } if ((m+n)%2 ==1) { return maxLeft; } int minRIght=0; if (i==m) { minRight=B[j]; } else if (j==n) { minRight=A[i]; } else { minRight=Math.min(B[j], A[i]); } return (maxLeft+minRight)/2.0; } } return 0.0; } } Complexity Analysis\nTime Complexity: O(log(min(m,n)))\tAt first the searching range is [0,m] and the length of this searching range will be reduced by half after each loop so we only need log(m) loops. Since we do constant operations in each loop the time complexity is O(log(m) and since m\u0026lt;=n the time complexity is O(log(min(m,n)) Space Complexity: O(1)\tWe only need constant memory to store 9 local variables so the space complexity is O(1) 5-Longest Palindromic Substring Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000.\nExample 1: Input: \u0026#34;babad\u0026#34; Output: \u0026#34;bab\u0026#34; Note: \u0026#34;aba\u0026#34; is also a valid answer Example 2: Input: \u0026#34;cbbd\u0026#34; Output: \u0026#34;bb\u0026#34; Longest Common Substring Some people will be tempted to come up with this quick solution which is unforunately flawed, \u0026ldquo;reverse S and become S\u0026rsquo;. Find the longest common substring between S and S\u0026rsquo; and that will be the longest palindromic substring.\u0026rdquo; This will work with some examples but there are some cases where the longest common substring is not a valid palindrome.\nEx. S=\u0026quot;abacdfgdcaba\u0026quot;, S'=\u0026quot;abacdgfdcaba\u0026quot; The longest common substring between S and S\u0026rsquo; is \u0026ldquo;abacd\u0026rdquo; and clearly this is not a valid palindrome\nWe can solve this problem however by checking if the substring\u0026rsquo;s indices are the same as the reversed substring\u0026rsquo;s original indices each time we find a longest common substring. If it is, then we attempt to update the longest palindrome found so far, if not we skip this and find the next candidate\nComplexity Analysis\nTime Complexity: O(n^2) Space Complexity: O(n^2) Brute Force The obvious brute force solution is to pick all possible starting and ending position for a substring and verify if it is a palindrome\nComplexity Analysis\nTime Complexity: O(n^3)\tIf n is the length of the input string, there are a total of (n 2) = n(n-1)/2 substrings and since verifying each substring takes O(n) time, the run time complexity is O(n^3) Space Complexity: O(1) Dynamic Programming We can improve on the brute force solution by avoid some unnecessary re-computation while validating palidromes. Consider the word \u0026ldquo;ababa\u0026rdquo;, if we already know that \u0026ldquo;bab\u0026rdquo; is a palindrome then we can determine that ababa is a palindrome by noticing that the two left and right letters connected to bab are the same.\nThis yields a straight forward dynamic programming solution where we initialize the one and two letters palindromes and then work our way up finding all three letters palindromes and so on.\nComplexity Analysis\nTime Complexity: O(n^2)\tSpace Complexity: O(n^2)\tUsing O(n^2) space to store the table Expand Around Center This approach allows us to solve this problem in O(n^2) time using only constant space complexity. We observe that a palindrome mirrors around its enter and therefore a palindrome can be expanded from its center and there are only 2n-1 such centers (for palindromes with an even number of letters like \u0026ldquo;abba\u0026rdquo; its center is in between two letters).\npublic String longestPalindrome(String s) { if (s==null || s.length() \u0026lt; 1) return \u0026#34;\u0026#34;; //edge case int start=0, end=0; for (int i=0; i\u0026lt;s.length(); i++) { int len1=expandAroundCenter(s,i,i); int len2=expandAroundCenter(s,i,i+1); int len=Math.max(len1,len2); if (len\u0026gt;end-start) { start= i-(len-1)/2; end=i+len/2 } } return s.substring(start,end+1); } private int expandAroundCenter(String s, int left, int right) { int L=left, R=right; while(L\u0026gt;=0 \u0026amp;\u0026amp; R\u0026lt;s.length() \u0026amp;\u0026amp; s.charAt(L)==s.charAt(R)) { L--; R++; } return R-L-1; } Manacher\u0026rsquo;s Algorithm There is an O(n) algorithm called Manacher\u0026rsquo;s algorithm, however, it is a non-trivial algorithm and no one would expect you to come up with this algorithm in a 45 minute coding session\n6-ZigZag Conversion The string \u0026ldquo;PAYPALISHIRING\u0026rdquo; is written in a zigzag pattern on a given number of rows like this:\nP A H N A P L S I I G Y I R And then read line by line: \u0026ldquo;PAHNAPLSIIGYIR\u0026rdquo;. Write a code that will take a string and make this conversion given a number of rows:\nstring convert(string s, int numRows); Example 1: Input: s=\u0026#34;PAYPALISHIRING\u0026#34;, numRows=3 Output: \u0026#34;PAHNAPLSIIGYIR\u0026#34; Example 2: Input: s=\u0026#34;PAYPALISHIRING\u0026#34;, numRows=4 Output: \u0026#34;PINALSIGYAHRPI\u0026#34; Explanation: P I N A L S I G Y A H R P I Sort by Row By iterating through the string from left to right we can easily determine which row in the Zig-Zag pattern that a character belongs to\nAlgorithm\nWe can use min(numRows,len(s)) lists to represent the non-empty rows of the Zig-Zag Pattern. Iterate through s from left to right appending each character to the appropriate row. The appropriate row can be tracked using two variables: the current row and the current direction.\nThe current direction only changes when we moved to the topmost row or moved down to the bottommost row\nclass Solution { public String convert(String s, int numRows) { if (numRows==1) return s;\t//if there is only one row return string List\u0026lt;StringBuilder\u0026gt; rows=new ArrayList\u0026lt;\u0026gt;(); for (int i=0; i\u0026lt;Math.min(numRows, s.length()); i++){ rows.add(new StringBuilder()); } int curRow=0; boolean goingDown=false; for(char c: s.toCharArray()) { rows.get(curRow).append(c); if (curRow==0 || curRow==numRows-1) { goingDown=!goingDown; } curRow+=goingDown ? 1 : -1; }\tStringBuilder ret= new StringBuilder(); for(StringBuilder row:rows) { ret.append(row); } return ret.toString(); } } Complexity Analysis\nTime Complexity: O(n)\twhere n==len(s) Space Complexity: O(n) Visit by Row Visit the characters in the same order as reading the Zig-Zag pattern line by line\nAlgorithm\nVisit all characters in row 0 first, then row 1, then row 2, and so on. For all whole numbers k, * characters in row 0 are located at indexes k*(2*numRows-2) * characters in row numRows -1 are located at indexes k*(2*numRows-2)+ numRows -1 * characters in inner row i are located at indexes k*(2*numRows-2)+i and (k+1)(2*numRows-2)-i\nclass Solution { public String convert(String s, int numRows) { if (numRows==1) return s; StringBuilder ret=new StringBuilder(); int n=s.length(); int cycleLen= 2* numRows -2; for (int i=0; i\u0026lt;numRows; i++) { for (int j=0; j+1\u0026lt;n; j+= cycleLen) { ret.append(s.charAt(j+i)); if (i!=0 \u0026amp;\u0026amp; i!=numROws-1 \u0026amp;\u0026amp; j+cycleLen-i\u0026lt;n) { ret.append(s.charAt(j+cycleLen-i)); } } return ret.toString(); } } } Complexity Analysis\nTime Complexity: O(n)\twhere n==len(s) Each index is visited once Space Complexity: O(n) C++ implementation can achieve O(1) if the return string is not considered extra space 7-Reverse Integer Given a 32- bit signed integer, reverse digits of an integer.\nExample 1: Input: 123 Output: 321 Example 2: Input: -123 Output: -321 Example 3: Input: 120 Output: 21 For the purpose of this problem assume that your function returns 0 when the reversed integer overflows\nPop and Push Digits and Check Before Overflow We can build up the reverse integer one digit at and time and before doing so we can check whether or not appedning another digit would cause overflow\nAlgorithm\nReversing an integer can be done similarly to reversing a string. We want to repeatedly \u0026ldquo;pop\u0026rdquo; the last digit off of x and push it to the back of the rev so that in the end rev is the reverse of x.\nTo push and pop digits without the help of some auxiliar stack/array we can use math\n//pop operation: pop = x%10; x/=10; //push operation: temp=rev*10+pop; rev =temp; This statement is dangerous however as the statement temp=rev*10+pop may cause an overflow and luckily it is easy to check beforehand whether or not this statement would cause an overflow.\nIf temp=rev*10+pop causes an overflow, then rev\u0026gt;=INTMAX/10 If rev\u0026gt; INTMAX/10, then temp=rev*10+pop is guaranteed to overflow if rev==INTMAX/10, then temp=rev*10 + pop will overflow if an only if pop\u0026gt;7 class Solution { public int reverse(int x) { int rev=0; while (x!=0) { int pop=x%10; x/=10; if (rev\u0026gt;Integer.MAX_VALUE/10||(rev==Integer.MAX_VALUE/10 \u0026amp;\u0026amp; pop\u0026gt;7)) return 0; if (rev\u0026lt;Integer.MIN_VALUE/10||(rev==Integer.MIN_VALUE/10 \u0026amp;\u0026amp; pop\u0026lt;-8)) return 0; rev=rev*10 +pop; } return rev; } } Complexity Analysis\nTime Complexity: O(log(x))\tThere are roughly log10(x) digits in x Space Complexity: O(1) 8-String to Integer (atoi) Implement atoi which converts a string to an integer\nThe function first discards as many whitespace characters as necessary until the first non-whitespace character is found. Then, starting from this character, takes an optional initial plus or minus sign followed by as many numerical digits as possible and interprets them as a numerical value.\nThe string can contain additional characters after those that form the integral number, which are ignored and have no effect on the behavior of this function.\nIf the first sequence of non-whitespace characters in str is not a valid integral number, or if no such sequence exits because either str is empty or it contains only whitespace characters, no conversion is performed.\nIf no valid conversion could be performed a zero value is returned\nNote:\nonly the space character \u0026rsquo; \u0026rsquo; is considered as whitespace character assume we are dealing with an environment which could only store integers within the 32-bit signed integer range: [-2^31, 2^31-1]. If the numerical value is out of the range of representable values, INT_MAX (2^31-1) or INT_MIN (-2^31) is returned Example 1: Input: \u0026#34;42\u0026#34; Output: 42 Example 2: Input: \u0026#34; -42\u0026#34; Output: -42 Example 3: Input: \u0026#34;4193 with words \u0026#34; Output: 4193 Example 4: Input: \u0026#34;words and 987\u0026#34; Output: 0 Example 5: Input: \u0026#34;-91283472332\u0026#34; Output: -2147483648 //out of the range of a 32-bit signed integer so INT_MIN is returned ASCII Conversion Recognize that ASCII characters are actually numbers and 0-9 digits are numbers starting from decimal 48 (0x30 hexadecimal)\n\u0026#39;0\u0026#39; is 48 \u0026#39;1\u0026#39; is 49 ... \u0026#39;9\u0026#39; is 57 So to get the value of any character digit you can just remove the \u0026lsquo;0\u0026rsquo;\n\u0026#39;1\u0026#39; - \u0026#39;0\u0026#39; =\u0026gt; 1 49 - 48 =\u0026gt; 1 public int myAtoi(String str) { int index=0, sign=1, total=0; //1. Empty string if (str.length() ==0) return 0; //2. Remove Spaces while(str.charAt(index)==\u0026#39; \u0026#39; \u0026amp;\u0026amp; index \u0026lt; str.length()) index++; //3. Handle signs if (str.charAt(index)==\u0026#39;+\u0026#39; || str.charAt(index)==\u0026#39;-\u0026#39;){ sign= str.charAt(index) == \u0026#39;+\u0026#39; ? 1:-1; index++; } //4. COnvert number and avoid overflow while(index\u0026lt;str.length()){ int digit= str.charAt(index) - \u0026#39;0\u0026#39;; if (digit\u0026lt;0||digit\u0026gt;9) break; //check if total will overflow after 10 times and add digit if (Integer.MAX_VALUE/10 \u0026lt; total || Integer.MAX_VALUE/10 == total \u0026amp;\u0026amp; Integer.MAX_VALUE%10\u0026lt;digit) { return sign==1 ? Integer.MAX_VALUE : Integer.MIN_VALUE; } total= 10* total+digit; index++; } return total*sign; } 9-Palindrome Number Determines whether an interger is a palindrome. An integer is a palindrome when it reads the same backward as forward.\nExample 1: Input: 121 Output: true Example 2: Input: -121 Output: false Explanation: From left to right, it reads -121, meanwhile from right to left it becomes 121- . Therefore it is not a palindrome Example 3: Input: 10 Output: false Explanation: Reads 01 from right to left. Therefore it is not a palindrome Revert Half of the Number A first idea which may come to mind is to convert the number into a string and check if the string is a palindrome but this would require extra non-constant space for creating the string not allowed by the problem description\nSecond idea would be reverting the number itself and comparing the number with the original number, if they are the same then the number is a palindrome, however if the reversed number is larger than int.MAX we will hit integer overflow problem.\nTo avoid the overflow issue of the reverted number, what if we only revert half of the int number? The reverse of the last half of the palindrome should be the same as the first half of the number if the number is a palindrome.\nIf the input is 1221, if we can revert the last part of the number \u0026ldquo;1221\u0026rdquo; from \u0026ldquo;21\u0026rdquo; to \u0026ldquo;12\u0026rdquo; and compare it with the first half of the number \u0026ldquo;12\u0026rdquo;, since 12 is the same as 12, we know that the number is a palindrome.\nAlgorithm\nAt the very beginning we can deal with some edge cases. All negative numbers are not palindrome and numbers ending in zero can only be a palindrome if the first digit is also 0 (only 0 satisfies this property)\nNow let\u0026rsquo;s think about how to revert the last half of the number. For the number 1221 if we do 1221%10 we get the last digit 1. To get the second last digit we divide the number by 10 1221/10=122 and then we can get the last digit again by doing a modulus by 10, 122%10=2. If we multiply the last digit by 10 and add the second last digit 1*10+2=12 which gives us the reverted number we want. COntinuing this process would give us the reverted number with more digits.\nNext is how do we know that we\u0026rsquo;ve reached the half of the number? Since we divided the number by 10 and multiplied the reversed number by 10 when the original number is less than the reversed number, it means we\u0026rsquo;ve gone through half of the number digits.\nclass Solution { public boolean isPalindrome(int x) { if (x\u0026lt;0 || (x%10==0 \u0026amp;\u0026amp; x!=0)) { return false; } int revertedNumber=0; while (x\u0026gt;revertedNumber){ revertedNumber=x%10+revertedNumber*10; x/=10; } //when the length is an odd number, we can get rid of the middle digit by //revertedNumber/10 //For example when the input is 12321, at the end of the while loop we get x=12, //revertedNumber=123, since the middle digit doesn\u0026#39;t matter in a palindrome we can //simply get rid of it return x==revertedNumber||x==revertedNumber/10; } } 10-Regular Expression Matching Given an input string (s) and a pattern (p), implement regular expression matching with support for \u0026lsquo;.\u0026rsquo; and \u0026lsquo;*\u0026rsquo;\n\u0026#39;.\u0026#39; Matches any single character \u0026#39;*\u0026#39; Matches zero or more of the preceding element The matching should cover the entire input string (not partial)\nNote:\ns could be empty and contains only lower case letters a-z p could be empty and contains only lower case letters a-z and characters like . or * Example 1: Input: s=\u0026#34;aa\u0026#34; p=\u0026#34;a\u0026#34; Output: false Explanation: \u0026#34;a\u0026#34; does not match the entire string \u0026#34;aa\u0026#34; Example 2: Input: s=\u0026#34;aa\u0026#34; p=\u0026#34;a*\u0026#34; Output: true Explanation: \u0026#39;*\u0026#39; means zero of more of the preceding element, \u0026#39;a\u0026#39;. Therefore, by repeating \u0026#39;a\u0026#39; once it becomes \u0026#34;aa\u0026#34; Example 3: Input: s=\u0026#34;ab\u0026#34; p=\u0026#34;.*\u0026#34; Output: true Explanation: \u0026#39;.*\u0026#39; means \u0026#34;zero or more (*) of any character (.)\u0026#34; Example 4: Input: s=\u0026#34;aab\u0026#34; p=\u0026#34;c*a*b\u0026#34; Output: true Explanation: c can be repeated 0 times, a can be repeated 1 time. Therefore it matches \u0026#34;aab\u0026#34; Example 5: Input: s=\u0026#34;mississippi\u0026#34; p=\u0026#34;mis*is*p*.\u0026#34; Output: false Recursion If there were no Kleene stars (the * wildcard characters for regular expressions), the problem would be easier- we simply check from left to right if each character of the text matches the pattern. When a star is present we may need to check for may different suffixes of the text and see if they match the rest of the pattern. A recursive solution is a straightforward way to represent this relationship\nclass Solution { public boolean isMatch(String text, String pattern) { if (pattern.isEmpty()) return text.isEmpty(); boolean first_match=(!text.isEmpty() \u0026amp;\u0026amp; (pattern.charAt(0)==text.charAt(0) || pattern.charAt(0)==\u0026#39;.\u0026#39;)); if (pattern.length()\u0026gt;=2 \u0026amp;\u0026amp; pattern.charAt(1) ==\u0026#39;*\u0026#39;){ return (isMatch(text,pattern.substring(2))|| (first_match \u0026amp;\u0026amp; isMatch(text.substring(1),pattern))); //note: pattern.substring(2) returns all of the characters after index 2 of pattern } else { return first_match \u0026amp;\u0026amp; isMatch(text.substring(1), pattern.substring(1)); } } } Complexity Analysis\nTime Complexity: Let T, P be the lengths of the text and the pattern respectively. In the worst case, a call to match(text[i:],pattern[2j:]) will be made (i+j i) times, and strings of the order O(T-i) and O(P-2*j) will be made. Thus the complexity has the order: summation from i=0 to T * summation from j=0 to P/2 * (i+j i) O(T+P-i-2j). We can show that this is bounded by O((T+P)2^(T+P/2)) Space Complexity:\tFor every call to match, we will create those strings as described above possibly creating duplicates. If memory is not freed, this will also take a total of O((T+P)2^(T+P/2)) space even though there are only order O(T^2+P^2) unique suffixes of P and T that are actually required Dynamic Programming As the problem has an optimal substructure, it is natural to cache intermediate results. We ask the question dp(i,j): does text[i:] and pattern[j:] match? We can describe our answer in terms of answers to questions involving smaller strings\nAlgorithm\nWe proceed with the same recursion as in Approach 1, except because calls will only ever be made to match(text[i:], pattern[j:]), we use dp(i,j) to handle those calls instead, saving us expensive string-building operations and allowing us to cache the intermediate results\nJava Top-Down Variation\nenum Result { TRUE, FALSE } class Solution { Result[][] memo; public boolean isMatch(String text, String pattern) { memo=new Result[text.length() +1][pattern.length() +1]; return dp(0,0,text,pattern); } public boolean dp(int i, int j, String text, String pattern) { if (memo[i][j]!=null) { return memo[i][j]==Result.TRUE; } boolean ans; if (j==pattern.length()){ ans=i==text.length(); } else { boolean first_match=(i\u0026lt;text.length() \u0026amp;\u0026amp; (pattern.charAt(j) == text.charAt(i) || patter.charAt(j) == \u0026#39;.\u0026#39;)); if (j+1\u0026lt;pattern.length() \u0026amp;\u0026amp; pattern.charAt(j+1)==\u0026#39;*\u0026#39;){ ans=(dp(i,j+1,text,pattern)||first_match\u0026amp;\u0026amp; dp(i+1,j,text,pattern)); } else { ans=first_match \u0026amp;\u0026amp; dp(i+1, j+1, text, pattern); } } memo[i][j]=ans? Result.TRUE: Result.FALSE; return ans; } } Complexity Analysis\nTime Complexity: Let T, P be the lengths of the text and the pattern respectively. The work for every call to dp(i,j) for i=0,...,T; j=0,...,P is done once and it is O(1) work. Hence the time complexity is O(TP) Space Complexity:\tThe only memory we use is the O(TP) boolean entries in our cache. Hence, the space complexity is O(TP) Non-Recursive The recursive programming solutions are pretty confusing so this implementation uses 2D arrays and Dynamic Programming\nThe logic works as follows:\n1. If p.charAt(j) == s.charAt(i) : dp[i][j] = dp[i-1][j-1]; 2. If p.charAt(j) == \u0026#39;.\u0026#39; : dp[i][j] = dp[i-1][j-1]; 3. If p.charAt(j) == \u0026#39;*\u0026#39;: Subconditions 1. If p.charAt(j-1)!= s.charAt(i):dp[i][j]=dp[i][j-2] //in this case a* only counts as empty 2. If p.charAt(i-1)== s.charAt(i) or p.charAt(i-1) == \u0026#39;.\u0026#39;: dp[i][j] = dp[i-1][j]\t//in this case a* counts as multiple a or dp[i][j] = dp[i][j-1]\t//in this case a* counts as single a or dp[i][j] = dp[i][j-2]\t//in this case a* counts as empty public boolean isMatch(String s, String p) { if (s==null || p==null){ return false; } boolean[][] dp=new boolean[s.length()+1][p.length()+1]; dp[0][0]=true; for (int i=0;i\u0026lt;p.length(); i++){ if (p.charAt(i)==\u0026#39;*\u0026#39; \u0026amp;\u0026amp; dp[0][i-1]){ dp[0][i+1]=true; } } for (int i=0;i\u0026lt;s.length();i++){ for (int j=0;j\u0026lt;p.length();j++){ if (p.charAt(j)==\u0026#39;.\u0026#39;){ dp[i+1][j+1]=dp[i][j]; } if (p.charAt(j)==s.charAt(i)){ dp[i+1][j+1]=dp[i][j]; } if (p.charAt(j)==\u0026#39;*\u0026#39;){ if (p.charAt(j-1)!=s.charAt(i) \u0026amp;\u0026amp; p.charAt(j-1) !=\u0026#39;.\u0026#39;){ dp[i+1][j+1]=dp[i+1][j-1]; } else{ dp[i+1][j+1]=(dp[i+1][j] || dp[i][j+1] || dp[i+1][j-1]); } } } } return dp[s.length()][p.length()]; } 11-Container with the Most Water Given n non negative integers a1,a2, \u0026hellip; , an where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forns a container such that the container contains the most water.\n^\t^ These two values form the container which could hold water at a max height of 7, these values are also 7 array indexes apart from each other so it could hold water at a max width of 7. The area of water which could be held is thus 7 x 7 = 49 Brute Force In this case we simply consider the area for every possible pair of the lines and find out the maximum area out of those.\npublic class Solution { public int maxArea(int[] height) { int maxarea=0; for (int i=0; i\u0026lt;height.length; i++){ for (int j=i+1;j\u0026lt;height.length;j++){ maxarea=Math.max(maxarea, Math.min(height[i],height[j])*(j-i)); } } return maxarea; } } Complexity Analysis\nTime complexity: O(n^2) Calculating the area for all n(n-1)/2 height pairs Space complexity: O(1) Constant extra space is used Two Pointer Approach The intuition behind this approach is that the area formed between the lines will always be limited by the height of the shorter line. Further, the farther the lines, the more will be the area obtained.\nWe take two pointers, one at the beginning and one at the end of the array constituting the length of the lines. Further, we maintain a variable maxarea to store the maximum area obtained till now. At every step, we find out the area formed between them, update maxarea and move the pointer pointing to the shorter line towards the other end by one step.\nInitially we consider the area constituting the exterior most lines. Now to maximize the area we need to consider the area between the lines of larger lengths. If we try to move the pointer at the longer line inwards, we won\u0026rsquo;t gain any increase in area, since it is limited by the shorter line. But moving the shorter line\u0026rsquo;s pointer could turn out to be benefical, as per the same argument, despite the reduction in width. This is done since a relatively longer line obtained by moving the shorter line\u0026rsquo;s pointer might overcome the reduction in area caused by the width reduction.\npublic class Solution { public int maxArea(int[] height) { int maxarea=0, l=0, r=height.length-1; while (l\u0026lt;r){ maxarea=Math.max(maxarea,Math.min(height[l],height[r])*(r-l)); if (height[l]\u0026lt;height[r]){ l++; } else{ r--; } } return maxarea; } } Complexity Analysis\nTime complexity: O(n) Single pass Space complexity: O(1) Constant space is used 12-Integer To Roman Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M\nSymbol\tValue I\t1 V\t5 X\t10 L\t50 C\t100 D\t500 M\t1000 For example, two is written as II in Roman numeral, just two one\u0026rsquo;s added together. Twelve is written as XII which is simply X + II. The number twenty seven is written as XXVII, which is XX + V + II.\nRoman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine which is written as IX. There are six instances where subtraction is used:\nI can be placed before V (5) and X (10) to make 4 and 9 X can be placed before L (50) and C(100) to make 40 and 90 C can be placed before D (500) and M(1000) to make 400 and 900 Given an integer, convert it to a roman numeral, input is guaranteed to be within the range from 1 to 3999\nExample 1: Input: 3 Output: \u0026#34;III\u0026#34; Example 2: Input: 4 Output: \u0026#34;IV\u0026#34; Example 3: Input: 9 Output: \u0026#34;IX\u0026#34; Example 4: Input: 58 Output: \u0026#34;LVIII\u0026#34; Explanation: L=50, V=5, III=3 Example 5: Input: 1994 Output: \u0026#34;MCMXCIV\u0026#34; Explanation: M=1000, CM=900, XC=90 and IV=4 String Array public static String intToRoman(int num) { String M[]={\u0026#34;\u0026#34;, \u0026#34;M\u0026#34;, \u0026#34;MM\u0026#34;, \u0026#34;MMM\u0026#34;}; //represents 1000, 2000, and 3000 since we know the number is in the range 1 to 3999 String C[]={\u0026#34;\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;CC\u0026#34;, \u0026#34;CCC\u0026#34;, \u0026#34;CD\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;DC\u0026#34;, \u0026#34;DCC\u0026#34;, \u0026#34;DCCC\u0026#34;, \u0026#34;CM\u0026#34;}; //represents 0, 100, 200, 300, 400, 500, 600, 700, 800, 900 String X[]={\u0026#34;\u0026#34;, \u0026#34;X\u0026#34;, \u0026#34;XX\u0026#34;, \u0026#34;XXX\u0026#34;, \u0026#34;XL\u0026#34;, \u0026#34;L\u0026#34;, \u0026#34;LX\u0026#34;, \u0026#34;LXX\u0026#34;, \u0026#34;LXXX\u0026#34;, \u0026#34;XC\u0026#34;}; //represents 0, 10, 20, 30, 40, 50, 60, 70, 80, 90 String I[]={\u0026#34;\u0026#34;, \u0026#34;I\u0026#34;, \u0026#34;II\u0026#34;, \u0026#34;III\u0026#34;, \u0026#34;IV\u0026#34;, \u0026#34;V\u0026#34;, \u0026#34;VI\u0026#34;, \u0026#34;VII\u0026#34;, \u0026#34;VIII\u0026#34;, \u0026#34;IX\u0026#34;}; //represents 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 return M[num/1000] + C[(num%1000)/100] + X[(num%100)/10] + I[num%10]; } 13-Roman to Integer Roman numerals are represented by seven different symbols I, V, X, L, C, D and M\nSymbol Value I\t1 V\t5 X\t10 L\t50 C\t100 D\t500 M\t1000 For example, two is written as II in Roman numeral, just two one\u0026rsquo;s added together. Twelve is written as XII which is simply X + II. The number twenty seven is written as XXVII, which is XX + V + II.\nRoman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine which is written as IX. There are six instances where subtraction is used:\nI can be placed before V (5) and X (10) to make 4 and 9 X can be placed before L (50) and C(100) to make 40 and 90 C can be placed before D (500) and M(1000) to make 400 and 900 Given an integer, convert it to a roman numeral, Input is guaranteed to be within the range from 1 to 3999\nExample 1: Input: \u0026#34;III\u0026#34; Output: 3 Example 2: Input: \u0026#34;IV\u0026#34; Output: 4 Example 3: Input: \u0026#34;IX\u0026#34; Output: 9 Example 4: Input: \u0026#34;LVIII\u0026#34; Output: 58 Explanation: L=50, V=5, III=3 Example 5: Input: \u0026#34;MCMXCIV\u0026#34; Output: 1994 Explanation: M=1000, CM=900, XC=90 and IV=4 Character Array class Solution { public int romanToInt(String s) { Map\u0026lt;Character, Integer\u0026gt; map = new HashMap(); map.put(\u0026#39;I\u0026#39;, 1); map.put(\u0026#39;V\u0026#39;, 5); map.put(\u0026#39;X\u0026#39;, 10); map.put(\u0026#39;L\u0026#39;, 50); map.put(\u0026#39;C\u0026#39;, 100); map.put(\u0026#39;D\u0026#39;, 500); map.put(\u0026#39;M\u0026#39;, 1000); char[] sc= s.toCharArray(); int total= map.get(sc[0]); int pre=map.get(sc[0]); for (int i=1; i\u0026lt;sc.length; i++) { int curr=map.get(sc[i]); if (curr\u0026lt;=pre) { total= total + curr; } else { total=total+curr -2*pre; } pre=curr; } return total; } } 14-Longest Common Prefix Write a function to find the longest common prefix string amongst an array of strings. If there is no common prefix, return an empty string \u0026quot;\u0026quot;\nExample 1: Input: [\u0026#34;flower\u0026#34;, \u0026#34;flow\u0026#34;, \u0026#34;flight\u0026#34;] Output: \u0026#34;fl\u0026#34; Example 2: Input: [\u0026#34;dog\u0026#34;, \u0026#34;racecar\u0026#34;, \u0026#34;car\u0026#34;] Output: \u0026#34;\u0026#34; Explanation: There is no common prefix among the input strings Note: All given inputs are in lowercase letters a-z\nHorizontal Scanning *Intuition:* For a start we will describe a simple way of find the longest prefix shared by a set of strings LCP(S1 \u0026hellip; Sn).We will use the observation that:\nLCP(S1 ... Sn) = LCP(LCP(LCP(S1, S2), S3), ... Sn) Algorithm:\nTo employ this idea, the algorithm iterates through the strings [S1 \u0026hellip; Sn]. finding at each iteration i the longest common prefix of strings LCP(S1 \u0026hellip; Si). When LCP(S1 \u0026hellip; Si) is an empty string, the algorithm ends. Otherwise after n iterations, the algorithm returns LCP(S1 \u0026hellip; Sn)\nExample: {leets, leetcode, leet, leeds} \\ / LCP{1,2} = leets leetcode leet \\\t{leets, leetcode, leet, leeds} \\ / LCP{1,3} = leet leet leet \\ {leets, leetcode, leet, leeds} \\ / LCP{1,4} leet leeds lee LCP{1,4} = \u0026#34;lee\u0026#34; public String longestCommon Prefix(String[] strs){ if (strs.length==0){ return \u0026#34;\u0026#34;; } String prefix=strs[0]; for (int i=1; i\u0026lt;strs.length; i++) { while (strs[i].indexOf(prefix) != 0) { prefix=prefix.substring(0, prefix.length() -1); if (prefix.isEmpty()) { return \u0026#34;\u0026#34;; } } return prefix; } } Complexity Analysis\nTime complexity: O(S)\tWhere S is the sum of all characters in all strings. In the worse case all n strings are the same. The algorithm compares the string S1 with the other strings [S2 ... Sn]. There are S character comparisons where S is the sum of all characters in the input array Space complexity: O(1) We only used constant extra space Vertical Scanning Imagine a very short string is at the end of the array. The above approach will still do S comparisons. One way to optimize this case is to do vertical scanning. We compare characters from top to bottom on the same column (same character index of the strings) before moving on to the next column.\npublic String longestCommonPrefix(String[] strs) { if (strs==null || strs.length==) return \u0026#34;\u0026#34;; for (int i=0; i\u0026lt;strs[0].length(); i++){ char c=strs[0].charAt(i); for (int j=1; j\u0026lt;strs.length; j++) { if (i==strs[j].length() || strs[j].charAt(i)!=c){ return strs[0].substring(0,i); } } } return strs[0]; } Complexity Analysis\nTime complexity: O(S) Where S is the sum of all characters in all strings. In the worst case there will be n equal strings with length m and the algorithm performs S=n*m character comparisons. Even the worst case is still the same as Approach 1, in the best case there are at most n*minLen comparisons where minLen is the length of the shortest string in the array. Space complexity: O(1)\tWe only used constant extra space Divide and Conquer The idea of the algorithm comes from the associative property of LCP operation. We notice that: LCP(S1 \u0026hellip; Sn) = LCP(LCP(S1 \u0026hellip; Sk), LCP(Sk+1 \u0026hellip; Sn)), where LCP(S1 \u0026hellip; Sn) is the longest common prefix in a set of strings [S1 \u0026hellip; Sn], 1\u0026lt;k\u0026lt;n\nAlgorithm\nTo apply the previous observation, we use the divide and conquer technique, where we split the LCP(Si \u0026hellip; Sj) problem into two subproblems LCP(Si \u0026hellip; Smid) and LCP(Smid+1 \u0026hellip; Sj), where mid is (i+j)/2. We use their solutions lcpLeft and lcpRight to construct the solution of the main problem LCP(Si \u0026hellip; Sj). To accomplish this we compare one by one the characters of lcpLeft and lcpRight till there is no character match. The found common prefix of lcpLeft and lcpRight is the solution of the LCP(Si \u0026hellip; Sj)\n{leetcode, leet, lee, le} / \\ Divide {leetcode, leet} {lee, le} Conquer\t|\t| {leet} {le} \\ / {le} Searching for the longest common prefix (LCP) in dataset {leetcode, leet, lee, le} public String longestCommonPrefix(String[] strs) { if (strs == null || strs.length ==0) return \u0026#34;\u0026#34;; return longestCommonPrefix(strs, 0, strs.length-1); } private String longestCommonPrefix(String[] strs, int l, int r) { if (l==r) { return strs[l]; } else { int mid=(l+r)/2; String lcpLeft= longestCommonPrefix(strs,l, mid); String lcpRight= longestCommonPrefix(strs,mid+1;r); return commonPrefix(lcpLeft,lcpRight); } } String commonPrefix(String left, String right) { int min=Math.min(left.length(), right.length()); for (int i=0; i\u0026lt;min; i++) { if (left.charAt(i) !=right.charAt(i) ){ return left.substring(0, i); } } return left.substring(0, min); } Complexity Analysis\nIn the worst case we have n equal strings with length m\nTime Complexity: O(S)\twhere S is the number of all characters in the array, S=m*n so time complexity is 2*T(n/2)+O(m). Therefore time complexity is O(S). In the best case the algorithm performs O(minLen * n) comparisons, where minLen is the shortest string of the array Space Complexity: O(m*log(n))\tThere is a memory overhead since we sotre recursive call in the execution stack. There are log(n) recursive calls, each store needs m space to store the result so space complexity is O(m*log(n)) Binary Search The idea is to apply binary search method to find the string with maximum value L, which is common prefix of all the strings. The algorithm searches the space in the interval (0 \u0026hellip; minLen), where minLen is minimum string length and the maximum possible common prefix. Each time search space is divided in two equal parts, one of them is discarded because it is sure that it doesn\u0026rsquo;t contain the solution. There are two possible cases:\nS[1\u0026hellip;mid] is not a common string. This means that for each j\u0026gt;i, S[1\u0026hellip;j] is not a common string and we discard the second half of the search space S [1\u0026hellip;mid] is common string. This means that for each i\u0026lt;j, S[1\u0026hellip;i] is a common string and we discard the first half of the search space, because we try to find longer common prefix {leets, leetcode, leetc, leeds} | \u0026#34;leets\u0026#34; / \\ \u0026#34;lee\u0026#34; \u0026#34;ts\u0026#34; midpoint \u0026#34;lee\u0026#34; in \u0026#34;leetcode\u0026#34; : yes \u0026#34;lee\u0026#34; in \u0026#34;leetc\u0026#34; : yes \u0026#34;lee\u0026#34; in \u0026#34;leeds\u0026#34; : yes | \u0026#34;leets\u0026#34; / \\ \u0026#34;lee\u0026#34; \u0026#34;ts\u0026#34; | / \\ \u0026#34;lee\u0026#34; \u0026#34;t\u0026#34; \u0026#34;s\u0026#34; midpoint \u0026#34;leet\u0026#34; in \u0026#34;leetcode\u0026#34; : yes \u0026#34;leet\u0026#34; in \u0026#34;leetc\u0026#34; : yes \u0026#34;leet\u0026#34; in \u0026#34;leeds\u0026#34; : no LCP= \u0026#34;lee\u0026#34; public String longestCommonPrefix(String[] strs) { if (strs==null || strs.length==0) return \u0026#34;\u0026#34;; int minLen=Integer.MAX_VALUE; for (String str: strs) minLen=Math.min(minLen, str.length()); int low=1; int high=min Len; while (low\u0026lt;=high) { int middle=(low+high)/2; if (isCommonPrefix(strs, middle) low=middle+1; else high=middle-1; } return strs[0].substring(0, (low + high)/2); } private boolean isCommonPrefix(String[] strs, int len) { String str1=strs[0].substring(0,len); for (int i=1; i\u0026lt;strs.length; i++) if (!strs[i].startsWith(str1)) return false; return true; } **Complexity Analysis\nIn the worst case we have n equal strings with length m\nTime complexity: O(S * log(n)), where S is the sum of all characters in all strings. The algorithm makes log(n) iterations, for each of them there are S=m*n comparisons, which gives in total O(S * log(n)) time complexity Space complexity: O(1). We only used constant extra space Further Thoughts Considering a slightly different problem:\nGiven a set of keys S= [S1, S2 ... Sn], find the longest common prefix among a string q and S. This LCP query will be called frequently We coule optimize LCP queries by storing the set of keys S in a Trie. See this for Trie implementation. In a Trie, each node descending from the root represents a common prefix of some keys. But we need to find the longest common prefix of a string q and all key strings. This means that we have to find the deepest path from the root, which satisfies the following conditions\nit is a prefix of query string q each node along the path must contain only one child element. Otherwise the found path will not be a common prefix among all strings the path doesn\u0026rsquo;t comprise of nodes which are marked as end of key. Otherwise the path couldn\u0026rsquo;t be a prefix of a key which is shorter than itself Algorithm\nThe only question left is how to find the deepest path in the Trie, that fulfills the requirements above. The most effective way is to build a trie from {S1 \u0026hellip; Sn] strings. Then find the prefix of query string q in the Trie. We traverse the Trie from the root, till it is impossible to continue the path in the Trie because one of the conditions above is not satisfied.\nSearching for the longest common prefix of string \u0026#34;le\u0026#34; in a Trie from dataset {lead, leet} Root 1 l ===========\u0026gt; \\ l 2 e ===============\u0026gt; \\ e LCP \u0026#34;le\u0026#34; FOUND\t=============\u0026gt; 3 a\t/ \\ e End of Key \u0026#34;lee\u0026#34; 6 4 d /\t\\ t END OF KEY \u0026#34;lead\u0026#34;\t7\t5 End of key \u0026#34;leet\u0026#34; public String longestCommonPrefix(String q, String[] strs) { if (strs == null || strs.length == 0) return \u0026#34;\u0026#34;; if (strs.length == 1) return strs[0]; Trie trie = new Trie(); for (int i = 1; i \u0026lt; strs.length ; i++) { trie.insert(strs[i]); } return trie.searchLongestPrefix(q); } class TrieNode { // R links to node children private TrieNode[] links; private final int R = 26; private boolean isEnd; // number of children non null links private int size; public void put(char ch, TrieNode node) { links[ch -\u0026#39;a\u0026#39;] = node; size++; } public int getLinks() { return size; } //assume methods containsKey, isEnd, get, put are implemented as it is described //in https://leetcode.com/articles/implement-trie-prefix-tree/) } public class Trie { private TrieNode root; public Trie() { root = new TrieNode(); } //assume methods insert, search, searchPrefix are implemented private String searchLongestPrefix(String word) { TrieNode node = root; StringBuilder prefix = new StringBuilder(); for (int i = 0; i \u0026lt; word.length(); i++) { char curLetter = word.charAt(i); if (node.containsKey(curLetter) \u0026amp;\u0026amp; (node.getLinks() == 1) \u0026amp;\u0026amp; (!node.isEnd())) { prefix.append(curLetter); node = node.get(curLetter); } else return prefix.toString(); } return prefix.toString(); } } Complexity Analysis\nIn the worst case query q has length m and is equal to all n strings of the array Time Complexity: O(S) where S is the number of all characters in the array, LCP query O(m) Trie build has O(S) time complexity. To find the common prefix of q in the Trie takes in the worst O(m). Space complexity: O(S) we only used additional S extra space for the Trie. 15-3Sum Given an array \u0026ldquo;nums\u0026rdquo; of n integers, are there elements a, b, c in nums such that a+b+c=0? Find all unique triplets in the array which gives the sum of zero.\nNote:\nThe solution set must not contain duplicate triplets\nExample: Given array nums = [-1, 0, 1, 2, -1, -4]. A solution set is: [ [-1, 0, 1], [-1, -1, 2] ] Sorted Array The method is to sort an input array and then run through all indices of a possible first element of a triplet. For each element we make another 2Sum sweep of the remaining part of the array. Also we want to skip elements to avoid duplicates in the answer without expending extra memory.\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; threeSum(int[] num) { //Arrays.sort re-arranges the array of integers in ascending order //ex. [1, 2, 3, 4] Arrays.sort(num); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; num.length-2; i++) { if (i == 0 || (i \u0026gt; 0 \u0026amp;\u0026amp; num[i] != num[i-1])) { //This lets us skip some of the duplicate entries in the array int lo = i+1, hi = num.length-1, sum = 0 - num[i]; //This is for the 2 Sum sweep while (lo \u0026lt; hi) { if (num[lo] + num[hi] == sum) { res.add(Arrays.asList(num[i], num[lo], num[hi])); while (lo \u0026lt; hi \u0026amp;\u0026amp; num[lo] == num[lo+1]) lo++; while (lo \u0026lt; hi \u0026amp;\u0026amp; num[hi] == num[hi-1]) hi--; //This lets us skip some of the duplicate entries in the array lo++; hi--; } else if (num[lo] + num[hi] \u0026lt; sum) lo++; else hi--; //This allows us to optimize slightly since we know that the array is sorted } } } return res; } Complexity Analysis\nTime Complexity: O(n^2) We go through a maximum of n elements for the first element of a triplet, and then when making a bi-directional 2Sum sweep of the remaining part of the array we also go through a maxiumum of n elements. Space Complexity: O(1)\tIf we assume the return linked list is not extra space, then we do not allocate any significant extra space 16-3Sum Closest Given an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution.\nExample: Given array nums=[-1, 2, 1, -4], and target=1. The sum that is closest to the target is 2. (-1+2+1=2) 3 Pointers Similar to the previous 3Sum problem, we use three pointers to point to the current element, next element and the last element. If the sum is less than the target, it means that we need to add a larger element so next element move to the next. If the sum is greater, it means we have to add a smaller element so last element move to the second last element. Keep doing this until the end. Each time compare the difference between sum and target, if it is less than minimum difference so far, then replace result with it, otherwise continue iterating.\npublic class Solution { public int threeSumClosest(int[] num, int target) { int result=num[0] + num[1] + num[num.length-1]; Arrays.sort(num); for (int i=0; i\u0026lt;num.length -2; i++) { int start= i+1, end = num.length -1; while (start \u0026lt; end) { int sum = num[i] + num[start] + num[end]; if (sum \u0026gt; target) { end--; } else { start++; } if (Math.abs(sum-target) \u0026lt; Math.abs(result-target)) { result=sum; } } } return result; } } 17-Letter Combinations of a Phone Number Given a string contianing digits from 2-9 inclusive, return all possible letter combinations that the number could represent.\nA mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.\n2 - abc 3 - def 4 - ghi\t5 - jkl\t6 - mno\t7 - pqrs 8 - tuv 9 - wxyz Example: Input: \u0026#34;23\u0026#34; Output: [\u0026#34;ad\u0026#34;, \u0026#34;ae\u0026#34;, \u0026#34;af\u0026#34;, \u0026#34;bd\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;bf\u0026#34;, \u0026#34;cd\u0026#34;, \u0026#34;ce\u0026#34;, \u0026#34;cf\u0026#34;]. Note: The above answer is in lexicographical order but the answer can be in any order\nBacktracking Backtracking is an algorithm for finding all solutions by exploring all potential candidates. If the solution candidate turns to not be a solution (or at least not the last one), backtracking algorithm discards it by making some changes on the previous step, ie backtracks and then tries again.\nHere is a backtrack function backtrack(combination, next_digits) which takes as arguments an ongoing letter combination and the next digits to check.\nIf there are no more digits to check that means the current combination is done If there are still digits to check: Iterate over the letters mapping to the next available digit Append the current letter to the current combination and proceed to check next digits: combination = combination + letter backtrack(combination + letter, next_digits[1:]). Visual Representation\nComplexity Analysis\nTime Complexity: O(3^N * 4^M) where N is the number of digits in the input that maps to 3 letters (eg. 2, 3, 4, 5, 6, 8) and M is the number of digits in the input that maps to 4 letters (eg. 7, 9) and N+M is the total number digits in the input Space Complexity: O(3^N * 4^M)\tsince one has to keep 3^N * 4^M solutions First In First Out (FIFO) Queue This solution utilizes the Single Queue Breadth First Search (BFS) which is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root and explores all of the neighbor nodes.\npublic List\u0026lt;String\u0026gt; letterCombinations(String digits) { LinkedList\u0026lt;String\u0026gt; ans = new LinkedList\u0026lt;String\u0026gt;(); if (digits.isEmpty()) return ans; String[] mapping = new String[] {\u0026#34;0\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;, \u0026#34;ghi\u0026#34;, \u0026#34;jkl\u0026#34;, \u0026#34;mno\u0026#34;, \u0026#34;pqrs\u0026#34;, \u0026#34;tuv\u0026#34;, {wxyz\u0026#34;}; ans.add(\u0026#34;\u0026#34;); for (int i = 0; i\u0026lt;digits.length(); i++) { int x = Character.getNumericValue(digits.charAt(i)); //we terminate the while loop when we encounter a new-formed string which is more than //the current level i //peek retrieves the first value of the linked list while (ans.peek().length==i){ //removes the head or the first value in the linkedlist String t = ans.remove(); for (char s : mapping[x].toCharArray()) { ans.add(t+s); //this works because add appends to the end of the list } } return ans; } } Complexity Analysis\nTime Complexity: O(3^N * 4^M) where N is the number of digits in the input that maps to 3 letters (eg. 2, 3, 4, 5, 6, 8) and M is the number of digits in the input that maps to 4 letters (eg. 7, 9) and N+M is the total number digits in the input Space Complexity: O(3^N * 4^M)\tsince one has to keep 3^N * 4^M solutions 18-4Sum Given an array nums of n integers and an integer target, are there elements a, b, c, and d in nums such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target\nNote: The solution set must not contain duplicate quadruplets\nExample: Given array nums = [1, 0, -1, 0, -2, 2], and target = 0 A solution set is: [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] Sorted Array The idea is the same as the other numbered sum problems like 2sum and 3sum. We sort the array and then proceed to interate through the values until we end up with a result that we are looking for.\npublic class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; fourSum(int[] num, int target) { ArrayList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans = new ArrayList\u0026lt;\u0026gt;(); if (num.length\u0026lt;4) { return ans; } Arrays.sort(num); for (int i=0; i\u0026lt;num.length-3; i++) { //picking the first candidate must leave room //for the other values if (num[i]+num[i+1]+num[i+2]+num[i+3]\u0026gt;target) { break; //first candidate too large, search finished } if (num[i]+num[num.length-1]+num[num.length-2]+num[num.length-3]\u0026lt;target) { continue; //first candidate too small } if(i\u0026gt;0 \u0026amp;\u0026amp; num[i]==num[i-1]) { continue; //prevents duplicate in ans list } for (int j=i+1; j\u0026lt;num.length-2; j++) { //picking the second candidate must //leave room for other values if (num[i]+num[j]+num[j+1]+num[j+2]\u0026gt;target) { break; //second candidate too large } if (num[i]+num[j]+num[num.length-1]+num[num.length-2]\u0026lt;target) { continue; //second candidate too small } if(j\u0026gt;i+1 \u0026amp;\u0026amp; num[j]==num[j-1]) { continue; //prevents duplicate results in ans list } int low=j+1, high=num.length-1; //two pointer search while(low\u0026lt;high) { int sum=num[i]+num[j]+num[low]+num[high]; if (sum==target) { ans.add(Arrays.asList(num[i],num[j],num[low],num[high])); while(low\u0026lt;high\u0026amp;\u0026amp;num[low]==num[low+1]) { low++; //skipping over duplicates } while(low\u0026lt;high \u0026amp;\u0026amp; num[high]==num[high-1] { high--; //skipping over duplicates } low++; high--; } //moving window else if (sum\u0026lt;target) { low++; } else { high--; } } } } return ans; } } 19-Remove Nth Node From End of List Given a linked list, remove the n-th node from the end of the list and return its head\nExample: Given linked list: 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5, and n=2 After removing the second node from the end, the linked list becomes 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 5 Note: Given n will always be valid\nFollow up: Could you do this in one pass?\nTwo Pass Algorithm Intuition\nWe notice that the problem could be simply reduced to another one: Remove the (L-n+1)th node from the beginning of the list, where L is the list length. This problem is easy to solve once we found the list length L.\nAlgorithm\nFirst we will add an auxiliary \u0026ldquo;dummy\u0026rdquo; node, which points to the list head. The \u0026ldquo;dummy\u0026rdquo; node is used to simplify some corner cases such as a list with only one node or removing the head of the list. On the first pass, find the list length L. Then we set a pointer to the dummy node and start to move it through the list till it comes to the (L-n)th node. We relink next pointer of the (L-n)th node to the (L-n+2)th node and we are done.\nD -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; NULL | v D -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 4 -\u0026gt; NULL public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; int length =0; ListNode first = head; while (first!=null) { length++; first=first.next; } length -= n; first = dummy; while (length\u0026gt;0) { length--; first=first.next; } first.next=first.next.next; return dummy.next; } Complexity Analysis\nTime Complexity: O(L) The algorithm makes two traversals of the list, first to calculate the list length L and second to find the (L-n)th node. There are 2L-n operations and time complexity is O(L) Space Complexity: O(1) We only used constant extra space One Pass Algorithm The previous algorithm could be optimized to one pass. Instead of one pointer, we could use two pointers. The first pointer advances the list by n+1 steps from the beginning, while the second pointer starts from the beginning of the list. Now, both pointers are separated by exactly n nodes. We maintain this constant gap by advancing both pointers together until the first pointer arrives past the last node. The second pointer will be pointing at the nth node counting from the last. We relink the next pointer of the node referenced by the second pointer to point to the node\u0026rsquo;s next next node.\nMaintaining N=2 nodes apart between the first and second pointer D\t-\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; NULL first Head second Move the first pointer N+1 steps | v D\t-\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; NULL second Head First Move the first and second pointers together until the first pointer arrives past the last node | v D\t-\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; NULL Head Second First Second pointer points to the nth node counting from last so link node to the node\u0026#39;s next next node | v D\t-\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; -\u0026gt; 5 -\u0026gt; NULL Head Second First public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode first = dummy; ListNode second = dummy; //Moves the first pointer so that the first and second nodes are separated by n nodes for (int i=1; i\u0026lt;=n+1; i++) { first = first.next; } //Move first to the end, maintaining the gap while (first!=null) { first=first.next; second=second.next; } second.next=second.next.next; return dummy.next; } Complexity Analysis\nTime Complexity: O(L) The algorithm makes one traversal of the list of L nodes. Therefore time complexity is O(L) Space Complexity: O(1)\tOnly constant extra space was used 20-Valid Parentheses Given a string containing just the characters \u0026lsquo;(\u0026rsquo;, \u0026lsquo;)\u0026rsquo;, \u0026lsquo;{\u0026rsquo;, \u0026lsquo;}\u0026rsquo;, \u0026lsquo;[\u0026rsquo;, \u0026lsquo;]\u0026rsquo;, determine if the input string is valid\nAn input string is valid if:\nOpen brackets must be closed by the same type of brackets Open brackets must be closed in the correct order Note that an empty string is also considered valid\nExample 1: Input: \u0026#34;()\u0026#34; Output: true Example 2: Input: \u0026#34;()[]{}\u0026#34; Output: true Example 3: Input: \u0026#34;(]\u0026#34; Output: false Example 4: Input: \u0026#34;([)]\u0026#34; Output: false Example 5: Input: \u0026#34;{[]}\u0026#34; Output: true Counting method Intuition\nImagine you are writing a small compiler for your college project and one of the tasks or sub-tasks for the compiler would be to detect if the parenthesis are in place or not.\nThe algorithm we will look at in this article can be then used to process all the parenthesis in the program your compiler is compiling and checking if all the parenthesis are in place. This makes checking if a given string of parenthesis is valid or not, an important programming problem.\nThe expressions that we will deal with in this problem can consist of three different types of parenthesis:\n() {} [] Before looking at how we can check if a given expression consisting of thes parenthesis is valid or not, let us look at a simpler version of the problem that consists of just one type of parenthesis. So, the expressions we can encounter in this simplified version of the problem are:\n(((((()))))) -- VALID ()()()() -- VALID (((((((() -- INVALID ((()(()))) -- VALID Let\u0026rsquo;s look at a simple algorithm to deal with this problem\nWe process the expression one bracket at a time starting from the left\nSuppose we encounter an opening bracket ie. (, it may or may not be an invalid expression because there can be a matching ending bracket somewhere in the remaining part of the expression. Here, we simply increment the counter keeping track of the left parenthesis till now. left += 1\nIf we encounter a closing bracket, this has two meanings:\nThere was no matching opening bracket for this closing bracket and in that case we have an invalid expression. This is the case when left==0 ie. when there are no unmatched left brackets available\nWe had some unmatched opening bracket available to match this closing bracket. This is the case when left\u0026gt;0 ie. we have unmatched left brackets available\nIf we encounter a closing bracket ie. ) when left==0, then we have an invalid expression on our hands. Else, we decrement left thus reducing the number of unmatched left parenthesis available.\nContinue processing the string until all parenthesis have been processed\nIf in the end we still have an unmatched left parenthesis available, this implies an invalid expression\nThe reason we discussed this particular algorithm here is because the approach for the approach for the original problem derives its inspiration from this very solution.\nIf we try and follow the same approach for our original problem, then it simply won\u0026rsquo;t work. The reason a simple counter based approach works above is because all the parenthesis are of the same type. So when we encounter a closing bracket, we simply assume a corresponding opening matching bracket to be available ie. if left\u0026gt;0\nBut in our problem, if we encounter say ], we don\u0026rsquo;t really know if there is a corresponding opening [ available or not. You could say:\nWhy not maintain a separate counter for the different types of parenthesis?\nThis doesn\u0026rsquo;t work because the relative placement of the parenthesis also matters here eg: [{]\nIf we simply keep counters here, then as soon as we encounter the closing square bracket, we would know there is an unmatched opening square bracket available as well. But, the **closest unmatched opening bracket available is a curly bracket and not a square bracket and hence the counting approach breaks here.\nStacks An interesting property about a valid parenthesis expression is that a sub-expression. (Not every sub-expression) eg.\n{ [ [ ] { } ] } ( ) ( ) ^ ^ | | The entire expression is valid, but sub portions of it are also valid in themselves. This lends a sort of a recursive structure to the problem. For example consider the expression enclosed within the marked parenthesis in the diagram above. The opening bracket is at index 1 and the corresponding closing bracket is at index 6.\nWhat if whenever we encounter a matching pair of parenthesis in the expression we simply remove it from the expression?\nLet\u0026rsquo;s have a look at this idea below where we remove the smaller expressions one at a time from the overall expression and since this is a valid expression, we would be left with an empty string in the end.\nThe stack data structure can come in handy here in representing this recursive structure of the problem. We can\u0026#39;t really process this from the inside out because we don\u0026#39;t have an idea about the overall structure. But, the stack can help us process this recursively ie. from outside to inwards. Lets take a look at the algorithm for this problem using stacks as the intermediate data structure.\nAlgorithm\nInitialize a stack S. Process each bracket of the expression one at a time If we encounter an opening bracket, we simply push it onto the stack. This means we will process it later, let us simply move onto the sub-expression ahead If encounter a closing bracket, then we check the element on top of the stack. If the element at the top of the stack is an opening bracket of the same type, then we pop it off the stack and continue processing. Else, this implies an invalid expression In the end, if we are left with a stack still having elements, then this implies an invalid expression Lets take a look at the implementation for this algorithm\nclass Solution { //Hash table that takes care of the mappings private HashMap\u0026lt;Character, Character\u0026gt; mappings; //Initialize the hash map with mappings. This simply makes the code easier to read public Solution() { this.mappings = new HashMap\u0026lt;Character, Character\u0026gt;(); this.mappings.put(\u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;); this.mappings.put(\u0026#39;}\u0026#39;, \u0026#39;{\u0026#39;); this.mappings.put(\u0026#39;]\u0026#39;, \u0026#39;[\u0026#39;); } public boolean isValid(String s) { // Initialize a stack to be used in the algorithm Stack\u0026lt;Character\u0026gt; stack = new Stack\u0026lt;Character\u0026gt;(); for (int i=0; i\u0026lt; s.length(); i++) { char c = s.charAt(i); // If the current character is a closing bracket if (this.mappings.containsKey(c)) { // Get the top element of the stack. If the stack is empty, set a dummy value of \u0026#39;#\u0026#39; char topElement = stack.empty() ? \u0026#39;#\u0026#39; : stack.pop(); // If the mapping for this bracket doesn\u0026#39;t match the stack\u0026#39;s top element, return false. if (topElement != this.mappings.get(c)) { return false; } } else { //If it was an opening bracket, push to the stack stack.push(c); } } //If the stack still contains elements, then it is an invalid expression. return stack.isEmpty(); } } Complexity Analysis\nTime Complexity: O(n)\tWe simply traverse the given string one character at a time and push and pop operations on a stack take O(1) time Space Complexity: O(n)\tIn the worst case, when we push all opening brackets onto the stack, we will end up pushing all the brackets onto the stack eg ((((((((((( 21-Merge Two Sorted Lists Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.\nExample: Input: 1-\u0026gt;2-\u0026gt;4, 1-\u0026gt;3-\u0026gt;4 Output: 1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;4 Recursive class solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null) return l2; if (l2 == null) return l1; if (l1.val \u0026lt; l2.val) { l1.next = mergeTwoLists(l1.next, l2); return l1; } else { l2.next = mergeTwoLists(l1, l2.next); return l2; } }\t} Non-Recursive Similar approach and implemenation to the recursive solution above but a little more intuitive and does not require memory being held on the stack (as the recursive program runs it has to store variables on the stack so that when the program jumps back it is able to continue)\nAs with most other linked list solutions, a dummy node is utilized and two pointers are used to keep track of where we are in the the two linked lists.\nclass solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { ListNode returnNode = new ListNode(-1); ListNode headNode = returnNode; while (l1 != null \u0026amp;\u0026amp; l2 != null) { if (l1.val \u0026lt;= l2.val) { returnNode.next = l1; l1 = l1.next; } else { returnNode.next = l2; l2 = l2.next; } returnNode = returnNode.next; } if (l1 == null) { returnNode.next = l2; } else if (l2 == null) { returnNode.next = l1; } return headNode.next; } } 22-Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.\nFor example: Given n=3, a solution set is: [ \u0026#34;((()))\u0026#34;, \u0026#34;(()())\u0026#34;. \u0026#34;(())()\u0026#34;, \u0026#34;()(())\u0026#34;, \u0026#34;()()()\u0026#34; ] Brute Force Intuition\nWe can generate all 2^(2n) sequences of ( and ) characters. Then we can check if each one is valid\nAlgorithm\nTo generate all sequences, we use recursion. All sequences of length n is just ( plus all sequences of length n-1, and then ) plus all sequences of length n-1.\nTo check whether a sequence is valid, we keep track of balance, the net number of opening brackets minuts closing brackets. If it falls below zero at any time, or doesn\u0026rsquo;t end in zero, the sequence is invalid - otherwise it is valid.\nclass Solution { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; combinations = new ArrayList(); generateAll(new char[2*n], 0, combinations); return combinations; } public void generateAll(char[] current, int pos, List\u0026lt;String\u0026gt; result) { if(pos == current.length) { if (valid(current)) { result.add(new String(current)); } } else { current[pos] = \u0026#39;(\u0026#39;; generateAll(current, pos+1, result); current[pos] = \u0026#39;)\u0026#39;; generateAll(current, pos+1, result); } } public boolean valid(char[] current) { int balance = 0; for (char c : current) { if(c == \u0026#39;(\u0026#39;) { balance++; } else { balance--; } if(balance \u0026lt; 0) { return false; } } return (balance == 0); } } Complexity Analysis\nTime Complexity: O(2^2n * n)\tFor each of 2^2n sequences, we need to create an validate the sequence, which takes O(n) work in the worst case Space Complexity: O(2^2n * n) Naively, every sequence could be valid, see Closure number for a tighter asymptotic bound Backtracking Intuition and Algorithm\nInstead of adding ( or ) every time as we do in the Brute Force algorithm, let\u0026rsquo;s only add them when we know it will remain a valid sequence. We can do this by keeping track of the number of opening and closing brackets we have placed so far.\nWe can start an opening bracket if we still have one (of n) left to place. And we can start a closing bracket if it would not exceed the number of opening brackets\nclass Solution { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; ans = new ArrayList(); backtrack(ans, \u0026#34;\u0026#34;, 0, 0, n); return ans; } public void backtrack(List\u0026lt;String\u0026gt; ans, String cur, int open, int close, int max){ if (cur.length() == max*2) { ans.add(cur); return; } if(open \u0026lt; max) { backtrack(ans, cur + \u0026#34;(\u0026#34;, open + 1, close, max); } if (close \u0026lt; open) { backtrack(ans, cur + \u0026#34;)\u0026#34;, open, close +1, max); } } } Complexity Analysis\nOur complexity analysis rests on understanding how many elements there are in generateParenthesis(n). This analysis is outside the scope of this article, but it turns out this is the nth Catalan number 1/(n+1) (2n choose n), which is bounded asymptotically by 4^n/(n* sqrt(n)).\nTime Complexity: O((4^n)/sqrt(n))\tEach valid sequence has at most n steps during the backtracking procedure Space Complexity: O((4^n)/sqrt(n))\tAs described above and using O(n) space to store the sequence Another way to think about the runtime of backtracking algorithms on interviewers is O(b^d), where b is the branching factor and d is the maximum depth of recursion.\nBacktracking is characterized by a number of decisions b that can be made at each level of recursion. If you visualize the recursion tree, this is the number of children each internal node has. You can also think of b as standing for \u0026ldquo;base\u0026rdquo;, which helps us remember that b is the base of the exponential.\nIf we make b decisions at each level of recursion, and we expand the recursion tree to d levels (ie. each path has a length of d), then we get b^d nodes. Since backtracking is exhaustive and must visit each of these nodes, the runtime is O(b^d)\nClosure Number To enumerate something, generally we would like to express it as a sum of disjoint subsets that are easier to count.\nConsider the closure number of a valid parentheses sequence s: the least index \u0026gt;= 0 so that `S[0], S[1], \u0026hellip; , S[2 * index + 1] is valid. Clearly, every parentheses sequence has a unique closure number. We can try to enumerate them individually.\nAlgorithm\nFor each closure number c, we know the starting and ending brackets must be at index 0 and 2 * c + 1. Then, the 2 * c elements between must be a valid sequence, plus the rest of the elements must be a valid sequence.\nThis is just some minor improvement to the backtracking solution using the fact that for all valid solutions the first char is always \u0026lsquo;(\u0026rsquo; and the lat char is always \u0026lsquo;)\u0026rsquo;. We initialize the starting string to \u0026lsquo;(\u0026rsquo; and set the recursion bottom condition to string reaching length of 2 * n - 1 - we know that we need to append a bracket at the end. There will not be much of an improvement in the runtime however.\nclass Solution { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; ans = new ArrayList(); if (n==0) { ans.add(\u0026#34;\u0026#34;); } else { for (int c=0; c\u0026lt;n; ++c) for (String left: generateParenthesis(c)) for (String right: generateParenthesis(n-1-c)) ans.add(\u0026#34;(\u0026#34; + left + \u0026#34;)\u0026#34; + right); } return ans; } } Complexity Analysis\nTime Complexity: O((4^n)/sqrt(n)) Space Complexity: O((4^n)/sqrt(n)) 23-Merge k Sorted Lists Merge k sorted linked lists and return it as one sorted list. Analyze and descibe its complexity:\nExample: Input: [ 1 -\u0026gt; 4 -\u0026gt; 5, 1 -\u0026gt; 3 -\u0026gt; 4, 2 -\u0026gt; 6 ] Output: 1 -\u0026gt; 1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 4 -\u0026gt; 5 -\u0026gt; 6 Brute Force Intuition and Algorithm\nTraverse all the linked lists and collect the values of the nodes into an array Sort and iterate over this array to get the proper value of nodes Create a new sorted linked list and extend it with the new nodes As for sorting you can refer to the Algorithms/Data Structures CheatSheet for more about sorting algorithms.\n146-LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.\nget(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1\nput(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.\nFollow up: Could both of these operations be done in O(1) time complexity?\nExample:\nLRUCache cache = new LRUCache(2 /* capacity */); cache.put(1, 1); cache.put(2, 2); cache.get(1); // returns 1 cache.put(3, 3); // evicts key 2 cache.get(2);\t// returns -1 (not found) Index Lowest Common Ancestor Count And Say Maximum SubArray Plus One Sqrt of X Climbing Stairs Remove Duplicates from sorted list Same Tree Symmetric Tree Max Depth of Binary Tree Convert Sorted Array to Binary Search Tree Balanced Binary Tree Minimum Depth of Binary Tree Path Sum Pascal\u0026rsquo;s Triangle Valid Palindrome Pascal\u0026rsquo;s Triangle II Best Time to Buy and Sell Stock Best Time to Buy and Sell Stock II Single Number Linked List Cycle Min Stack Intersection of Two Linked Lists Two Sum II - Input array is sorted Excel Sheet Column Title Majority Element Excel Sheet Column Number Factorial Trailing Zeroes Combine Two Tables Second Highest Salary Employees Earning More Than Their Managers Duplicate Emails Customers Who Never Order Rotate Array Delete Duplicate Emails Rising Temperature X of a Kind in a Deck of Cards Reverse Integer Add Two Numbers Longest Substring Without Repeating Characters House Robber Happy Number Remove Linked List Elements Count Primes Isomorphic Strings Reverse LinkedList Contains Duplicate Contains Duplicate II Implement Stack Using Queues Invert Binary Tree Fibonacci Number kth Largest Element Power Of Two Valid Sudoku Implement Queue Using Stack Palindrome LinkedList Delete Node in a Linked List Is Anagram Binary Tree Paths Add Digits Largest Perimeter Triangle Ugly Number Missing Number Is Bad Version Move Zeroes Word Pattern Can Win Nim Power Of Three Power of Four Reverse String Implement strStr() Reverse Vowels of a String Intersection of two arrays Is Perfect Square Sum of Two Integers Guess Number Higher or Lower Ransom Note First Unique Character in a String Find the Difference Nth Digit Sum of Left Leaves Longest Palindrome Fizz Buzz Third maximum Number Add Two Strings Construct Quad Tree N-ary Tree Level Order Traversal Number of Segments in a String Binary Tree Level Order Traversal Path Sum III Find All Anagrams in a String Arranging Coins Hamming Distance String Compression Number of Boomerangs Find All Numbers Disappeared in an Array Assign Cookies Poor Pigs Find Pivot Index Squares of a Sorted Array Repeated Substring Pattern Island Perimeter Number Complement Binary Watch Minimum Moves to Equal Array Elements License Key Formatting Max Consecutive Ones Permutations Construct the Rectangle Merge Intervals Merged sorted lists Next Greater Element I String Without AAA or BBB Keyboard Row Find Mode in Binary Search Tree Base 7 Relative Ranks Perfect Number Detect Capital Longest Uncommon Subsequence I Course Schedule II Letter Combinations of a Phone Number Sudoku Solver Bulls and Cows N-Queens 1 K-diff pairs in an Array Is Subsequence Minimum Absolute Difference in BST BST Tree to Greater Tree Student Attendance Record I Reverse Words in String III Quad Tree Intersection Long Pressed Name Binary Tree Zigag Level Order Traversal Array Partition I Reshape the matrix Swap Nodes in Pairs Generate Parentheses Distribute Candies Maximum Subproduct Subarray Binary Tree Right Side View Find Minimum in Rotated Sorted Array Binary Search Tree Iterator Find Peak Element Next Permutation Search in Rotated Sorted Array Transpose Matrix Merge K sorted lists Lowest Common Ancestor public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { TreeNode current = root; while (current != null){ if (p.val \u0026lt; current.val \u0026amp;\u0026amp; q.val \u0026lt; current.val)\t// Both located in left side. current = current.left; else if (p.val \u0026gt; current.val \u0026amp;\u0026amp; q.val \u0026gt; current.val)\t// Both located in right side current = current.right; else return current;\t// Seperate branches, therefore current is lca. } return null; } Count And Say The updated version runs in 2ms and passes 96.85% submissions.\npublic String countAndSay(int n) { String result = \u0026#34;1\u0026#34;;\t// initial result StringBuilder temp;\t// to create intermediate strings efficiently. int len;\t// length of the result string. for (int i = 1; i \u0026lt; n; ++i){\t// We need to iterate n-1 times, because 1st result is 1 int startIndex = 0;\t// we will look at each index of result temp = new StringBuilder();\t// and store freq,char in the builder len = result.length(); while (startIndex \u0026lt; len){ char ch = result.charAt(startIndex++);\t// get the char at startIndex, and increment it, because we also want to look at the next character int count = 1;\t// intialize it\u0026#39;s count to 1, we just saw it. while (startIndex \u0026lt; len \u0026amp;\u0026amp; ch == result.charAt(startIndex)){ count++;\t// If next also matches, increment count and startIndex startIndex++; } temp.append(count).append(ch);\t// No more match, Add the freq and the char } result = temp.toString();\t// Update result to generate the next cound-and-say } return result; } Maximum SubArray public int maxSubArray(int[] nums) { int localMax = nums[0];\t// keeps track of max sum between the previous and current int globalMax = nums[0];\t// keeps track of global max sum. /* The idea is as follows: If the current element is greater than the previous local max, then we found an element that is a better option then before. Then, if that localmax changed and is greater than our global max, update our global max. */ for (int i = 1; i \u0026lt; nums.length; i++){ localMax = Math.max(localMax + nums[i], nums[i]); globalMax = Math.max(localMax, globalMax); } return globalMax; } Plus One public int[] plusOne(int[] digits) { digits[digits.length-1]++;\t// Add one to the last place. if (digits[digits.length-1] == 10)\t// If it became 10, { for (int i = digits.length-1; i \u0026gt; 0; i--)\t// Then add one to its previous place { if (digits[i] == 10){\t// If that also results in 10, keep propogating that 1 digits[i-1]++;\t// upstream digits[i] = 0; } } if (digits[0] == 10){\t// If the index 0 is 10, then the number is a multiple of 10. digits = new int[digits.length+1]; digits[0] = 1;\t// So increase length by 1 and set index 0 to 1. } } return digits; } Sqrt of X public int mySqrt(int x) { long x1 = 10 - (100 - x)/20;\t// Using Newton\u0026#39;s method of computing square roots. boolean done = false; while (!done) { long x2 = x1 - (x1*x1 - x)/(2*x1); if (x2 == x1) done = true; else x1 = x2; } return (int)x1-1; } Climbing Stairs public int climbStairs(int n) { if (n \u0026lt; 4)\t// I chose n \u0026lt; 4 because climbStairs(0 \u0026lt;= n \u0026lt;= 3) = n return n; int[] dp = new int[n+1]; for (int i = 0; i \u0026lt; 4; i++) dp[i] = i; //return naiveDP(n, dp); return efficientDP(n); } public int naiveDP(int n, int dp[]){ if (dp[n] != 0)\t// If already computed, return it. return dp[n]; int ways = naiveDP(n-1, dp) + naiveDP(n-2, dp);\t// Just like Fibonacci. dp[n] = ways;\t// Save it. return ways; } public int efficientDP(int n){ if (n \u0026lt; 4) return n; int[] dp = new int[n+1];\t// Initialize dp of length n+1 to store n\u0026#39;th way. for (int i = 0; i \u0026lt; 4; i++) dp[i] = i;\t// climbStairs(0 \u0026lt;= n \u0026lt;= 3) = n for (int i = 3; i \u0026lt;= n; i++)\t// climbStairs(n) = climbStairs(n-1) + climbstairs(n-2); dp[i] = dp[i-1] + dp[i-2]; // So fetch those values from the dp array. return dp[n]; } Remove Duplicates from sorted list public ListNode deleteDuplicates(ListNode head){ ListNode current = head; // while we haven\u0026#39;t reached the tail while (current != null \u0026amp;\u0026amp; current.next != null) { // if current\u0026#39;s next is the same as current, skip and update its next while (current.next != null \u0026amp;\u0026amp; current.val == current.next.val) current.next = current.next.next; current = current.next; } return head; } Same Tree public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null \u0026amp;\u0026amp; q == null)\t// Two empty trees return true; // If one of the node is null, the two trees can\u0026#39;t be equal. if ((p == null \u0026amp;\u0026amp; q != null) || (p != null \u0026amp;\u0026amp; q == null)) return false; // If the values in the two nodes are same, compare its\u0026#39;s left and right sub-tree. if (p.val == q.val) return isSameTree(p.left, q.left) \u0026amp;\u0026amp; isSameTree(p.right, q.right); return false;\t// If nothing worked out, they can\u0026#39;t be same. } Symmetric Tree public boolean isSymmetric(TreeNode root) { return isSymmetricIterative(root); } public boolean isSymmetricIterative(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; track = new LinkedList\u0026lt;\u0026gt;(); track.add(root);\t// Add the root twice so we can compare its left and right track.add(root); while (!track.isEmpty()) { TreeNode x = track.poll();\t// Remove 2 nodes TreeNode y = track.poll(); if (x == null \u0026amp;\u0026amp; y == null)\t// If they are both null, skip it. continue; if (x == null || y == null || x.val != y.val) return false;\t// If values don\u0026#39;t match or one is null track.add(x.left);\t// Otherwise add them in this order -\u0026gt; LRRL track.add(y.right);\t// because we need to compare left most with the track.add(x.right);\t// right most, then inner left with inner right. track.add(y.left); } return true;\t// Everything\u0026#39;s all right, so they must be symmetric. } public boolean isSymmetricRecursive(TreeNode root) { return helperRecursive(root, root); } private boolean helperRecursive(TreeNode x, TreeNode y) { if (x == null || y == null)\t// Base Case: Both or one is null, so true return true; return (x.val == y.val \u0026amp;\u0026amp; helperRecursive(x.left, y.right) \u0026amp;\u0026amp; helperRecursive(x.right, y.left)); // Check if values match and 1.left matches with the 2.right and 1.right matches with 2.left } Max Depth of Binary Tree /* If root is null, height is 0 else add 1 and find if the left or the right has a greater depth. */ public int maxDepth(TreeNode root) { return root == null ? 0 : 1 + Math.max(maxDepth(root.left), maxDepth(root.right)); } Convert Sorted Array to Binary Search Tree public TreeNode sortedArrayToBST(int[] nums) { return aux(nums, 0, nums.length-1); } private TreeNode aux(int[] n, int left, int right) { if (left \u0026gt; right)\t// Either empty, or return a null node return null; int mid = (left+right+1)/2;\t// Create a node with the middle value TreeNode root = new TreeNode(n[mid]); root.left = aux(n, left, mid-1);\t// Compute the left (which is the mid in left side) root.right = aux(n, mid+1, right);\t// Compute the right (which is the mid in right side) return root; } Balanced Binary Tree public boolean isBalanced(TreeNode root) { return isBalancedBottomUp(root); } public boolean isBalancedTopDown(TreeNode root) { if (root == null) return true; // if difference between root\u0026#39;s left and right is \u0026gt; 1, they\u0026#39;re not balanced if (Math.abs((getHeight(root.left) - getHeight(root.right))) \u0026gt; 1) return false; // otherwise, we need to check if the left and right subtree are also balanced. return isBalanced(root.left) \u0026amp;\u0026amp; isBalanced(root.right); } private int getHeight(TreeNode node) { // Standard height of a binary tree calculator if (node == null) return 0; return 1 + Math.max(getHeight(node.left), getHeight(node.right)); } public boolean isBalancedBottomUp(TreeNode root) { return getHeight2(root) != -1;\t// -1 means not balanced. } private int getHeight2(TreeNode node) { if (node == null) return 0; int lHeight = getHeight2(node.left);\t// Get the height of left and right tree int rHeight = getHeight2(node.right); // If at any point there was a height difference of more than 1 or previous node\u0026#39;s leftheight || rightheight returned -1, return -1 to let the next node know there was an imbalance. if ((Math.abs(lHeight-rHeight) \u0026gt; 1) || lHeight == -1 || rHeight == -1) return -1; return 1 + Math.max(lHeight, rHeight); // Else carry on with the normal procedure } Minimum Depth of Binary Tree public int minDepth(TreeNode root) { // Base case if (root == null) return 0; // Left is null, find minheight from right side if (root.left == null) return 1 + minDepth(root.right); // Right is null, find minheight from left side if (root.right == null) return 1 + minDepth(root.left); // Else, both are not null, so compute min height from the two sides. return 1 + Math.min(minDepth(root.left), minDepth(root.right)); } Path Sum public boolean hasPathSum(TreeNode root, int sum) { if (root == null) return false;\t// No sum exist sum -= root.val;\t// Sum decreases if (root.left == null \u0026amp;\u0026amp; root.right == null)\t// If we are at a leaf return sum == 0;\t// Check if the sum is 0. return hasPathSum(root.left, sum) || hasPathSum(root.right, sum); // Otherwise look if you can make sum = 0 by exploring the left or right side. } Pascal\u0026rsquo;s Triangle public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; generate(int numRows) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; pt = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; numRows; i++)\t// Need to add all n rows { List\u0026lt;Integer\u0026gt; temp = new ArrayList\u0026lt;\u0026gt;();\t// temp list to store values for (int j = 0; j \u0026lt;= i; j++) { if (j == 0 || i == j)\t// First and last values are always 1. temp.add(1); else\t// Else, get the previous row and surrounding two values and add them temp.add(pt.get(i-1).get(j-1) + pt.get(i-1).get(j)); } pt.add(temp);\t// Add it to pt. } return pt; } Valid Palindrome public boolean isPalindrome(String s) { if (s.length() \u0026gt; 0){\t// Only do this is s is not empty s = s.toLowerCase();\t// Convert it to lowercase int left = 0;\t// Initialize left and right pointers int right = s.length()-1; while (left \u0026lt; right)\t// continue while we haven\u0026#39;t hit the middle of the string { // If char at left is not a letter or a number, skip it. if (!Character.isLetter(s.charAt(left)) \u0026amp;\u0026amp; !Character.isDigit(s.charAt(left))) left++; // Same with char at right. else if (!Character.isLetter(s.charAt(right)) \u0026amp;\u0026amp; !Character.isDigit(s.charAt(right))) right--; //Char\u0026#39;s are now alphanumeric. else if (s.charAt(left) != s.charAt(right))\t// If they don\u0026#39;t match return false;\t// return false else\t// They matched, so try to match the inner string { left++; right--; } } } return true;\t// No mismatch found, return true. } Pascal\u0026rsquo;s Triangle II public List\u0026lt;Integer\u0026gt; getRow(int rowIndex) { ArrayList\u0026lt;Integer\u0026gt; row = new ArrayList\u0026lt;\u0026gt;(); row.add(1);\t// First is always 1. // Using the nth row formula to compute the coeeficients. You can google \u0026#34;nth row Pascal\u0026#34; for (int i = 0; i \u0026lt; rowIndex; i++) row.add((int)(1.0*row.get(i)*(rowIndex-i)/(i+1))); return row; } Best Time to Buy and Sell Stock /* The general idea is that if the price you are looking at right now in the array minus the minimum observed so far is greater than the maximum profit you recorded, update the max. */ public int maxProfit(int[] prices) { if (prices.length == 0)\t// Empty array return 0; int min = prices[0]; int max = 0; for (int i = 1; i \u0026lt; prices.length; i++) { if (prices[i] \u0026lt; min) min = prices[i]; else if (prices[i] - min \u0026gt; max) max = prices[i]-min; } return max; } Best Time to Buy and Sell Stock II /* The general idea is that the moment you observe a valley and consecutive peak, make the trade by buying the stock on the valley day and selling it on the peak day. */ public int maxProfit(int[] prices) { int sum = 0; for (int i = 0; i \u0026lt; prices.length-1; i++) if (prices[i+1] \u0026gt; prices[i]) sum += (prices[i+1] - prices[i]); return sum; } Single Number /* The general idea is that XOR of two same numbers returns 0 and XOR with 0 returns the same number. So if there is only one element that doesn\u0026#39;t have a pair, all the remaining will XOR with themselves at one point and give 0 but not the singleton element. */ public int singleNumber(int[] nums) { int num = nums[0]; for (int i = 1; i \u0026lt; nums.length; i++) num ^= nums[i]; return num; } Linked List Cycle // Using the slow-fast runner technique. public boolean hasCycle(ListNode head) { if (head == null) return false; ListNode first = head;\t// Slow runner ListNode second = first.next;\t// Fast Runner // while second is not at the end or it isn\u0026#39;t the tail while (second != null \u0026amp;\u0026amp; second.next != null) { if (second == first)\t// If fast made a full loop and met up with slow return true;\t// We got a cycle first = first.next;\t// Slow moves one step second = second.next.next;\t// Second advances two. } return false;\t// We don\u0026#39;t have a cycle } Min Stack class MinStack { int min; Stack\u0026lt;Integer\u0026gt; stack; public MinStack() { min = Integer.MAX_VALUE; stack = new Stack\u0026lt;\u0026gt;(); } public void push(int x) { stack.push(x);\t// Push the value if (x \u0026lt; min)\t// If that value is minimum than we have, update min min = x; stack.push(min);\t// Push the minimum on top of the stack for constant time }\t// minimum retrieval. public void pop() { stack.pop();\t// Pop the minimum. stack.pop();\t// Pop the actual element meant to be popped if (stack.isEmpty())\t// If empty, min is Max int value min = Integer.MAX_VALUE; else min = stack.peek();\t// Otherwise, min would be the top most element since we }\t// always push the minimum on top of any element we push. public int top() { return stack.elementAt(stack.size()-2);\t// Top element is actually at second last }\t// index since the last element is the minimum. public int getMin() { return min; } } Intersection of Two Linked Lists /* The general idea is that if you are done traversing any of the lists, make it\u0026#39;s pointer point to the head of the other list and start iterating. The reasoning is that the second time they iterate, they will have traversed exactly the same distance (it\u0026#39;s length plus the other list\u0026#39;s head to the intersecting node) and will meet at the intersecting node. */ public ListNode getIntersectionNode(ListNode headA, ListNode headB) { int count = 0; ListNode pA = headA; ListNode pB = headB; while (pA != pB){ pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; } return pA; } Two Sum II - Input array is sorted public int[] twoSum(int[] numbers, int target) { int left = 0, right = numbers.length-1; while (left \u0026lt; right)\t// Narrow down the window from both sides until they add up. { int sum = numbers[left] + numbers[right]; if (sum \u0026gt; target)\t// We overshot, so decrease the window from right right--; else if (sum \u0026lt; target)\t// Undershot, increase windows from left so next sum is more left++; else break;\t// Found the two numbers } return new int[] {left+1, right+1};\t// +1 because LeetCode followed 1-n indexing. } Excel Sheet Column Title public String convertToTitle(int n) { String res = \u0026#34;\u0026#34;; while (n \u0026gt; 0) { /* 1 is A and 26 is Z, so n-1 to change it to 0-25 scheme. Then, % 26 to find how much it is off on a full alphabet cycle, add 65 (ASCII for A) and convert it to char */ res = String.valueOf((char)(65+((n-1)%26))) + res; n = (n-1) / 26;\t// Subtract 1 and divide by 26 to get prepare for the next character } return res; } Majority Element Uses Moore\u0026rsquo;s Algorithm\n// This is the implementation of Moore\u0026#39;s Algorithm for O(n) complexity. public int majorityElement(int[] nums) { int major = nums[0]; int count = 1; for (int i = 0; i \u0026lt; nums.length; i++){ if (major == nums[i]) count++; else count--; if (count == 0){ major = nums[i]; count = 1; } } return major; } Excel Sheet Column Number /* Start from the end of String s, compute the ASCII for the char, +1 for 1-26 Alphabet-Scheme (hence -64 instead of -65) and multiply it to 26^{distance from the end of the string} */ public int titleToNumber(String s) { int length = s.length()-1; int total = 0; for (int i = length; i \u0026gt; -1; i--) total += (int)(s.charAt(i)-64) * Math.pow(26,length-i); return total; } Factorial Trailing Zeroes /* The general idea is that every factorial that has 5 as a multiple also has 2 to multiply to 10. So if we can count the number of times we can divide n by 5, should gives us the number of trailing zeroes. O(log(n) base 5) complexity. */ public int trailingZeroes(int n) { int res = 0; while (n \u0026gt; 4) { res += n / 5; n /= 5; } return res; } Combine Two Tables select FirstName, LastName, City, State from Person left join Address on Address.personId = person.personId; Second Highest Salary select max(salary) as SecondHighestSalary from Employee where salary not in (select max(salary) from employee); Employees Earning More Than Their Managers select emp.Name as Employee from Employee emp, Employee man where emp.managerId = man.Id and emp.salary \u0026gt; man.salary; Duplicate Emails select email from person group by (email) having count(*) \u0026gt; 1; Customers Who Never Order select name as Customers from Customers where customers.id not in (select customerId from orders); Rotate Array public void rotate(int[] nums, int k) { k %= nums.length;\t// k == nums.length ? Then it\u0026#39;s a full rotation and no change if (k == 0) return; reverse(nums, 0 , nums.length-1);\t// First reverse the full array reverse(nums, 0, k-1);\t// Then reverse element from index 0 to k-1 reverse(nums, k, nums.length-1);\t// Then reverse all elements from k to end of Array } // Reverse function that reverses the array from specified indices. public void reverse(int[] nums, int start, int end) { while (start \u0026lt; end){ int temp = nums[start]; nums[start] = nums[end]; nums[end] = temp; start++; end--; } } Delete Duplicate Emails delete from Person where Id not in (select min_id from (select min(Id) as min_id from Person group by Email) as a) Rising Temperature select w2.id from weather w1, weather w2 where Datediff(w2.recorddate, w1.recorddate) = 1 and w2.temperature \u0026gt; w1.temperature; X of a Kind in a Deck of Cards public boolean hasGroupsSizeX(int[] deck) { HashMap\u0026lt;Integer, Integer\u0026gt; freq = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; deck.length; i++)\t// Record the frequencies freq.put(deck[i],freq.getOrDefault(deck[i],0)+1); /* deck = [1,1,2,2,2,2,3,3,3,3,3,3] number 1 has len of 2, number 2 has len of 4, number 3 has len of 6, they share a Greatest common divisor of 2, which means diving them into group of size X = 2, will be valid. Thus we just have to ensure each length (of a number) shares a Greatest Common Divisor that\u0026#39;s \u0026gt;= 2. */ int hcf = 0; for (int i: freq.keySet()) hcf = gcd(hcf, freq.get(i)); return hcf \u0026gt; 1; } private static int gcd(int x, int y) { int temp = 0; while (y != 0){ temp = y; y = x % y; x = temp; } return x; } Reverse Integer public int reverse(int x) { int sign = x \u0026lt; 0 ? -1 : 1; x = x * sign;\t// Make x positive long n = 0; while (x \u0026gt; 0){ n = n * 10 + x % 10;\t// Start adding from the end. x /= 10; } return (int)n == n ? (int)n*sign : 0;\t// Try converting to int from long, if no change, }\t// Return n * sign, else 0 cause overflow. Add Two Numbers public ListNode addTwoNumbers(ListNode l1, ListNode l2) { int carry = 0;\t// To record the carry int sum = 0;\t// To record the total of two vals ListNode dummy = new ListNode(0);\t// Dummy\u0026#39;s next is the actual head ListNode curr = dummy; do{ if (l1 == null)\t// If one of the node is null, we set it to a l1 = new ListNode(0);\t// dummy value of 0 so we can adjust for if (l2 == null)\t// different length of the two lists. l2 = new ListNode(0); sum = l1.val + l2.val + carry;\t// Add the two vals and the carry. carry = sum \u0026lt; 10 ? 0 : 1;\t// Record the carry for the next iteration curr.next = new ListNode(sum % 10);\t// next node\u0026#39;s value is sum % 10. curr = curr.next;\t// advance current, l1 and l2. l1 = l1.next; l2 = l2.next; } while(l1 != null || l2 != null); if (carry == 1)\t// In the end, if carry is 1, it was from curr.next = new ListNode(carry);\t// from adding last terms, so make next node 1 return dummy.next;\t// Return the actual head. } Longest Substring Without Repeating Characters public int lengthOfLongestSubstring(String s) { if (s == null || s.length() == 0) return 0; int[] hash = new int[128];\t// To store the occurence of characters int maxLength = 0; for (int i = 0, j = 0; j \u0026lt; s.length(); j++){ i = Math.max(hash[s.charAt(j)], i);\t// Check the most recent index of character. maxLength = Math.max(maxLength, j-i+1);\t// That minus current pointer gives length hash[s.charAt(j)] = j+1;\t// Record the index of the next character. } return maxLength; } House Robber /* The basic idea is that if you are robbing house i, the maximum loot may come from by robbing the i-2th house or by robbing the i-3th house. Therefore rob both and then find the path that gave the maximum profit. Example: loot = [1,9,3,8,4,3,6,4,3,5,7,6] Profit DP = [1,9,4,17,13,20,23,24,26,29,33,35] Here, dp[2] = loot[2] + loot[1] dp[4] = loot[4] + max(dp[2], dp[1]) dp[5] = loot[5] + max(dp[3], dp[2]) and so on. In the end, just compare the last two elements to check which path gave us the maximum profit. Some people might not prefer modifying the original nums array. In that case, you can initialize another dp array of same length, initialize the first two elements as dp[0] = nums[0] and dp[1] = nums[1] and dp[3] = nums[0] + nums[2] and then performing the same loop. In that case, you would be using O(n) space. */ public int rob(int[] nums) { if (nums.length == 0 || nums == null)\t// 3 Base Case return 0; if (nums.length == 1) return nums[0]; else if (nums.length == 2) return Math.max(nums[0], nums[1]); else{ nums[2] = nums[0] + nums[2];\t// House 3 profit is rob House 1 and 3. for (int i = 3; i \u0026lt; nums.length; i++) nums[i] = nums[i] + Math.max(nums[i-2], nums[i-3]); return Math.max(nums[nums.length-1], nums[nums.length-2]); } } Happy Number public boolean isHappy(int n) { return isHappyConstantSpace(n);\t// Much faster than set method //return isHappySet(n); } private boolean isHappyConstantSpace(int n){ int numSeenLessThan10 = 0;\t// If I see 10 single digits, then it means that I am while (n != 1){\t// now starting to see repititions. if (n \u0026lt; 10)\t// Each time I see a num \u0026lt; 10, increment the counter numSeenLessThan10++; if (numSeenLessThan10 \u0026gt; 9) return false; n = getSquare(n);\t// Get the total of square of its digits. } return true; } /* The general idea is that the moment you see a repition, it can\u0026#39;t be a happy number, so keep track of digit square obtained so far. If they hit 1, well and good, otherwise there will be some repition, so return false. */ private boolean isHappySet(int n){ HashSet\u0026lt;Integer\u0026gt; seen = new HashSet\u0026lt;\u0026gt;();\t// Keep track of numbers while (true){ n = getSquare(n);\t// Get the sum of digits square if (n == 1)\t// If it\u0026#39;s 1, it\u0026#39;s a happy number return true; else if (seen.contains(n))\t// If it\u0026#39;s a repition of something return false;\t// seen before, it\u0026#39;s not a happy no. else seen.add(n);\t// If not seen, add it. } } private int getSquare(int n){\t// Add the squares of the digits. int total = 0; while (n != 0){ int digit = n % 10; total += digit * digit; n /= 10; } return total; } } Remove Linked List Elements public ListNode removeElements(ListNode head, int val) { while (head != null \u0026amp;\u0026amp; head.val == val)\t// While head contains the val, skip head = head.next;\t// the head ListNode current = head; while (current != null \u0026amp;\u0026amp; current.next != null){\t// While we have something to iterate if (current.next.val == val)\t// If current\u0026#39;s val match, skip the current.next = current.next.next;\t// next node. else current = current.next;\t// Else advance to the next node. } return head; } Count Primes public int countPrimes(int n) { if (n \u0026lt; 2) return 0;\t// No prime numbers for numbers \u0026lt; 2 boolean[] store = new boolean[n];\t// Using Sieve of Eratosthenes for (int i = 2; i*i \u0026lt;= n; i++)\t// Start from i = 2 to sqrt(n) if (!store[i])\t// If store[i] = false, then mark all its for (int j = i*i; j \u0026lt; n; j += i)// multiples in the store as true store[j] = true;\t// True = not a prime, false = prime int count = 0; for (int i = 2; i \u0026lt; n; i++)\t// Loop through the array, count if (!store[i]) count++; return count; } Isomorphic Strings public boolean isIsomorphic(String s, String t) { if (s.length() != t.length())\t// Can\u0026#39;t be isomorphic is string lengths do not return false;\t// match char[] hashS = new char[128];\t// To store String s\u0026#39; match char[] hashT = new char[128];\t// To store String t\u0026#39;s match for (int i = 0; i \u0026lt; s.length(); i++){ char charS = s.charAt(i), charT = t.charAt(i); if (hashS[charS] != hashT[charT])\t// If the values at respective characters index return false;\t// do not match, return false hashS[charS] = (char)(i+1);\t// Otherwise, mark those index with the same hashT[charT] = (char)(i+1);\t// arbitrary value. I chose a simple (i+1) to }\t// to mark both the hash with the same value. return true;\t// Everything worked out, return true; } Reverse LinkedList // Recursive public ListNode reverseList(ListNode head) {\t// Very tricky. Refer to the demo below if (head == null || head.next == null) return head; ListNode node = reverseList(head.next); head.next.next = head; head.next = null; return node; } //Iterative public ListNode reverseList(ListNode head) { if (head == null || head.next == null) return head;\t// No point in reversing empty or 1-sized list ListNode curr = head, prev = null; ListNode nextNode; while (curr != null){\t// While we haven\u0026#39;t reached the tail nextNode = curr.next;\t// Store the next node curr.next = prev;\t// Current\u0026#39;s next becomes it\u0026#39;s previous prev = curr;\t// Advance previous to current. curr = nextNode;\t// Make current the actual next node } return prev;\t// Current is at null, so it\u0026#39;s previous is the }\t// new head. ![reverse Linked list](/Users/devkapupara/Desktop/Notes/dependencies/reverse Linked list.jpg)\nContains Duplicate public boolean containsDuplicate(int[] nums) { if (nums.length \u0026lt; 2) return false;\t// There can\u0026#39;t be any duplicates. HashSet\u0026lt;Integer\u0026gt; store = new HashSet\u0026lt;\u0026gt;();\t// Store unique values. for (int n: nums){ if (!store.add(n))\t// Add func returns true if n was\u0026#39;nt present, return true;\t// false if duplicate. Therefore if it was a }\t// duplicate, return true. return false;\t// No duplicates, so return false } Contains Duplicate II public boolean containsNearbyDuplicate(int[] nums, int k) { if (nums.length \u0026lt; 2) return false; int left = 0, right = 0; HashSet\u0026lt;Integer\u0026gt; store = new HashSet\u0026lt;\u0026gt;();\t// Use a rotating window of size k while (right \u0026lt; nums.length){\t// While we haven\u0026#39;t processed everything if (store.contains(nums[right]))\t// If our current window contains duplicate return true; store.add(nums[right]);\t// No duplicates in the window right++;\t// Increase right to visit the new element if (right - left \u0026gt; k){\t// If window becomes \u0026gt; k store.remove(nums[left]);\t// remove the number on the left side of left++;\t// the window and increase the left counter }\t// for new window from the next index } return false;\t// No duplicates found in any window. } Implement Stack Using Queues class MyStack { Deque\u0026lt;Integer\u0026gt; stack; /** Initialize your data structure here. */ public MyStack() { stack = new ArrayDeque\u0026lt;\u0026gt;(); } /** Push element x onto stack. */ public void push(int x) { stack.add(x); } /** Removes the element on top of the stack and returns that element. */ public int pop() { return stack.removeLast(); } /** Get the top element. */ public int top() { return stack.peekLast(); } /** Returns whether the stack is empty. */ public boolean empty() { return stack.isEmpty(); } } Invert Binary Tree public TreeNode invertTree(TreeNode root) { if (root == null) return null; TreeNode temp = root.left;\t// Swap the left and right nodes root.left = root.right; root.right = temp; invertTree(root.left);\t// Then swap the subsequent trees of those nodes. invertTree(root.right); return root;\t// Return the original root. } Fibonacci Number // Iterative public int fib(int N) { if (N \u0026lt; 2)\t// fib(0) = 0; fib(1) = 1 return N; int f0 = 0, f1 = 1, fn = 0; for (int i = 2; i \u0026lt;= N; i++){ fn = f0 + f1;\t// fib(n) = fib(n-1) + fib(n-2) f0 = f1;\t// f0 becomes f1 f1 = fn;\t// f1 becomes fn } return f1; } // Dynamic Programming private int fibDP(int N){ if (N \u0026lt; 2) return N; int[] dp = new int[N+1];\t// To store intermediate result dp[1] = 1;\t// fib(0) = 0; fib(1) = 1 for (int i = 2; i \u0026lt;= N; i++) dp[i] = dp[i-1]+dp[i-2];\t// fib(i) = fib(i-1) + fib(i-2) return dp[N];\t// Return the last number in the array } kth Largest Element The minheap algorithm has $O(n lg n) $ complexity and $O(1)$ space. The idea here is that we use a minheap to keep only the k greatest elements. If size becomes more than k, we remove the smallest element at the top of the heap. Thereby, at the end, our kth largest element will be at the top. QuickSelect Algorithm performs in $O(n)$ best case, $O(n^2)$ worst case when the pivot chosen is always the largest, so we use a random pivot. // MinHeap Algorithm public int kthLargest(int[] nums, int k){ PriorityQueue\u0026lt;Integer\u0026gt; q = new PriorityQueue\u0026lt;\u0026gt;((n1,n2) -\u0026gt; n1 - n2);\t// Initialize minheap for (int n: nums){ q.add(n);\t// Add number one by one if (q.size() \u0026gt; k)\t// If size is greater than k q.poll();\t// Remove the topmost element } return q.poll();\t// The topmost element is our answer } // QuickSelect Algorithm - Hoare\u0026#39;s Partition Scheme private int[] arr; public int kthLargest(int[] nums, int k){ arr = nums; return quickselect(0, nums.length-1, nums.length-k);// kth largest is (n-k)th largest } private int quickselect(int left, int right, int k){ if (left == right)\t// Array contains only 1 element, that\u0026#39;s the answer return arr[left]; Random rand = new Random();\t// Choose a random pivot between left and right int pivotIndex = left + rand.nextInt(right-left);\t// but not left pivotIndex = partition(left, right, pivotIndex);\t// Partition, and find it\u0026#39;s correct index if (k == pivotIndex)\t// That index is equal to kth statistic return arr[pivotIndex]; else if (k \u0026lt; pivotIndex)\t// If it\u0026#39;s less than the index, our ans lies in the return quickselect(left, pivotIndex-1, k);\t// left side else return quickselect(pivotIndex+1, right, k);\t// Otherwise, it\u0026#39;s on the right side. } private int partition(int left, int right, int pivotIndex){ int pivot = arr[pivotIndex];\t// Partition element swap(pivotIndex, right);\t// Move that element to the end int wall = left - 1;\t// wall is initially before everything for (int i = left; i \u0026lt; right; i++){ if (arr[i] \u0026lt; pivot)\t// If the current element is \u0026lt; than the pivot, then swap(i, ++wall);\t// we need to swap it with the element next to wall. } swap(right, ++wall);\t// Lastly, swap the element at wall and the end. return wall; } private void swap(int i, int j){ int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } Power Of Two public boolean isPowerOfTwo(int n) { if (n \u0026lt; 1) return false;\t// n \u0026lt; 0 cannot be powers of 2 while (n \u0026gt; 2){ if (n % 2 != 0)\t// If n is odd, it can\u0026#39;t be a power of 2. return false; n = n / 2;\t// It is a multiple of 2, so divide it by 2. } return true;\t// n came out to be 1 which is a power of 2, so return true. } Valid Sudoku private char[][] board; public boolean isValidSudoku(char[][] board){ this.board = board; return rowCheck() \u0026amp;\u0026amp; colCheck() \u0026amp;\u0026amp; boxCheck();\t// Check row first, then column and at }\t// last, boxes because they are time // consuming. private boolean onePassCheck(){ HashSet\u0026lt;Integer\u0026gt;[] rows = new HashSet[9];\t// 1 HashSet for each row HashSet\u0026lt;Integer\u0026gt;[] columns = new HashSet[9];\t// 1 HashSet for each column HashSet\u0026lt;Integer\u0026gt;[] boxes = new HashSet[9];\t// 1 HashSet for each box. for (int i = 0; i \u0026lt; 9; i++){ rows[i] = new HashSet\u0026lt;\u0026gt;(); columns[i] = new HashSet\u0026lt;\u0026gt;(); boxes[i] = new HashSet\u0026lt;\u0026gt;(); } for (int i = 0; i \u0026lt; 9; i++){ for (int j = 0; j \u0026lt; 9; j++){ int n = (int)(board[i][j]); if (n != -2){\t// -2 = \u0026#39;.\u0026#39;\tint boxIndex = (i/3) * 3 + j/3;\t// Calculate which box we are in. if (!rows[i].add(n) || !columns[j].add(n) || !boxes[boxIndex].add(n)) return false;\t// If the row set or the column set or the }\t// box set contains that val, return false. } } return true; } private boolean rowCheck(){\t// Horizontal check boolean[] arr; for (char[] row: board){ arr = new boolean[9]; for (char c: row){ int val = c-\u0026#39;0\u0026#39;; if (val != -2){\t// val = -2 means \u0026#39;.\u0026#39; in the board if (arr[val-1])\t// If val already seen, invalid sudoku return false; arr[val-1] = true;\t// else, Mark that index as seen. } } } return true; } private boolean colCheck(){\t// Vertical Check. boolean[] arr; for (int col = 0; col \u0026lt; board.length; col++){ arr = new boolean[9]; for (int row = 0; row \u0026lt; board[0].length; row++){ int val = board[row][col]-\u0026#39;0\u0026#39;; if (val != -2){ if (arr[val-1]) return false; arr[val-1] = true; } } } return true; } private boolean boxCheck(){\t// For the 9 sub boxes, let the single for (int i = 0; i \u0026lt; 9; i+=3){\t// box checker check it\u0026#39;s validity. for (int j = 0; j \u0026lt; 9; j+=3)\t// If any of the subbox was invalid, if (!singleBoxCheck(i,j))\t// we abort and return false. return false; } return true; } private boolean singleBoxCheck(int topRightRow, int topRightCol){ boolean[] arr = new boolean[9]; for (int i = 0; i \u0026lt; 3; i++){\t// Each sub box has 3 rows and 3 columns for (int j = 0; j \u0026lt; 3; j++){ int val = board[topRightRow+i][topRightCol+j]-\u0026#39;0\u0026#39;;\t// This gives us the value at if (val != -2){\t// each cell in the sub box and we fill the if (arr[val-1])\t// arr with all values that are seen. return false;\t// If seen twice, return false; arr[val-1] = true; } } } return true; } } Implement Queue Using Stack /* Since we reverse stack1 into stack2, stack2 is basically our queue, so if stack2 isn\u0026#39;t empty, then the topmost element is what we need when we pop or peek. If it is empty, then again fill it with whatever\u0026#39;s there is stack1, and it again becomes the correct queue. */ Stack\u0026lt;Integer\u0026gt; stack1; Stack\u0026lt;Integer\u0026gt; stack2; public MyQueue() { stack1 = new Stack\u0026lt;\u0026gt;(); stack2 = new Stack\u0026lt;\u0026gt;(); } public void push(int x) { stack1.push(x);\t// Push onto stack1 } public int pop() { peek();\t// First call the peek function, to make sure stack 2 isn\u0026#39;t return stack2.pop();\t// empty. Then, the topmost element of stack2 is what we want } /** Get the front element. */ public int peek() { if (stack2.isEmpty()){\twhile (!stack1.isEmpty()) stack2.push(stack1.pop()); } return stack2.peek();\t// stack2 is basically the queue, so return whatever\u0026#39;s on the top } /** Returns whether the queue is empty. */ public boolean empty() { return stack1.isEmpty() \u0026amp;\u0026amp; stack2.isEmpty(); } Palindrome LinkedList public boolean isPalindrome(ListNode head) { if (head == null || head.next == null)\t// Size 0 or 1 list, must be unique. return true; if (head.next.next == null)\t// Size 2 list, compare the head and tail return head.val == head.next.val;\t// values ListNode middleNode = head;\t// Standard Rabbit-Tortoise pointers. ListNode fastPointer = head;\t// Fast pointer jumps twice so by the time // it reaches the end of the list, middlenode ListNode curr = head;\t// is at the middle of the linkedlist. ListNode prev = null; ListNode nextNode;\t// These three nodes are for reversing the // first half of the list while (fastPointer != null \u0026amp;\u0026amp; fastPointer.next != null){ middleNode = middleNode.next;\t// Advance middle once, fastpointer twice fastPointer = fastPointer.next.next; nextNode = curr.next;\t// Reverse the curr node, but first store the curr.next = prev;\t// next newNode. By doing this, we would have prev = curr;\t// reversed exactly half of the list because curr = nextNode;\t// fastpointer advacnes at double the speed. } if (fastPointer != null)\t// If faspointer isn\u0026#39;t null, then we have an middleNode = middleNode.next;\t// odd length list, so advance middle once, // List looks like 1-\u0026gt;2-\u0026gt;3-\u0026gt;2-\u0026gt;1 instead of while (middleNode != null){\t// 1-\u0026gt;2-\u0026gt;3-\u0026gt;3-\u0026gt;2-\u0026gt;1 if (middleNode.val != prev.val)\t// While middle isn\u0026#39;t null, check middlenode return false;\t// val and prev val. Prev is basically the middleNode = middleNode.next;\t// the point where the list reverses. prev = prev.next;\t// Advance middle and next. } return true;\t// Values matched, so return true. }\t// Reversed list looks like this: // 1\u0026lt;-2\u0026lt;-3\u0026lt;-prev middle-\u0026gt;3-\u0026gt;2-\u0026gt;1 in even len // 1\u0026lt;-2\u0026lt;-prev middle-\u0026gt;2-\u0026gt;1 in odd lengths. Delete Node in a Linked List public void deleteNode(ListNode node) { node.val = node.next.val;\t// Node\u0026#39;s value becomes its next node\u0026#39;s value node.next = node.next.next; // Node\u0026#39;s next is it\u0026#39;s next\u0026#39;s next. } Is Anagram public boolean isAnagram(String s, String t) { if (s.length() != t.length())\t// Can\u0026#39;t be anagram if size aren\u0026#39;t the same return false; int[] store = new int[26];\t// Acts like a hashmap for (int i = 0; i \u0026lt; s.length(); i++)\t// Increment the count by 1 in the store for the store[s.charAt(i)-\u0026#39;a\u0026#39;]++;\t// index = position of char in the alphabet for (int i = 0; i \u0026lt; t.length(); i++){\t// Loop throught the second string, decrement if (--store[t.charAt(i)-\u0026#39;a\u0026#39;] \u0026lt; 0)\t// count of each character in store by 1, but if return false;\t// it goes below 0, then it means that character }\t// occurred more than it did in s. So false. return true;\t// Everything matched, so return true. } Binary Tree Paths List\u0026lt;String\u0026gt; paths = new ArrayList\u0026lt;\u0026gt;(); public List\u0026lt;String\u0026gt; binaryTreePaths(TreeNode root) { if (root == null)\t// No paths return paths; String rootval = root.val + \u0026#34;\u0026#34;;\t// Converting int to string. traverse(root, rootval); return paths; } private void traverse(TreeNode root, String s){ if (root.left == null \u0026amp;\u0026amp; root.right == null)\t// It\u0026#39;s a leaf, and you found a path paths.add(s);\t// so add it to the list if (root.left != null)\t// Left side is traversable, so traverse(root.left, s + \u0026#34;-\u0026gt;\u0026#34; + root.left.val);\t// visit it and record its value. if (root.right != null)\t// Same as above, but for right side. traverse(root.right, s + \u0026#34;-\u0026gt;\u0026#34; + root.right.val); } Add Digits private int constantTime(int n){ if (n \u0026lt; 10) return n;\t// Already a single digit int result = n % 9; if (result == 0)\t// If perfectly divisible by 9, then sum will be 9. return 9; return result;\t// Otherwise, the result is going to be n % 9. } private int iterative(int num){ while (num \u0026gt; 9){\t// While number isn\u0026#39;t between 2-9 num = sumOfDigits(num);\t// make num = sum of it\u0026#39;s digits. } return num; } private int sumOfDigits(int n){\t// Standard method to add the digits of a number. int sum = 0; while (n != 0){ sum += n % 10;\t// Extract the last digit, add it to sum. n /= 10;\t// Divide the num by 10. } return sum; } Largest Perimeter Triangle public int largestPerimeter(int[] A) { Arrays.sort(A);\t// Sort so the largest sides are at the end. for (int i = A.length-3; i \u0026gt;= 0; --i)\t// Triangle inequality Theorem : a + b \u0026gt; c if (A[i] + A[i+1] \u0026gt; A[i+2])\t// If sum of last two is greater than the last return A[i] + A[i+1] + A[i+2];\t// we found out max perimeter, otherwise return 0;\t// decrease i by i, then check the next three }\t// triplets // In the end if nothing works out, we return 0. Ugly Number public boolean isUgly(int num) { if (num \u0026lt; 1) return false;\t// Negative numbers are automatically non ugly while (num % 2 == 0)\t// Keep dividing number by 2 till it is divisible num /= 2; while (num % 3 == 0)\t// Keep dividing by 3 num /= 3; while (num % 5 == 0)\t// and 5 num /= 5; return num == 1;\t// If num isn\u0026#39;t 1, that means that there are other prime factors }\t// except 2,3 and 5. Missing Number public int missingNumber(int[] nums) {\t// Since it\u0026#39;s given that the array contains int nsum = (nums.length*(nums.length+1))/2;\t// all numbers from 0-n, we use the formula int arraySum = nums[0];\t// to compute sum of n numbers. for (int i = 1; i \u0026lt; nums.length; i++)\t// Then we loop through the array to compute arraySum += nums[i];\t// the sum of the array. return nsum - arraySum;\t// Subtract the array sum from the required }\t// sum, and that gives us the missing number Is Bad Version public int firstBadVersion(int n) {\t// Basic Binary Search Algorithm int low = 1, high = n; int mid; while (low \u0026lt; high){ mid = low + (high - low)/2;\t// high - low to prefent integer overflow. if (isBadVersion(mid))\t// if the model at mid was bad version, then we high = mid;\t// could possibly have a bad version before it else low = mid+1;\t// If it wasn\u0026#39;t, then our first bad version lies }\t// beyond the middle element. return low; } Move Zeroes /* The general idea is that we know the end of the array is going to contain zeroes. So first, iterate over the array, if you find any non-zero value, copy it down to the front of the array. Then we you are done, length of the array minus the last index where you copied the non-zero element is the number of zeroes you need to fill in. So iterate from that last non-zero index to the end of the array and fill in zeroes. */ public void moveZeroes(int[] nums) { int lastNonZeroIndex = 0; for (int i = 0; i \u0026lt; nums.length; i++) if (nums[i] != 0) nums[lastNonZeroIndex++] = nums[i]; for (int i = lastNonZeroIndex; i \u0026lt; nums.length; i++) nums[i] = 0; } /* This solution is an extension of the above, but a better one because we only swap elements when needed and do not do any unnecessary writes. Start from the beginning of the array, maintain the last position of non-zero value you saw, and the current element. If you see a non-zero value, swap the current value with the index just after the last non-zero index you have, and then increment the non-zero index by 1 because you just found a new non-zero value. This helps us prepare for the next non-zero value we find and copy it at this index+1. By doing so, we are basically partitioning the array into non-zeroes and zero values. */ public void moveZeroes(int[] nums) { for (int lastNonZeroIndex = 0, i = 0; i \u0026lt; nums.length; i++){ if (nums[i] != 0) swap(nums, i , lastNonZeroIndex++); } } private void swap(int[] a, int i, int j){ int temp = a[i]; a[i] = a[j]; a[j] = temp; } Word Pattern public boolean wordPattern(String pattern, String str) { String[] words = str.split(\u0026#34; \u0026#34;);\t// Split str into words if (pattern.length() != words.length)\t// If length of pattern and words mismatch return false;\t// then pattern do not match HashMap\u0026lt;Character, String\u0026gt; patternStore = new HashMap\u0026lt;\u0026gt;();\t// Map pattern char to word HashMap\u0026lt;String, Character\u0026gt; wordMap = new HashMap\u0026lt;\u0026gt;();\t// Map word to pattern char for (int i = 0; i \u0026lt; words.length; i++){ char c = pattern.charAt(i);\t// Get the char patternStore.putIfAbsent(c, words[i]);\t// Put it in patternStore if absent if (!patternStore.get(c).equals(words[i]))\t// If it was already there and it doesn\u0026#39;t return false;\t// map to words[i], we have a violation wordMap.putIfAbsent(words[i], c);\t// Now check the other way around. If if (wordMap.get(words[i]) != c)\t// words is absent in the map, map it to return false;\t// the char. If present, then fetch it\u0026#39;s }\t// mapping and check if both match to c. return true;\t// No violation, so return true } Can Win Nim public boolean canWinNim(int n) { return n % 4 != 0;\t// You can always win the game if n is not divisible by 4. } Power Of Three public boolean isPowerOfThree(int n) { if (n \u0026lt; 1)\t// If negative, it can\u0026#39;t be a power of 3. return false; while (n % 3 == 0)\t// While n is divisible by 3, keep dividing it. n /= 3; return n == 1;\t// In the end, if it was a power of 3, then n should be 1. } Power of Four /* You can also use the iterative method that I have used in Power of Two and Power of Three problems. I just wanted to try a different approach here. This is a constant time function. */ public boolean isPowerOfFour(int num) { double pow = Math.log(num)/Math.log(4);\t// Calculate x in 4^x = num using logs. return pow == (int)pow;\t// Making sure that x is an integer and not a }\t// fractional exponent. Reverse String /* 1 Liner solution. Basically, create a StringBuilder of the string, the builder already has a reverse method, so reverse it and then return it\u0026#39;s toString. */ public String reverseString(String s) { return new StringBuilder(s).reverse().toString(); } /* Golfing aside, here is how one is expected to solve it in an interview. */ public String reverseString(String s) { char[] array = s.toCharArray();\t// Create a char array of the string int len = array.length;\t// length of the array for (int i = 0; i \u0026lt; len/2; i++){\t// We only need to iterate over half the array. char temp = array[i];\t// Swap the 0th index element with (len-1)th, array[i] = array[len-i-1];\t// 1st index element with (len-2)th, until you get array[len-i-1] = temp;\t// to the middle element. } return new String(array);\t// Return a new string with the reversed array. } Implement strStr() /* The basic idea here is that you only need to iterate haystack length - needle length, and then check the substring of size = needle length in haystack from each index. If you are successfully able to match each character of the needle in the corresponding substring in haystack, return the index you start from. */ public int strStr(String haystack, String needle) { if (needle.length() \u0026gt; haystack.length())\t// Needle length can\u0026#39;t be \u0026gt; than haystack return -1; int hl = haystack.length(); int nl = needle.length(); if (nl == 0)\t// Empty strings are always a match starting return 0;\t// from 0. for (int i = 0; i \u0026lt;= hl-nl; i++){\t// Iterate haystack length - needle length. for (int j = 0; j \u0026lt; nl \u0026amp;\u0026amp; haystack.charAt(i+j) == needle.charAt(j); ++j)} if (j == nl-1)\t// We are checking how far from i can we return i;\t// match. If i matched with j, increment j }\t// and then match the character i+1 to j. }\t// If that matches, increment j and match i+2 return -1;\t// j == n-1 checked wether or not if we were }\t// able to match the full needle string, if // yes, then i is our index // in the end, nothing matched, so return -1 Reverse Vowels of a String public String reverseVowels(String s) { if (s.length() \u0026lt; 2) return s;\t// No need to reverse a string of length 0 or 1 char[] str = s.toCharArray();\t// Get the char array int left = 0; int right = str.length-1; while (left \u0026lt; right){ while (left \u0026lt; right \u0026amp;\u0026amp; !isVowel(str[left]))\t// While left is pointing to a left++;\t// consonant, increment it/ while (left \u0026lt; right \u0026amp;\u0026amp; !isVowel(str[right]))\t// While right is pointing to a right--;\t// consonant, decrement it. char temp = str[left];\t// Left and right are now pointing str[left] = str[right];\t// to vowels, so swap it. str[right] = temp;\t// And then increment left and left++;\t// decrement right to process the right--;\t// inner string } return new String(str);\t// Return a string from the reveresed array. } private boolean isVowel(char c){\t// Function to check if a character is a vowel. switch (c) { case \u0026#39;a\u0026#39;: case \u0026#39;e\u0026#39;: case \u0026#39;i\u0026#39;: case \u0026#39;o\u0026#39;: case \u0026#39;u\u0026#39;: case \u0026#39;A\u0026#39;: case \u0026#39;E\u0026#39;: case \u0026#39;I\u0026#39;: case \u0026#39;O\u0026#39;: case \u0026#39;U\u0026#39;: return true; default: return false; } } Intersection of two arrays public int[] intersection(int[] nums1, int[] nums2) { Set\u0026lt;Integer\u0026gt; set1 = new HashSet\u0026lt;Integer\u0026gt;();\t// Record all unique values in set 1 for (int i: nums1) set1.add(i); Set\u0026lt;Integer\u0026gt; intersect = new HashSet\u0026lt;\u0026gt;();\t// We will use it to record intersection for (int i: nums2)\t// For each value in nums2 array if (set1.contains(i))\t// If set1 contains it, we found an intersect.add(i);\t// intersecting element, so add it. int[] res = new int[intersect.size()];\t// We will now convert the set to an int i = 0;\t// array and then return the array. for (int n: intersect) res[i++] = n; return res; } Is Perfect Square /** The basic idea here is to close in on the square root using binary search algorithm. I handle 4 seperately because it\u0026#39;s root is the only one where 4/3 \u0026lt; it\u0026#39;s square root. All other numbers square root is greater than its value/3. So we create a lowerBound of 1 and an upperBound of num/3. Then if the middle value\u0026#39;s square overshoots, we make upperBound = mid-1, otherwise increment lowerBound to mid+1. This way, we close on the square root from both sides, and if the middle values is the square root, it\u0026#39;s square will yield num. */ public boolean isPerfectSquare(int num) { if (num \u0026lt; 2 || num == 4) return true; long lowerBound = 1; long upperBound = num/3; long mid; long square; while (lowerBound \u0026lt;= upperBound){ mid = lowerBound + (upperBound-lowerBound)/2; square = mid*mid; if (square == num) return true; if (square \u0026gt; num) upperBound = mid-1; else lowerBound = mid+1; } return false; } Sum of Two Integers I cannot explain it better than this post.\npublic int getSum(int a, int b) { if (a == 0) return b; if (b == 0) return a; int sum = a ^ b; int carry = a \u0026amp; b; if (carry == 0) return sum; return getSum(sum, carry \u0026lt;\u0026lt; 1); } Guess Number Higher or Lower public int guessNumber(int n) {\t// Standard binary search algorithm int low = 1, high = n, result = -2;\t// Arbitrary result, but not 0 int mid = 0; while (result != 0){ mid = low + (high-low)/2;\t// Check the mid. result = guess(mid);\t// Check if our guess is correct if (result == -1)\t// If result == -1, then we overshot high = mid-1;\t// So we can discard all values \u0026gt; mid else if (result == 1)\t// If result == 1, we undershot low = mid+1;\t// Need to discard all the values \u0026lt; mid } return mid;\t// Result == 0, so return the mid. } Ransom Note public boolean canConstruct(String ransomNote, String magazine) { int[] store = new int[26]; for (char c: magazine.toCharArray())\t// First, fill the store with available store[c-\u0026#39;a\u0026#39;]++;\t// characters from the magazine for (char c: ransomNote.toCharArray())\t// Then, scan through the note, decrement if (--store[c-\u0026#39;a\u0026#39;] \u0026lt; 0)\t// each char\u0026#39;s index by 1 because we used return false;\t// it. If it\u0026#39;s frequency drops below 0, return true;\t// then it means that we need more chars }\t// than available. In the end, return // true if everything worked out. First Unique Character in a String public int firstUniqChar(String s) { int[] freq = new int[26];\t// Preprocess freq array to maintain freq of each char[] chars = s.toCharArray();\t// character in the string s for (char c: chars) ++freq[c-\u0026#39;a\u0026#39;]; for (int i = 0; i \u0026lt; chars.length; i++)\t// Make a second pass through the chars of the if (freq[chars[i]-\u0026#39;a\u0026#39;] == 1)\t// string in order, and if any of the char\u0026#39;s return i;\t// frequency is 1, that\u0026#39;s our unique char return -1;\t// Otherwise, no unique character } Find the Difference /** The general idea here is same as the problem where we are required to find a unique int in an array containing duplicates except one. We use the xor operator between each character of the string s and t, and the ones that are duplicate will xor to give 0. XOR of any element with 0 is the element itself, and XOR of two same elements gives 0. This way, since string s and t basically has pairs of repeating characters except one, the unique element will XOR with 0 and give us it\u0026#39;s ASCII code. The only thing we need to take care of is to now shift it up by 26, so we add \u0026#39;a\u0026#39; and convert it to char. */ public char findTheDifference(String s, String t) { int xor = 0; for (char c: s.toCharArray()) xor ^= c-\u0026#39;a\u0026#39;; for (char c: t.toCharArray()) xor ^= c-\u0026#39;a\u0026#39;; return (char)(xor+\u0026#39;a\u0026#39;); } Nth Digit /** Notice that # of digits between 0-9 is 1*9, 10-99 is 2*90, 100-999 is 3*900. If we generalize it, it is exactly equal to 9 * (num of digits in the number) * 10^{# of digits - 1}. */ public int findNthDigit(int n) { if (n \u0026lt; 10) return n; int pow = 1;\t// First we need to figure out how many digits there are long upperBound = 9;\t// in the number. while (n \u0026gt; upperBound){ n -= upperBound;\t// If n is a two digit number, subtract the 9 single digit ++pow;\t// numbers, if 3 digit, subtract the first 189 digits. upperBound = (long)Math.pow(10, pow-1) * pow * 9; }\t// pow allows us to track how many digits there are in num. int num = (int)Math.pow(10,pow-1) + (n-1)/pow;\t// Calculate which number we want int position = pow - 1 - (n-1) % pow;\t// Calculate which index we want for (int i = 0; i \u0026lt; position; i++)\t// Divide num that many times num /= 10; return num % 10;\t// num % 10 gives us that digit. } Sum of Left Leaves public int sumOfLeftLeaves(TreeNode root) { if (root == null)\t// Empty tree, therefore total is 0. return 0; int sum = 0;\t// Initialize sum. // Look ahead and check. If left is not null but left is a leaf, then sum is the value of the left leaf. // But if left is null or left is an inner node, then we need to explore it, so sum is whatever the subtree from the left node returns. if (root.left != null \u0026amp;\u0026amp; root.left.left == null \u0026amp;\u0026amp; root.left.right == null) sum = root.left.val; else sum = sumOfLeftLeaves(root.left); // We computed the sum of the left side. Now we need to traverse the right side and fetch // the sum, so total sum is sum of the left side as computed above + sum returned by // traversing the right side. return sum + sumOfLeftLeaves(root.right); } Longest Palindrome public int longestPalindrome(String s) { int[] freq = new int[128];\t// To record the frequency of each char for (char c: s.toCharArray()) freq[c]++;\t// Increment count by 1 for each character observed int len = 0;\t// length of the longest palindrome boolean isOdd = false;\t// Check if our palindrome length is odd for (int i = 0; i \u0026lt; 128; i++){\t// Go through each character\u0026#39;s index if (freq[i] != 0){\t// Only if it has been observed atleast once int val = freq[i];\t// Store it\u0026#39;s frequency int used;\t// Record how many of it\u0026#39;s occurrences we will use if (val % 2 == 0)\t// If a perfect multiple of 2, we will use all used = val; else{ used = val-1;\t// If odd occurrences, then the max we can use to form a isOdd = true;\t// valid palindrome is val-1. It also tells us that the }\t// palindrome is going to be of odd length. len += used;\t// Finally, increment length by the number of chars used } } if (isOdd)\t// If length is odd, we can always insert any single return len+1;\t// character in the middle to keep the palindrome valid. return len;\t// If the length is even, then we can\u0026#39;t do anything. } Fizz Buzz public List\u0026lt;String\u0026gt; fizzBuzz(int n) { List\u0026lt;String\u0026gt; nums = new ArrayList\u0026lt;String\u0026gt;(); for (int i = 1; i \u0026lt;= n; ++i){\t// Loop from 1 to n if (i % 15 == 0)\t// If i divisible by 15, add \u0026#34;FizzBuzz\u0026#34; nums.add(\u0026#34;FizzBuzz\u0026#34;); else if (i % 5 == 0)\t// i\u0026#39;s not a multiple of 15, check if it\u0026#39;s a nums.add(\u0026#34;Buzz\u0026#34;);\t// multiple of 5. If so, add \u0026#34;Buzz\u0026#34; else if (i % 3 == 0)\t// i\u0026#39;s not a multiple of 5, check if it\u0026#39;s a nums.add(\u0026#34;Fizz\u0026#34;);\t// multiple of 3, if so, add \u0026#34;Fizz\u0026#34; else nums.add(i+\u0026#34;\u0026#34;);\t// Otherwise, just add the String type of the }\t// number return nums; } Third maximum Number public int thirdMax(int[] nums) { if (nums.length == 0)\t// Empty array return 0; if (nums.length == 1)\t// Size 1 array return nums[0]; if (nums.length == 2)\t// Size 2 array, check between 0th element or 1st element return nums[0] \u0026gt; nums[1] ? nums[0] : nums[1]; long firstMax = Long.MIN_VALUE;\t// Lowest values for all three long secondMax = Long.MIN_VALUE; long thirdMax = Long.MIN_VALUE; for (int i: nums){\t// For each number in the array if (i \u0026gt; firstMax){\t// If num \u0026gt; than the largest, then old largest thirdMax = secondMax;\t// becomes second largest and second largest becomes secondMax = firstMax;\t// first largest, then update the largest. firstMax = i; } else if (i \u0026gt; secondMax \u0026amp;\u0026amp; i != firstMax){\t// If num \u0026gt; second and num is not is the thirdMax = secondMax;\t// same as first, first largets becomes secondMax = i;\t// second largest and update the second } else if (i \u0026gt; thirdMax \u0026amp;\u0026amp; i != secondMax \u0026amp;\u0026amp; i != firstMax) // // If num \u0026gt; third, we thirdMax = i;\t// need to check that it is not the same }\t// as the first and second largest. if (thirdMax == Long.MIN_VALUE)\t// This check allows us to make sure that return (int)firstMax;\t// we do indeed have a third max and is return (int)thirdMax;\t// not what we initialized initially. } Add Two Strings public String addStrings(String num1, String num2) { if (num1.equals(\u0026#34;0\u0026#34;)) return num2; if (num2.equals(\u0026#34;0\u0026#34;)) return num1; /** We use a char array to maintain the digit at each index. We want the array to be of the size of the largest string + 1 to handle carry bit if any at the end. We start adding each digit of the string from the end, and place it in it\u0026#39;s correct index at the end of the sum array. This way, we avoid reversing it and return the answer in constant time. Take care to convert the digit you compute by adding \u0026#39;0\u0026#39;. Lastly, if the carry bit is 1, we need to make the 0th index as 1, and return the string by using the sum array. If it\u0026#39;s not 1, then the sum array has a leading 0 which we don\u0026#39;t want. So we use Java\u0026#39;s String constructor that takes in the char array, startingIndex in that array and the number of elements of that array we want. So if the carry isn\u0026#39;t 1, we technically want everything from index 1 and # of elements = sum.length - 1 because we discard 0 index. */ char[] sum = new char[1 + Math.max(num1.length(), num2.length())]; int index = sum.length-1, idx1 = num1.length()-1, idx2 = num2.length()-1, carry = 0, total = 0; int n1, n2; while (idx1 \u0026gt;= 0 || idx2 \u0026gt;= 0){ n1 = idx1 \u0026lt; 0 ? 0 : num1.charAt(idx1--)-\u0026#39;0\u0026#39;; n2 = idx2 \u0026lt; 0 ? 0 : num2.charAt(idx2--)-\u0026#39;0\u0026#39;; total = n1 + n2 + carry; carry = total/10; sum[index--] = (char)(total % 10 + \u0026#39;0\u0026#39;); } if (carry == 1){ sum[0] = \u0026#39;1\u0026#39;; return new String(sum); } return new String(sum, 1, sum.length-1); } Construct Quad Tree private int[][] grid;\t// Store it once, instead of passing it over \u0026amp; over. public Node construct(int[][] _grid) { grid = _grid; return helper(0,0,grid.length);\t// Ask helper to build the tree. } private Node helper(int top, int left, int len){ if (len \u0026lt;= 0)\t// Base case: if empty grid or if we are done return null;\t// checking the full grid, return null int key = grid[top][left];\t// Get the topleft value, and start checking the box for (int i = 0; i \u0026lt; len; ++i){\t// of len*len. If at any point, the value doesn\u0026#39;t for (int j = 0; j \u0026lt; len; ++j){\t// match the key, we have found a breakpoint from if (grid[top+i][left+j] != key){\t// where we need to break the grid into four int offset = len/2;\t// grids, each of len = len/2. The topleft grid has return new Node(true, false, // the same top and left point, the topright helper(top,left, offset),\t// grid has left point shifted to helper(top, left + offset, offset),\t// the right by offset. helper(top+offset, left, offset),\t// The bottom left grid helper(top+offset, left+offset, offset));\t// is shifted }\t// downwards by offset with the same left point. The bottom right grid will }\t// have an index where it\u0026#39;s top is shifted down by len/2 and left by left/2. }\t// We know that the node will have a value = true if 1 else false and it won\u0026#39;t be a leaf, so true, false, topleft, topright, bottomleft, bottomright. return new Node(key == 1, true, null, null, null, null);\t// Everything passed, so we return a new Node whose value is true if key is 1, else false and it will be a leaf, with // no children, so 4 nulls. } N-ary Tree Level Order Traversal public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(Node root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;();\t// Result list if (root == null)\t// If root is null, return empty list. return res; Queue\u0026lt;Node\u0026gt; q = new LinkedList\u0026lt;\u0026gt;();\t// BFS Queue. Add the root. q.add(root); while (!q.isEmpty()){\t// While q isn\u0026#39;t empty int size = q.size();\t// Check how many elements in that level List\u0026lt;Integer\u0026gt; level = new ArrayList\u0026lt;\u0026gt;(size);// level list to store elements. for (int i = 0; i \u0026lt; size; i++){\t// Remove each node for whatever the size Node n = q.poll();\t// Add that node\u0026#39;s value and add all of level.add(n.val);\t// its children to the queue. for (Node child: n.children) q.add(child); } res.add(level);\t// Add the level array to the result } return res;\t// Return the result list. } Number of Segments in a String public int countSegments(String s) { if (s.length() == 0)\t// Empty String return 0; int segments = 0;\t// Record segments char prev = s.charAt(0);\t// We will compare adjacent characters. for (int i = 1; i \u0026lt; s.length(); ++i){\t// Start looking at chars from index 0 char curr = s.charAt(i);\t// Get the current char if (prev != \u0026#39; \u0026#39; \u0026amp;\u0026amp; curr == \u0026#39; \u0026#39;)\t// If previous char wasn\u0026#39;t a space but the ++segments;\t// current char is, we found a segment. prev = curr;\t// Make previous = current for next iteration } /** This line is important. If prev was an empty space, that means that all we have been looking at was empty spaces towards the end. So return whatever segments we found in the beginning of the string. But if prev wasn\u0026#39;t a space, that means the char next to prev might have been an empty space or just a normal character. In any case, we would want to include that last segment, so we return segment+1. */ return prev == \u0026#39; \u0026#39; ? segments : segments+1; } Binary Tree Level Order Traversal public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); if (root == null)\t// Empty Tree return result; Queue\u0026lt;TreeNode\u0026gt; q = new LinkedList\u0026lt;\u0026gt;();\t// BFS Queue q.add(root); while (!q.isEmpty()){\t// While we have something to process List\u0026lt;Integer\u0026gt; level = new ArrayList\u0026lt;\u0026gt;(); int size = q.size();\t// Check how many elements at the current level for (int i = 0; i \u0026lt; size; i++){ TreeNode node = q.poll();\t// Remove one element each time if (node != null){\t// If not null, add it\u0026#39;s val to the level list, level.add(node.val);\t// and it\u0026#39;s left and right children to the queue q.add(node.left);\t// to process in order q.add(node.right); } } if (!level.isEmpty())\t// If level list wasn\u0026#39;t empty, result.add(level);\t// add it to the result list. } return result; } Path Sum III HashMap\u0026lt;Integer, Integer\u0026gt; sumToWays;\t// Record how many ways there are to form sum int ways;\t// Total number of ways. public int pathSum(TreeNode root, int sum) { sumToWays = new HashMap\u0026lt;\u0026gt;(); ways = 0; sumToWays.put(0,1);\t// 1 way to form a sum of 0. helper(root, 0, sum); return ways; } /** The idea here is as follows. Start with the root node, and keep a running total. We maintain how many ways there to form a running sum. Then we check how many ways there are to form (running sum) - (sum we are looking for). If there is a way to form it, then we increase the number of ways to form sum. We then have to update the map to record how many ways can the running sum be formed. If it\u0026#39;s something we could form before, increment it, or else set it to 1. Now, traverse the left side and then the right side. In the end, for each time we incremented the count for a running sum, we need to decrement it because we are backtracking. We are first going down, incrementing the count for runningSum, then we move up and decrement it by 1 for each time we observed it. This is to maintain the Pre-Order traversal. */ private void helper(TreeNode node, int runningSum, int sum){ if (node == null) return; runningSum += node.val; ways += sumToWays.getOrDefault(runningSum-sum, 0); sumToWays.put(runningSum, sumToWays.getOrDefault(runningSum, 0)+1); helper(node.left, runningSum, sum); helper(node.right, runningSum, sum); sumToWays.put(runningSum, sumToWays.get(runningSum)-1); } Find All Anagrams in a String public List\u0026lt;Integer\u0026gt; findAnagrams(String s, String p) { List\u0026lt;Integer\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); int start = 0, end = 0, slen = s.length(), plen = p.length(); if (slen == 0 || slen \u0026lt; plen || plen == 0) return result; int[] freq = new int[26];\t// Store the freq of chars in p for (char c: p.toCharArray()) freq[c-\u0026#39;a\u0026#39;]++; char[] sArr = s.toCharArray();\t// Get the chars of the string s as an array while (end \u0026lt; slen){\t// While everything is not processed if (--freq[sArr[end]-\u0026#39;a\u0026#39;] \u0026gt;= 0)\t// decrease the freq of the char at index end plen--;\t// if it\u0026#39;s \u0026gt; 0, then we matched something in p // so decrease plen by 1. while (plen == 0){\t// If plen goes to 0, we were able to match all if (end-start+1 == p.length())\t// chars of p. If length of the matched chars is result.add(start);\t// equal to length p, we found a start point. if (freq[sArr[start]-\u0026#39;a\u0026#39;] \u0026gt;= 0)\t// Check if the freq of char at start index is plen++;\t// \u0026gt;= 0. If it is, shift the window to the right ++freq[sArr[start++]-\u0026#39;a\u0026#39;];\t// but first restore the frequency of the char }\t// at the index start. end++;\t// Get ready to inspect the new element } return result;\t// Return the answer. } Arranging Coins The idea is as follows. Sum of first n numbers is given by $\\frac{n^2+n}{2}$. We need to find $n$ such that sum of $n$ numbers is closest to the number of coins we have. That is, $\\frac{n^2+n}{2} = k$ where $k$ is the number of coins we have. So, everything boils down to solving the quadratic equation $n^2 + n - 2k = 0$. We use the quadratic formula where for any quadratic equation $ax^2 -bx + c$ is solved substituting for $a$, $b$ and $c$ in $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2}$. Here, $a$ and $b$ are always going to be 1, while $c$ is always going to be $2k$. Substitute those, and solve the equation.\npublic int arrangeCoins(int n) { // return solveQuadratic(n); return iterative(n); } private int solveQuadratic(int n){ return (int)(Math.sqrt(1 + 8*(long)n)-1)/2; } private int iterative(int n){ int used = 1, level = 0;\t// Coins used, and level completed. while (n \u0026gt; 0){\t// While coins left are greater than 0. n-=used;\t// Calculcate remaining coins. if (n \u0026gt; -1)\t// If there are still some coins left, ++level;\t// we were able to fill the level. ++used;\t// Prepare used for the next level, which is plus 1. } return level;\t// Return level } Hamming Distance public int hammingDistance(int x, int y) { int diff = 0;\t// Track differences while (x != 0 || y != 0) {\t// While both of them aren\u0026#39;t 0 if (x % 2 != y % 2)\t// Check the bit of x and y by mod 2. If they are unequal diff++;\t// increment difference. x /= 2;\t// Divide x and y by 2. y /= 2; } return diff; } String Compression public int compress(char[] chars) { int len = chars.length;\t// No need to reverse array of length 0 or 1 if (len \u0026lt; 2) return len; int arrayIndex = 0;\t// To maintain the length of new array. int start = 0;\t// start index int end = 0;\t// end index while (end \u0026lt; len){ char first = chars[start];\t// Record the char we are looking at. int count = 0;\t// count is 0. while (end \u0026lt; len \u0026amp;\u0026amp; chars[end] == first){\t// while the char is the same ++end;\t// increment end to check next char ++count;\t// and increment the count. } start = end;\t// shift start to end to check next sequence of chars chars[arrayIndex++] = first;\t// our arrayIndex points to to the new array\u0026#39;s if (count != 1){\t// indices. So copy the first char to arrayIndex. if (count \u0026gt; 1 \u0026amp;\u0026amp; count \u0026lt; 10)\t//Only if count isn\u0026#39;t 1, if count is less than 10 chars[arrayIndex++] = (char)(count+\u0026#39;0\u0026#39;);\t// then we simply convert count to char and write it next to the char we just overwrote. else\t// Otherwise, it has many digits. So convert it to for (char c: String.valueOf(count).toCharArray()){\t// string and add all it\u0026#39;s digit to the array one by one while increment arrayIndex. chars[arrayIndex++] = c; } } } return arrayIndex;\t// Wherever arrayIndex is, is the new length for the array. } Number of Boomerangs public int numberOfBoomerangs(int[][] points) { int boomerangs = 0; HashMap\u0026lt;Double, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;();\t// To record points with same dist for (int[] i: points){\t// Compute distance between one point and every other. map.clear()\t// clear map before each relative distance computation for (int[] j: points){\t// Compute distance with other points if (i == j)\t// Don\u0026#39;t compare the same two points. continue; double dist = Math.sqrt(Math.pow(i[0]-j[0],2) + Math.pow(i[1]-j[1],2)); int prevCount = map.getOrDefault(dist, 0);\t// Check how many points are equidistant from point i. boomerangs += prevCount * 2;\t// Number of boomerangs = whatever pairs there were before times 2, because you can form twice the number of different orders. map.put(dist, prevCount+1);\t// Increase the count of points observed for that distance. } } return boomerangs;\t// return number of boomerangs } Find All Numbers Disappeared in an Array /** The idea is simple. For each number in the array, since it\u0026#39;s gauranteed that that the values lie are inclusive [1,n], we can look at the index value-1. So check that index, and mark that value as negative. That is why I take the absolute value. Check value at that index, if negative, it means we have visited it via some other duplicate value. But if it\u0026#39;s positive, then we are seeing it for the first time, so make it\u0026#39;s value negative. Make a second pass. For values that are still positive, that means those indices were never visited, hence left positive. So add 1 to them and add it to the set. Eg: Given array a = [4,3,2,7,8,2,3,1], 1.\tval = 4 =\u0026gt; idx = 3 \u0026amp; a[3] \u0026gt; 0, therefore, a[3] *= -1 a = [4,3,2,-7,8,2,3,1] 2.\tval = 3 =\u0026gt; idx = 2 \u0026amp; a[2] \u0026gt; 0, therefore a[2] *= -1 a = [4,3,-2,-7,8,2,3,1] 3.\tval = -2 =\u0026gt; idx = abs(-2)-1 = 1 \u0026amp; a[1] \u0026gt; 0, therefore a[1] *= -1 a = [4,-3,-2,-7,8,2,3,1] 4.\tval = -7 =\u0026gt; idx = abs(-7)-1 = 6 \u0026amp; a[6] \u0026gt; 0, therfore a[6] *= -1 a = [4,-3,-2,-7,8,2,-3,1] 5.\tval = 8 =\u0026gt; idx = abs(8)-1 = 7 \u0026amp; a[7] \u0026gt; 0, therfore a[7] *= -1 a = [4,-3,-2,-7,8,2,-3,-1] 6.\tval = 2 =\u0026gt; idx = 1 but a[2] \u0026lt; 0. No change. 7.\tval = -3 =\u0026gt; idx = abs(-3)-1 = 2 but a[2] \u0026lt; 0. No change. 8.\tval = -1 =\u0026gt; idx = abs(-1)-1 = 0 \u0026amp; a[0] \u0026gt; 0, therefore a[0] *= -1 a = [-4,-3,-2,-7,8,2,-3,-1] Observation: Notice index 4 and 5 have positive values, since those values were never encountered, so the values at those indexes never became negative. Hence missing values are 5 and 6. */ public List\u0026lt;Integer\u0026gt; findDisappearedNumbers(int[] nums) { List\u0026lt;Integer\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (int i: nums){\t// For each number in the array int idx = Math.abs(i)-1;\t// Look at the index that the number corresponds to if (nums[idx] \u0026gt; 0)\t// If val is -ve, then it means we have encountered it. nums[idx] *= -1; // If not, make it -ve. } for (int i = 0; i \u0026lt; nums.length; ++i) if (nums[i] \u0026gt; 0)\t// Make another pass through the array, and the indices result.add(i+1);\t// where value was positive, index+1 was missing from return result;\t// the array } Assign Cookies /** We will employ a greedy algorithm where we first try to content children whose requirements are small. We do this by sorting both the arrays, so we can match the child with least requirement with the smallest cookie available. */ public int findContentChildren(int[] g, int[] s) { Arrays.sort(g); Arrays.sort(s); int satisfied = 0, i = 0, j = 0; while (i \u0026lt; g.length \u0026amp;\u0026amp; j \u0026lt; s.length){\t// While children are left and we have cookies, if (s[j] \u0026gt;= g[i]){\t// Check if the cookie at index j \u0026gt;= child i\u0026#39;s requirement satisfied++;\t// If so, increment the number of content child and we will i++;\t// process the next child. }\t// If cookie j \u0026lt; child i\u0026#39;s demand, check the next cookie by j++;\t// incrementing j. If cookie j \u0026gt; child i\u0026#39;s demand, we will }\t// still need to increment j, hence outside the conditional. return satisfied;\t// Return number of satisfied children } Poor Pigs Link to the solution explanation. This problem is phrased poorly and I had to read the comments by other users to understand what it required from me. The link I marked here explains the logic pretty good. But the simple logic is this: The number of rounds $r = \\frac{Total Test Time}{Minutes To Die} +1$. Each pig has chances of dying in each round or staying alive till the end, so we plus 1. Now given the number of rounds $r$ and the number of samples $s$, how many volunteers $v$ will you need? $r^v = s$. Each round has some volunteers which in total at the end should be able to test out all the samples. Therefore, $v =\\log_rs$.\npublic int poorPigs(int buckets, int minutesToDie, int minutesToTest) { int base = minutesToTest/minutesToDie+1;\t// How many rounds can you perform? return (int)Math.ceil(Math.log(buckets)/Math.log(base)); } Find Pivot Index public int pivotIndex(int[] nums) { int sum = 0, leftSum = 0;\t// We will test each index as a pivot by sliding it -\u0026gt; for (int i: nums)\t// Precalculate the sum of the array sum += i; for (int i = 0; i \u0026lt; nums.length; ++i){\t// Check if the sum of the leftSide of i is if (leftSum == sum - leftSum - nums[i])\t// equal to totalSum - leftSideSum - pivot return i;\t// which is i. If so, return i. leftSum += nums[i];\t// Otherwise add nums[i] to the leftSum and }\t// slide pivot to the -\u0026gt;. return -1;\t// No pivot found. Return -1. } Squares of a Sorted Array public int[] sortedSquares(int[] A) { int len = A.length;\t// Length of array A int pivot = 0;\t// Pivot is the index where values goes from -ve to +ve. while (pivot \u0026lt; len \u0026amp;\u0026amp; A[pivot] \u0026lt; 0) // While values are -ve. ++pivot;\t// increment pivot. We exit when we find a positive. int[] squares = new int[len];\t// Result array int index = 0;\t// Keeps track of where to where to put elements in result array if (pivot == 0)\t// pivot = 0 means pivot didn\u0026#39;t shift, there are only +ve values for (int i: A)\t// So fill in the array with squares of numbers. squares[index++] = i*i; else{\t// Otherwise we have a negative somewhere. int left = pivot-1;\t// So we will compare values left and right of the pivot int right = pivot;\t// and whichever\u0026#39;s smaller fills up the array first. while (left \u0026gt; -1 \u0026amp;\u0026amp; right \u0026lt; len){ int lsquare = A[left] * A[left]; int rsquare = A[right] * A[right]; if (lsquare \u0026lt; rsquare){\t// left \u0026lt; right, so add left square. decrement left squares[index++] = lsquare; --left; } else if (rsquare \u0026lt; lsquare){\t// right \u0026lt; left, add right square and increment. squares[index++] = rsquare; ++right; } else{ squares[index++] = lsquare;\t// both are equal. add both square and squares[index++] = rsquare;\t// decrement left, increment right. --left;\t// Continue doing this until we hit either end ++right;\t// of the array. }\t// In the end we need to check if elements on }\t// either side are left to be filled in. while (left \u0026gt; -1)\t// Left side elements remain, so fill their squares[index++] = A[left] * A[left--]; // squares one by one till none left. while (right \u0026lt; len)\t// Right side elements remain, so fill their squares[index++] = A[right] * A[right++];\t// squares in } return squares; } Repeated Substring Pattern We use the KMP Algorithm that allows us to match a string \u0026rsquo;s\u0026rsquo; with another string \u0026lsquo;p\u0026rsquo; to find the longest sequence of characters in \u0026rsquo;s\u0026rsquo; that match \u0026lsquo;p\u0026rsquo;. We can use a Naive Pattern match where we start from the beginning of the string and start comparing the characters of \u0026rsquo;s\u0026rsquo; with \u0026lsquo;p\u0026rsquo;. Initially, we keep the partition at index 0. If the character\u0026rsquo;s match, we move partition to the right by 1 till we get to the end of the string. If something doesn\u0026rsquo;t match, we don\u0026rsquo;t move the partition but look at the next character to match. In the end, wherever the partition is, that\u0026rsquo;s our longest length we could match with string \u0026lsquo;p\u0026rsquo;. The complexity of that is O(len(p)(len(s)-len(p)+1)).\nKMP fixes it by skipping characters that we know already match. In this problem, we aren\u0026rsquo;t matching with any other string but itself. So, we start from index 1 of the string and compare it from the beginning. If they match, we increase j by 1, note it down in lps array and then increase i by 1 to check the next character. j basically measures the longest chain of characters we were able to match. If we couldn\u0026rsquo;t match character at index i and if streak was greater than 0, then our new streak becomes whatever it was in the previous round of matching characters. If the streak is 0, then we simply note down at index i in our lps array 0, meaning longest length measured upto index i was 0.\npublic boolean repeatedSubstringPattern(String s) { int maxLength = lps(s); return maxLength \u0026gt; 0 \u0026amp;\u0026amp; s.length() % (s.length() - maxLength) == 0; } private int lps(String s){ int len = s.length(); int[] lps = new int[len]; int i = 1;\t// To match the string with itself. int j = 0; while (i \u0026lt; len){ if (s.charAt(i) == s.charAt(j)){\t// if the chars match lps[i] = ++j;\t// we record that # of matches at index i was ++i;\t// 1+j and increment i to check next character } else{\t// character did not match if (j \u0026gt; 0)\t// If our matching streak \u0026gt; 0 j = lps[j-1];\t// our new streak becomes the previous round\u0026#39;s streak else\t// Otherwise, streak is already 0. lps[i++] = 0;\t// So we record that # of matches made at i is 0 }\t// We increment i to check next index. } return lps[len-1];\t// Longest prefix length that was also a suffix }\t// is whatever was recorded at the end of array. Island Perimeter The idea is simple. Count the number of cells with value 1 which denotes the land. Check towards the left and up to that cell and check if it shares any edge with another cell with value 1. If it does record that. In the end, the formula for perimeter is 4 * (the number of land cells) - 2 * (overlapping edges).\nReasoning: Perimeter of a square is 4 times the length of it\u0026rsquo;s side. Here all squares are of length 1. So total perimeter is 4*(number of cells with value = 1). But we also need to account the edges that are common between two adjacent land cells. If one square shares an edge with another, we just lost one side from both the square, resulting in a loss of two sides. Therefore, we need to subtract twice the number of overlapping edges from the total perimeter to get the total perimeter.\npublic int islandPerimeter(int[][] grid) { int land = 0; int overlap = 0; for (int row = 0; row \u0026lt; grid.length; ++row) for (int col = 0; col \u0026lt; grid[0].length; ++col){ if (grid[row][col] == 1){ ++land; if (row-1 \u0026gt; -1 \u0026amp;\u0026amp; grid[row-1][col] == 1)\t// Check above the current cell. ++overlap;\t// If it\u0026#39;s a land, we need to record one overlap. if (col-1 \u0026gt; -1 \u0026amp;\u0026amp; grid[row][col-1] == 1)\t// Similarly, check to the left. ++overlap;\t// If it\u0026#39;s a land, we need to increment overlap } } return 4*land - 2*overlap;\t// Check the reasoning above. } Number Complement public int findComplement(int num) { int pow2 = 1;\t// Easily keep track of power of 2. int comp = 0;\t// Complement number while (num != 0){\t// Since num gets divided by 2, it will be 0 in the end. int bit = num % 2 == 0 ? 1 : 0;\t// If bit is 0 then complement is 1 \u0026amp; vice versa. comp += bit * pow2;\t// Multiply it by the appropriate power of 2 and add to comp pow2 *= 2;\t// Update power of 2 for next iteration. num /= 2;\t// Divide num by 2 to get the next bit. } return comp;\t// Comp is now the complement. } Binary Watch The idea is as follows. We have 10 lights. First 4 represent hours. Namely 1, 2, 4 and 8, which are the first four powers of 2. The next 6 lights, represent minutes. Those are 1, 2, 4, 8, 16 and 32. These are powers of 2 from 0-5. So if we iterate from 1 to 9, powers of numbers 1-3 gives us hours and powers of numbers 4-9 minus 4 gives us minutes. So, if we have, let\u0026rsquo;s say 2 lights, we need to find every combination of 2 lights. So in our helper function, we iterate from 1-9 to check every hour and minute combination. We also need to keep a track of the lights that we used, so we don\u0026rsquo;t use the same light again. If hours are \u0026gt; 11 or minutes are \u0026gt; 59, we have an invalid time and we can abort. If the number of lights are 0, that means we found a valid time and we should add it to the result. Now, if the lights are not 0, then we need to check every possible combination from the last light used to 9. If i \u0026lt; 4, then we are looking at an hourly combination, otherwise it\u0026rsquo;s a minute combination. So we recurse with updated lights used, decrease the numOfLights since we used one, update respective hours or minutes until we hit base case.\nList\u0026lt;String\u0026gt; result; public List\u0026lt;String\u0026gt; readBinaryWatch(int num) { result = new ArrayList\u0026lt;\u0026gt;(); helper(0, num, 0, 0); return result; } private void helper(int lightsUsed, int numOfLights, int hrs, int min){ if (hrs \u0026gt; 11 || min \u0026gt; 59)\t// Base case. Invalid time return; if (numOfLights == 0){\t// All lights used, so add time to the list. result.add(hrs + \u0026#34;:\u0026#34; + (min \u0026lt; 10 ? \u0026#34;0\u0026#34; + min : min)); return; } for (int i = lightsUsed; i \u0026lt; 10; i++){\t// Otherwise start recursing from number of prev if (i \u0026lt; 4)\t// light used. i \u0026lt; 4 means hours helper(i+1, numOfLights-1, hrs + (int)Math.pow(2, i), min); else\t// i = [4,9] means minute. So recurse. helper(i+1, numOfLights-1, hrs, min + (int)Math.pow(2,i-4)); } } Minimum Moves to Equal Array Elements This was an interesting problem. But after working out a few examples by hand, you can notice that it is always a question of bringing the minimum element in par with everyone. So if you know the minimum of the array, we can check how many steps it will take to bring the minimum in par with other element by calculating the distance between them. For example,\nLet the array be [1,2,3]\nWe can observe that the minimum here is 1. Let us list down all steps to make all elements equal.\n[2,2,4], Keeping the second element fixed. Notice that distance between the element where 1 was and where 3 was is till the same. [3,3,4], Keeping the last element fixed. [4,4,4], Keeping last element fixed. Here, we first tried to make 1 equal to it\u0026rsquo;s neighbor, which required us 1 step. Now, once it becomes equal to 1, the problem is how to make the last element in the original array, which is 3 equal to 1. It requires 2 steps, resulting in a total of of 3. The reason is that the moment you decide to increment the minimum element to match the next element, you fix the neighboring element and have to increment everything else. This will make the minimum and its neighbor the same, but it will also keep the distance between the minimum and all other elements the same because we just incremented everything.\nSo, the total number of moves required is the distance between the elements of the array and the minimum.\npublic int minMoves(int[] nums){ int min = nums[0]; for (int i: nums) if (i \u0026lt; min) min = i; int moves = 0; for (int i: nums) moves += i-min; return moves; } Now the above solution required two passes of the array. Can we do even better? Notice that in the end, all we are doing is finding the min and subtracting min from all the elements in the array. That means we are subtracting min n times where n is the length of the array. Why n times? Because there are n elements in the array. Shouldn\u0026rsquo;t it be (n-1) times? No, because the distance of the min from min is 0. So we need to subtract min from itself too, so n times. We can achieve this by first calculating the total of the array while simultaneously keeping track of the minimum. Once done, all we need to do is subtract min n times from the sum, which is equivalent to subtracting min from each element. This results in a much overall better algorithm, requiring only 1 pass of the array.\npublic int minMoves(int[] nums) { int sum = 0, min = nums[0]; for (int i: nums){ sum += i; if (i \u0026lt; min) min = i; } return sum - min*nums.length; } License Key Formatting The idea is simple.\nI maintain a temporary array s that contains only the characters in string S after converting them to uppercase. I maintain a variable length that counts how many characters I found in the string S. If length is 0, that means it contains only dashes (-). Then I record the offset. Offset basically measures how many characters of the String S will be grouped unevenly in the beginning part of the string. I can check that by using the modulus operator and finding out the remainder. That many characters (of length \u0026lt; K) will be in the beginning part of the string. Next step is to calculate how many dashes I will need. It\u0026rsquo;s basically length / K. Then I create the char array that will hold the characters of the formatted key. It\u0026rsquo;s length will be number of characters + the dashes we will need. We need to take care of a special case here. If the offset is 0, meaning I was able to divide characters in equal group, I need to subtract 1. Eg, let\u0026rsquo;s say we had 8 characters and K was 4. dashes = 8 / 4 = 2. We can divide 8 characters equally into 2 groups using only 1 dash. But since dashes was 2, it is clearly off by 1. This is the case when offset is 0. kIndex tracks where character is to be inserted in the key array. used tracks how many characters of the array s, which indirectly holds the characters of String S, are used. First I copy down the characters of length offset. Because those are the ones of uneven length. kIndex and used variables are updated. Last thing to do is to use all the remaining characters in array s, but we take K characters at a time, because we know that the segments are going to be of equal length. We also need to insert \u0026lsquo;-\u0026rsquo; after each segment, but only if kIndex is not at the beginning or at the end of the key array, because inserting it at those points is invalid. Create a new string and return it. public String licenseKeyFormatting(String S, int K) { char[] s = new char[S.length()]; int length = 0; for (char c: S.toCharArray()) if (c != \u0026#39;-\u0026#39;) s[length++] = Character.toUpperCase(c); if (length == 0) return \u0026#34;\u0026#34;; int offset = length % K; int dashes = length / K; char[] key = new char[length + dashes + (offset == 0 ? -1 : 0)]; int kIndex = 0; int used = 0; while (used \u0026lt; offset) key[kIndex++] = s[used++]; while (used \u0026lt; index){ if (kIndex \u0026gt; 0 \u0026amp;\u0026amp; kIndex \u0026lt; key.length) key[kIndex++] = \u0026#39;-\u0026#39;; for (int i = 0; i \u0026lt; K; ++i) key[kIndex++] = s[used++]; } return new String(key); } Max Consecutive Ones Solution 1: I came up with this solution initially. 4 ms runtime and passes 99.97% submissions.\npublic int findMaxConsecutiveOnes(int[] nums) { int start = 0;\t// Keep track of start of a streak, if any int max = 0;\t// max length of the streak while (start \u0026lt; nums.length){\t// While we are not at the end of the array if (nums[start] == 1){\t// Check if we have a 1 at start, if so int streak = 0;\t// initialize streak and check how long can we continue while (start \u0026lt; nums.length \u0026amp;\u0026amp; nums[start] == 1){\t// that streak. ++streak;\t// Increment streak and left for each consecutive 1 ++start;\t// make sure you don\u0026#39;t forget that start \u0026lt; nums.length }\t// before checking nums[start] to prevent out-of-bounds if (streak \u0026gt; max)\t// Check if the current streak is better than the max = streak;\t// previous streak. } ++start;\t// Increment start in either case to check for new }\t// streaks. return max; } Solution 2: After analyzing the problem further, I noticed that 0 denotes the end of a streak. If we observe 1, we increment streak by 1. But if I see a 0, I reset my streak to 0. This solution too had a 4 ms runtime and passed 99.97% submissions.\npublic int findMaxConsecutiveOnes(int[] nums) { int max = 0;\t// Global max streak int streak = 0;\t// Local max streak. for (int i: nums){\t// For each number in nums if (i == 1){\t// If we see a 1 ++streak;\t// increment our ongoing streak. if (streak \u0026gt; max)\t// If the local streak \u0026gt; global max max = streak;\t// update global max streak. } else\t// otherwise we just saw a 0. streak = 0;\t// So our streak resets to 0. } return max;\t// return the global max streak. } Permutations The idea is as follows. Given an array a = {1,2,3}, we want to generate all it\u0026rsquo;s possible combinations. What we are trying to do here is that we first take the element at index 0, and find permutations of the remaining thing. When we do that, we insert the element at index 0 in front of the list to get 1 permutation. Similarly, we then take the element at index 1, and permute the remaining contents of the array and insert the element at index 1 in the beginning of the array to get another permutation and so on. In this problem, we are asked to return a list of list, so we first copy the numbers of the array into an ArrayList. Let\u0026rsquo;s run this code for the above example.\nGiven nums = {1,2,3}, our ArrayList will be the same, al = [1,2,3]. Our result list is empty, result = [] and index = 0.\nhelper([1,2,3], 0)\nswap (0, 0) → al = [1,2,3]\nhelper(1,2,3, 1)\nswap(1, 1) → al = [1,2,3]\nhelper([1,2,3], 2)\nswap(2, 2) → [1,2,3]\nhelper([1,2,3], 3)\nWe update our result list now, because index == length. Therefore, result = [[1,2,3]]. Our recursive stack collapses and we move on to the next instruction, which is undo the step, al = [1,2,3].\nswap(1, 2) → al = [1,3,2]\nhelper([1,3,2], 3)\nAgain, index == length, add it to the list. result = [[1,2,3], [1,3,2]]. Recursion stack collapses, we undo the swap, al = [1,2,3]\nswap(0, 1) → al = [2,1,3]\nhelper([2,1,3], 1)\nswap(1,1) → al = [2,1,3]\nhelper([2,1,3], 2)\nswap(2, 2) → al = [2,1,3]\nhelper([2,1,3], 3)\nindex == length, add the current order to the list. result = [[1,2,3], [1,3,2], [2,1,3]]\nswap(1, 2) → al = [2,3,1]\nhelper([2,3,1], 3)\nindex == length, add the order to the list. Result = [[1,2,3], [1,3,2], [2,1,3], [2,3,1]]\nswap(0, 2) → al = [3,2,1]\nhelper([3,2,1], 2)\nswap(2,2) → al = [3,2,1]\nhelper([3,2,1], 3)\nindex == length, add the order to the list. Result = [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,2,1]]\nswap(1,2) → al = [3,1,2]\nhelper([3,1,2], 3)\nindex == length, add the order to the list. Result = [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,2,1], [3,1,2]]\nAll branches have been explored now, since the iteration ends and we return the result list.\nint len;\t// To store the length of the input array List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; result;\t// Result list public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { result = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; numList = new ArrayList\u0026lt;\u0026gt;();\t// Creating a copy of the nums array for (int i: nums)\t// because it\u0026#39;s easier to create a list from a list. numList.add(i);\t// Add everything to the list. len = nums.length; helper(numList, 0);\t// Call the aux function. return result; } private void helper(List\u0026lt;Integer\u0026gt; order, int index){ if (index == len)\t// If we have checked all the numbers in the array, add a result.add(new ArrayList\u0026lt;\u0026gt;(order));\t// clone of the list to the array. for (int i = index; i \u0026lt; len; ++i){\t// Otherwise from index to the end of the array, swap(order, i, index);\t// take one element, swap it with itself, then the next and helper(order, index+1);\t// so on. Recurse again, but on the next index we just swapped. swap(order, i, index);\t// Undo the swap so that it helps us in generating the next }\t// permutation. } private void swap(List\u0026lt;Integer\u0026gt; list, int i, int j){\t// Swap elements in a list. int temp = list.get(i); list.set(i, list.get(j)); list.set(j, temp); } Construct the Rectangle The idea is very simple. We just need to iterate from width = sqrt(area) to 1 and check if area is perfectly divisible by width. If at any point, width is divisible, then that must be our minimum difference length and width, because we are diverging from the center on both sides. Width decreases while length keeps increasing. Think of it like this, for area = 24, we have many factors of 24, namely 1, 2, 3,4, 6, 8, 12, 24. It\u0026rsquo;s sqrt when rounded down is 4. So we check for width = 4, is 24 perfectly divisible by 4? Yes, so divide it and whatever you get is going to be the minimal difference values. Suppose 4 and 6 weren\u0026rsquo;t the factors for 24. In that case we decrease width by 1, which is 3. Check again, is 24 divisible by 3. Yes? Then that must be our answer. We are diverging away from the center on both sides equally, width to the left towards 1 and length to the right towards area . Therefore the moment we find one value that divides area perfectly, that\u0026rsquo;s our required values.\npublic int[] constructRectangle(int area) { int[] dimensions = {area, 1};\t// We know that if nothing works out, n*1 is always boolean done = false;\t// going to be the answer int width = (int)Math.sqrt(area);\t// We only need to check width from sqrt(area) while (!done){\t// While not done if (area % length == 0){\t// check if area is perfectly divisible by width dimensions[0] = width;\t// if so, we found our width and the length. dimensions[1] = area/width; done = true;\t// mark done as false } --width;\t// otherwise decrease the length } return dimensions;\t// return the dimensions found. Merge Intervals public List\u0026lt;Interval\u0026gt; merge(List\u0026lt;Interval\u0026gt; intervals) { if (intervals == null || intervals.size() \u0026lt; 2) return intervals; Collections.sort(intervals, (a,b) -\u0026gt; a.start-b.start);\t// Sort the list so we can // compare adjacent intervals. List\u0026lt;Interval\u0026gt; merged = new ArrayList\u0026lt;\u0026gt;(); merged.add(intervals.get(0));\t// Add the initial interval. for (Interval i: intervals){\t// For each interval Interval last = merged.get(merged.size()-1);// Get the last added time. if (i.start \u0026gt; last.end)\t// If it\u0026#39;s time is greater than the last merged.add(i);\t// interval\u0026#39;s end, it doesn\u0026#39;t overlap else{\t// otherwise it does. last.end = last.end \u0026gt; i.end ? last.end : i.end;\t// So check which has greater end time, and make the last added interval\u0026#39;s time equals that merged.set(merged.size()-1, last);\t// And set it as the last added interval } } return merged;\t// Return the merged list. } Merged sorted lists counterA keeps track of which element we are looking at in array \u0026lsquo;a\u0026rsquo;. Same with counterB counterK keeps track of where to insert the element in array \u0026lsquo;a\u0026rsquo;, since a has enough space. The problem states that it might have more than enough space, so we use only the spaces we need, which is the total of both their sizes. Since indexing in an array is 0-based, we subtract 1. We insert elements from the end, since the end part of \u0026lsquo;a\u0026rsquo; is empty. We can insert from the front, but then we would need to shift elements to the right after each insertion from \u0026lsquo;b\u0026rsquo;. If array values are equal, add them to the end, and decrease both their counter to check new values in the next iteration If not equal, then check which one is greater, since the last part of the array should contain larger values. Whichever\u0026rsquo;s greater, put it in \u0026lsquo;a\u0026rsquo; at index \u0026lsquo;counterA\u0026rsquo; and decrement the respective counter. In the end, we might have some leftover elements either from \u0026lsquo;a\u0026rsquo; or \u0026lsquo;b\u0026rsquo; because we only process elements that are equal to the min(size(a), size(b)), until we run out of elements in one of the array. So, whichever array has elements pending, add it to the front of the array and return a. public int[] merge(int[] a, int sizeA, int[] b, int sizeB) { int counterA = sizeA-1, counterB = sizeB-1, counterK = sizeA+sizeB-1; while (counterA \u0026gt; -1 \u0026amp;\u0026amp; counterB \u0026gt; -1){ if (a[counterA] == b[counterB]){ a[counterK--] = a[counterA--]; a[counterK--] = b[counterB--]; } else a[counterK--] = a[counterA] \u0026gt; b[counterB] ? a[counterA--] : b[counterB--]; } while (counterA \u0026gt; -1) a[counterK--] = a[counterA--]; while (counterB \u0026gt; -1) a[counterK--] = b[counterB--]; return a; } Next Greater Element I public int[] nextGreaterElement(int[] nums1, int[] nums2) { HashMap\u0026lt;Integer, Integer\u0026gt; index = new HashMap\u0026lt;\u0026gt;();\t// We use the hashmap to keep a for (int i = 0; i \u0026lt; nums2.length; ++i)\t// track of the index of each value index.put(nums2[i], i);\t// in nums 2. That way, when we want // to look for a value greater than a val in nums1, we know int[] result = new int[nums1.length];\t// which index to start iterating from. for (int i = 0; i \u0026lt; nums1.length; ++i){\t// So for each val in nums1 int val = nums1[i]; for (int j = index.get(val); j \u0026lt; nums2.length; ++j){\t// Iterate from that value\u0026#39;s if (nums2[j] \u0026gt; val){\t// index in nums2 to the end, and see if you can result[i] = nums2[j];\t// find any val \u0026gt; nums1[i]. If you do, save it break;\t// in the result array and break the loop. } } if (result[i] == 0)\t// Now if we didn\u0026#39;t find any value, then result[i] would be result[i] = -1;\t// 0, so we set that index to -1 in our result array. } return result;\t// simply return the result array. } String Without AAA or BBB public String strWithout3a3b(int A, int B) {\tchar[] ch = new char[A+B];\t// We create an char array to store string chars int index = 0; char max = A \u0026gt; B ? \u0026#39;a\u0026#39; : \u0026#39;b\u0026#39;;\t// record the most frequent occurring element char min = max == \u0026#39;a\u0026#39; ? \u0026#39;b\u0026#39; : \u0026#39;a\u0026#39;;\t// and the least frequent occurring element while (A \u0026gt; 0 || B \u0026gt; 0){\t// While we haven\u0026#39;t added all of the elements // We check that if our current index \u0026gt; 1 and our previoud two characters in the array // are the same, then we must have written the max occurring char, so it\u0026#39;s time to write // the minimum occurring element. We write it, and then decrement the specific A or B. if (index \u0026gt; 1 \u0026amp;\u0026amp; max == ch[index-1] \u0026amp;\u0026amp; max == ch[index-2]){ ch[index++] = min; if (min == \u0026#39;a\u0026#39;)\t// If the minimum freq element is \u0026#39;a\u0026#39;, decrement A A--; else B--;\t// otherwise decrement B } else if (B \u0026gt; A){\t// Otherwise, if B occurs more than A, then set char to B ch[index++] = \u0026#39;b\u0026#39;;\t// decrement B and increment index B--; } else{\t// A occurs more, so add A to the char array. ch[index++] = \u0026#39;a\u0026#39;;\t// Increment index, decrement A count A--; } } return new String(ch);\t// Create a string from the char array and return it. } Keyboard Row // Maps each character to the row in the keyboard in which it occurs. private int[] map = {2,3,3,2,1,2,2,2,1,2,2,2,3,3,1,1,1,1,2,1,1,3,1,3,1,3}; public String[] findWords(String[] words) { String[] w = new String[words.length];\t// Store filtered words int index = 0;\t// Where to insert the filtered words for (String s: words)\t// for each word in words if (checkWord(s.toLowerCase()))\t// convert it to lowercase and check if all char w[index++] = s;\t// occurs in the same row, if it does, add it return Arrays.copyOfRange(w, 0, index);\t// Simply return a copy of the array from 0 }\t// index private boolean checkWord(String word){\t// Check if all chars in the word belong in the int row = map[word.charAt(0)-\u0026#39;a\u0026#39;];\t// same row. Check first chars row for (char c: word.toCharArray()){\t// For all the chars in the word if (map[c-\u0026#39;a\u0026#39;] != row)\t// if that char belongs to a different row, return false;\t// return false } return true;\t// All chars in same row, return true. } Find Mode in Binary Search Tree private TreeNode parent;\t// Keep track of parent at each node private int maxMode;\t// maxMode we found private int currentMode;\t// mode recorded at each node private Set\u0026lt;Integer\u0026gt; modes;\t// keep distinct modes found public int[] findMode(TreeNode root){ if (root == null)\t// node is null, so return empty array return new int[0]; maxMode = 1;\t// we have just seen the root, so maxMode so far is 1. currentMode = 1;\t// so is the current mode modes = new HashSet\u0026lt;\u0026gt;(); modes.add(root.val);\t// add the root to our modes set traverse(root);\t// start traversing it\u0026#39;s left and right branches int[] result = new int[modes.size()];\t// We have found all the modes int idx = 0;\t// keep track of where to insert elements in result array for (int i: modes)\t// add all the distinct modes one by one result[idx++] = i; return result;\t// and return it. } private void traverse(TreeNode node){ if (node == null)\t// if node is null, stop return;\t// otherwise traverse the left branch traverse(node.left);\t// Once we hit the null, we start backtracking to the leaf updateMode(node);\t// then we call updateMode with the node parent = node;\t// once it\u0026#39;s done, we update parent as the current node, so traverse(node.right);\t// when we backtrack, we can easily check that node and it\u0026#39;s }\t// next node\u0026#39;s value for similarity. Then traverse right. private void updateMode(TreeNode node){ if (parent != null \u0026amp;\u0026amp; parent.val == node.val){\t// If parent node isn\u0026#39;t null and the ++currentMode;\t// node\u0026#39;s value is the same as parent, we update currentMode if (currentMode \u0026gt;= maxMode){\t// If the currentMode is greater or equal to maxMode if (currentMode \u0026gt; maxMode)\t// just check if it\u0026#39;s greater. If it is, remove all modes.clear();\t// previously recorded modes modes.add(node.val);\t// Add the current node to the set and update the maxMode = currentMode;\t// maxMode } } else{\t// otherwise, value\u0026#39;s aren\u0026#39;t the same. so our currentMode currentMode = 1;\t// becomes 1. If maxMode is also 1, then all we have been if (maxMode == 1)\t// seeing are distinct values, so add that node\u0026#39;s value to modes.add(node.val);// to the mode\u0026#39;s set. } } Base 7 Solution 1 without StringBuilder (Beats 100%, 7ms)\npublic String convertToBase7(int num) { if (num == 0) return \u0026#34;0\u0026#34;; int len = (int)(Math.log(Math.abs(num))/Math.log(7))+1;\t// Calculate # of bits int idx;\t// where to start inserting from char[] digits; if (num \u0026lt; 0) {\t// If num is negative num = -num;\t// Make it positive digits = new char[len+1];\t// We need one more space for -ve sign in the front digits[0] = \u0026#39;-\u0026#39;;\t// Put the -ve sign idx = len;\t// and index is now len } else{ digits = new char[len];\t// otherwise we only need \u0026#34;len\u0026#34; spaces idx = len-1;\t// index is len-1 } while (num != 0) {\t// While num != 0, calculate remainder and add it. digits[idx--] = (char)(num % 7 + \u0026#39;0\u0026#39;);\t// Divide number by 7 num /= 7; } return new String(digits);\t// Just create a string and return it. } Solution 2 with StringBuilder\npublic String convertToBase7(int num) { StringBuilder sb = new StringBuilder(); boolean isNegative = num \u0026lt; 0;\t// Just so we can know if we need to add the \u0026#34;-\u0026#34; sign if (num \u0026lt; 0)\t// Take the absolute value of num num = -num; while (num \u0026gt; 6) {\t// Keep adding the remainder, and dividing num by 7. sb.append(num % 7); num /= 7; } sb.append(num);\t// Add whatever is left at the end. if (isNegative)\t// If num was negative, add the minus sign. sb.append(\u0026#39;-\u0026#39;); return sb.reverse().toString();\t// Reverse the builder and return the toString() } Relative Ranks The idea employed here is simple. We need to store the relative ranks in sorted order. We can sort the array for that, but that is O(n log n). We can do better than that by finding the relative rank in linear time. First we find the maximum score in the array and create another array of length = maxScore + 1. We add 1 so that when we see the maxScore in the nums, we can assign it to maxScore index. Once we have done that, now we iterate over the nums array. Variable i keeps track of what rank to assign. We check a value in the array and at that index in our reverse sorted array, we put i+1, which basically marks it\u0026rsquo;s rank based on it\u0026rsquo;s position in the rankings. Some of then indexes would be default, that is a score of 0. We then check each value in the descend array and if it\u0026rsquo;s not 0, we assign it a rank, but not if the ranks are 1, 2 or 3. In that case, we assign it a special value of Gold, SIlver or Bronze.\npublic String[] findRelativeRanks(int[] nums) { int maxScore = nums[0]; for (int n: nums) if (n \u0026gt; maxScore) maxScore = n; int[] descend = new int[maxScore+1]; for (int i = 0; i \u0026lt; nums.length; ++i) descend[nums[i]] = i+1; String[] result = new String[nums.length]; int rank = 1; for (int i = descend.length-1; i \u0026gt; -1; --i){ int idx = descend[i]; if (descend[i] != 0){ if (rank == 1) result[idx-1] = \u0026#34;Gold Medal\u0026#34;; else if (rank == 2) result[idx-1] = \u0026#34;Silver Medal\u0026#34;; else if (rank == 3) result[idx-1] = \u0026#34;Bronze Medal\u0026#34;; else result[idx-1] = rank + \u0026#34;\u0026#34;; ++rank; } } return result; } Perfect Number public boolean checkPerfectNumber(int num) { if (num == 1)\t// 1 is a special case, where it\u0026#39;s only factor is itself. return false; int total = 1;\t// We know our total will atleast be 1, 1 is everyone\u0026#39;s factor for (int i = 2; i \u0026lt;= Math.sqrt(num); ++i)\t// Only loop through num\u0026#39;s sqrt if (num % i == 0){\t// If i divides num perfectly int otherFactor = num/i;\t// Calculate the other factor total += i + (otherFactor == i ? 0 : otherFactor);\t// If i and other factor are }\t// different, add them both, otherwise just i. return total == num;\t// Check in the end if your total is the same as num } Detect Capital public boolean detectCapitalUse(String word) { int len = word.length(); if (len \u0026lt; 2)\t// Empty or size 1 words are ok. return true; char[] chars = word.toCharArray();\t// Get the char array boolean isUpper = false;\t// by default we let isUpper to false if (chars[0] \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; chars[0] \u0026lt;= \u0026#39;Z\u0026#39;)\t// Check if first two letters are uppercase isUpper = chars[1] \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; chars[1] \u0026lt;= \u0026#39;Z\u0026#39;; // If first was upper and second wasnt for (int i = 1; i \u0026lt; len; ++i){\t// isUpper = false, otherwise true. boolean isAlsoUpper = chars[i] \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; chars[i] \u0026lt;= \u0026#39;Z\u0026#39;; // We check onwards 1 char if (isUpper \u0026amp;\u0026amp; !isAlsoUpper)\t// If that char is lower and previous part was return false;\t// not lower, invalid use. if (!isUpper \u0026amp;\u0026amp; isAlsoUpper)\t// Or if previous part was lower and current letter return false;\t// is upper, we return false. } return true;\t// Everything proceeded smoothly. So return true. } Longest Uncommon Subsequence I This is those kind of problems that shouldn\u0026rsquo;t be up there. The problem is stated rather poorly and the solution is even stupider. All you are checking for is if the two string\u0026rsquo;s aren\u0026rsquo;t the same, then whichever one has a larger length is essentially the longest uncommon subsequence because the other string cannot form the full string. I know, it\u0026rsquo;s stupid.\npublic int findLUSlength(String a, String b) { if (a.equals(b)) return -1; return a.length() \u0026gt; b.length() ? a.length() : b.length(); } Course Schedule II This is a graph problem where we require to sort the vertices topologically. There are two choices we have for sorting topologically - Depth First Search approach based on finshing times or the Kahn\u0026rsquo;s Algorithm. I have used Kahn\u0026rsquo;s algorithm in this solution. Runtime is 2ms [beats 100%] and uses 45.3 MB space [beats than 90.16%]. The idea for Kahn\u0026rsquo;s is simple - Enqueue all the nodes which has 0 incoming edges because those are the ones that can be started first. Then while the queue isn\u0026rsquo;t empty, remove one node at a time, process it\u0026rsquo;s outgoing nodes and decrease their indegrees by one. The reasoning behind that is let\u0026rsquo;s say Node 2 has two prerequisites, Node 0 and Node 1. Node 0 and Node 1 have 0 indegrees. So our first two nodes would be Node 1 and Node 0 and if they are finished, then their outgoing Nodes can be started, that is Node 2. Now when you decrease any node\u0026rsquo;s indegree and they become zero, add them to the queue because they can now be started. Keep doing this until the queue is empty.\nIn my approach, I\u0026rsquo;m avoiding any unnecessary data structure and using only the most basic ones like array\u0026rsquo;s. So instead of using the queue, what I do is fill the array order which also stores the topological order. idx keeps track of the last index available to fill in the array. start mimics the poll behaviour of a queue. while (start != idx) makes sure that while we still have nodes to process, remove the one that can be started and decrease all the indegrees of outgoing edges.\npublic int[] findOrder(int numCourses, int[][] prerequisites) { int[] indegrees = new int[numCourses];\t// We maintain each node\u0026#39;s indegree List\u0026lt;Integer\u0026gt;[] graph = new ArrayList[numCourses];\t// Each node\u0026#39;s outgoing edges for (int[] edge: prerequisites) {\t// Process each edge indegrees[edge[0]]++;\t// Update indegrees if (graph[edge[1]] == null)\t// Also store the edge in graph graph[edge[1]] = new ArrayList\u0026lt;Integer\u0026gt;(); graph[edge[1]].add(edge[0]); } int[] order = new int[numCourses];\t// We don\u0026#39;t technically need a queue. int idx = 0; for (int i = 0; i \u0026lt; numCourses; ++i) // Find all nodes who indegree is 0 if (indegrees[i] == 0) // and put them in the order array order[idx++] = i; int start = 0; // start tracks node to be polled. while (start != idx) { // while we can poll the queue int u = order[start++]; // poll the node u if (graph[u] != null) // If node u has outgoing edges for (int out: graph[u]) // Then for each of those nodes if (--indegrees[out] == 0) // decrease their indegrees and check if it\u0026#39;s 0 order[idx++] = out; // if it\u0026#39;s 0, add it to our queue (order) } if (idx != numCourses)\t// Cycle check. If our idx != numCourses then return new int[] {};\t// not all nodes could be processed. So we have return order;\t// a cycle. Otherwise return our order array. } Letter Combinations of a Phone Number Runtime: 0 ms, faster than 100.00% of Java online submissions for Letter Combinations of a Phone Number.\nMemory Usage: 35.9 MB, less than 98.63% of Java online submissions for Letter Combinations of a Phone Number.\nHow do we count numbers? 16, 17, 18, 19 and then what? 20 right? We see that the last number is 19, we can\u0026rsquo;t go past 9 so we set it to 0 and then increment the precedding digit to get 20. The idea is the same for this problem too. We keep a levels array to keep track of which character do we take from which number\u0026rsquo;s allowed alphabet letters. For example, let\u0026rsquo;s say the input string is 23. Our levels array would [0, 0] in the beginning. This says pick characters at index 0 and 0 from alphabet characters corresponding to 2 and 3 which gives us ad. Then, we increase the last most counter in our levels array by 1 giving us [0, 1]. This allows us to get ae in the next iteration and levels array would be [0, 2]. We get af and levels array becomes [0, 3]. Now this is where it becomes interesting. We are only allowed three letters for the digit corresponding to 3 and since we already used all of them , we now need to shift to the next character for digit 2, which is b. Level array looks like [1, 0]. This will allow us to get [b,e]. So you get the rough idea now. Only thing now is we watch out when to stop. We stop when we have utilized all available characters from the 0th index\u0026rsquo;s number\u0026rsquo;s allowed alphabet letters. In this case, we stop when levels array look like [3, 0].\nclass Solution { private char[][] map = { {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;}, // 2 {\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;}, // 3 {\u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;}, // 4 {\u0026#39;j\u0026#39;, \u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;}, // 5 {\u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;o\u0026#39;}, // 6 {\u0026#39;p\u0026#39;, \u0026#39;q\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;s\u0026#39;}, // 7 {\u0026#39;t\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;v\u0026#39;}, // 8 {\u0026#39;w\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;} // 9 }; private List\u0026lt;String\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); // Maintain the list of combinations private int[] numbers; // numbers parsed from input private int[] levels; // utility array to keep track of next character in string private int n; // number of input digits. private List\u0026lt;String\u0026gt; solution(String digits) { if (digits == null || digits.length() == 0) // stop if null or empty string return result; n = digits.length(); numbers = new int[n]; levels = new int[n]; for (int i = 0; i \u0026lt; digits.length(); ++i) { // parse all the digits from the string as int if ((numbers[i] = digits.charAt(i) - \u0026#39;0\u0026#39;) \u0026lt; 2) // stop if any of them is 0 or 1 return result; } helper(); // start recursion return result; } private void helper() { if (levels[0] == map[numbers[0]-2].length) // if we are done iterating over all possible combinations, return; // stop recursion. char[] s = new char[n]; // stores all the characters of the string for (int i = 0; i \u0026lt; n; ++i) // loop through levels array. The value at each index s[i] = map[numbers[i]-2][levels[i]]; // tells us which character to keep from which map index levels[n-1]++; // Increase the entry at the end of the levels array for (int i = levels.length-1; i \u0026gt; 0; --i) { // Now loop through the levels array from the end if (levels[i] == map[numbers[i]-2].length) { // If the value = total number of characters allowed for that number levels[i] = 0; // then we set it to 0 and increment the previous level entry levels[i - 1]++; } } result.add(new String(s)); // Add the string and induce next recursive call. helper(); } } Sudoku Solver Runtime: 4 ms, faster than 90.01% of Java online submissions for Sudoku Solver.\nMemory Usage: 35.1 MB, less than 71.93% of Java online submissions for Sudoku Solver.\nprivate char[][] board; public void solveSudoku(char[][] board) { this.board = board; solve(0, 0); } private boolean solve(int row, int col) { if (col == 9) { // If col is 9, make it 0 and shift to the next row col = 0; row += 1; if (row == 9) // If row is also 9 now, then it means we have successfully filled all cells return true; // So return true and end backtracking. } for (int i = 1; i \u0026lt; 10; ++i) { // Otherwise, we start picking values from 1-9 if (board[row][col] == \u0026#39;.\u0026#39;) { // And try to plug it into empty cells if (isValid(row, col, i)) { // If that value is valid in that cell board[row][col] = (char)(i+\u0026#39;0\u0026#39;); // fill it if (solve(row, col+1)) // and move on to fill the next cell via recursive call return true; // If the recursion ended by returning true, then return true to signal success else // Otherwise, we were not able to put an value in that cell board[row][col] = \u0026#39;.\u0026#39;; // so change it back to 0 and the backtracking would try the next higher value in that cell. } } else return solve(row, col+1); // That cell wasn\u0026#39;t empty, so move on to the next empty cell. } return false; // No solution found. } private boolean isValid(int row, int col, int val) { // row check for (int c = 0; c \u0026lt; 9; ++c) if (board[row][c] - \u0026#39;0\u0026#39; == val) return false; // column check for (int r = 0; r \u0026lt; 9; ++r) if (board[r][col] - \u0026#39;0\u0026#39; == val) return false; // box check int top = row / 3 * 3; int left = col / 3 * 3; for (int i = 0; i \u0026lt; 3; ++i) { for (int j = 0; j \u0026lt; 3; ++j) { if (board[top+i][left+j] - \u0026#39;0\u0026#39; == val) return false; } } return true; } Bulls and Cows Runtime: 1 ms, faster than 100.00% of Java online submissions for Bulls and Cows.\nMemory Usage: 36.3 MB, less than 100.00% of Java online submissions for Bulls and Cows.\nThe idea is simple, first record the frequency of the digits of the secret number. Then we first find number of bulls by checking for exact indices match. After that we start to record the number of cows. The way we do is by again iterating over the guess string; only if there was a character mismatch and we still have the character available from freq table, we have a cow. Update it and decrement the frequency of the number we just used up.\npublic String getHint(String secret, String guess) { int bulls = 0; int cows = 0; int[] freq = new int[10];\t// Freq of available digits from secret for (int i = 0; i \u0026lt; guess.length(); ++i) { char s = secret.charAt(i); freq[s - \u0026#39;0\u0026#39;]++;\t// Record the freq of the digit if (s == guess.charAt(i)) {\t// If it\u0026#39;s a match, we have a bulls. bulls++; freq[s - \u0026#39;0\u0026#39;]--;\t// We just used the character, so decrement it. } } for (int i = 0; i \u0026lt; guess.length(); ++i) { int s = secret.charAt(i) - \u0026#39;0\u0026#39;;\t// Convert the chars into int int g = guess.charAt(i) - \u0026#39;0\u0026#39;; if (s != g \u0026amp;\u0026amp; freq[g] \u0026gt; 0) {\t// Only if they are a mismtach and we have a number g available in freq table cows++;\t// then it\u0026#39;s a cow. freq[g]--;\t// We used up the number, so decrement it\u0026#39;s freq. } } return new StringBuilder().append(bulls).append(\u0026#34;A\u0026#34;).append(cows).append(\u0026#34;B\u0026#34;).toString(); } N-Queens I Runtime: 3 ms, faster than 73.76% of Java online submissions for N-Queens. Memory Usage: 37.6 MB, less than 100.00% of Java online submissions for N-Queens.\nThe idea is same as sudoku, but insteading of scanning rows, we scan columns. Start with row 0, column 0 and see if we can place a queen there, if yes place it and try the next cell of row 0 by recursing. We can\u0026rsquo;t put the queen in the same row again, so we keep changing rows with column 1 until we find somewhere to place it. Keep doing this until you were successfully able to place all the queens as checked by the condition col == n. If so, add that solution to our list of accepted solutions.\npublic class NQueens { private int[][] board; private int n; private List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; result; public List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; solveNQueens(int n) { this.n = n; board = new int[n][n]; result = new ArrayList\u0026lt;\u0026gt;(); solve(0); return result; } private boolean solve(int col) { if (col == n) addToList(); for (int row = 0; row \u0026lt; n; ++row) { if (canPlaceQueen(row, col)) { board[row][col] = 1; if (solve(col+1)) { return true; } else board[row][col] = 0; } } return false; } private void addToList() { List\u0026lt;String\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); StringBuilder sb; for (int[] r: board) { sb = new StringBuilder(); for (int i: r) sb.append(i == 1 ? \u0026#39;Q\u0026#39; : \u0026#39;.\u0026#39;); list.add(sb.toString()); } result.add(list); } private boolean canPlaceQueen(int row, int col) { // Check all rows for the same column for (int i = 0; i \u0026lt; col; ++i) { if (board[row][i] == 1) return false; } // Check upper left diagonal of the cell for (int i = row, j = col; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0; i--, j--) { if (board[i][j] == 1) return false; } // Check lower left diagonal of the cell. for (int i = row, j = col; i \u0026lt; n \u0026amp;\u0026amp; j \u0026gt;= 0; i++, j--) { if (board[i][j] == 1) return false; } return true; } } K-diff pairs in an Array Pretty intuitive solution. Build a frequency HashMap for all the numbers in the array. In a special case where diff is 0, just count occurences in our freq map whose values are 2 or more. In other case, just loop through all the keys and make sure it\u0026rsquo;s supplement exists to count the number of K-diff pairs.\npublic int findPairs(int[] nums, int k) { if (k \u0026lt; 0) return 0; int pairs = 0; HashMap\u0026lt;Integer, Integer\u0026gt; freq = new HashMap\u0026lt;\u0026gt;(); for (int i: nums) { freq.put(i, freq.getOrDefault(i, 0)+1); } if (k == 0) { for (int i: freq.values()) if (i \u0026gt; 1) pairs++; return pairs; } for (int i: freq.keySet()) { if (freq.containsKey(i+k)) pairs++; } return pairs; } Is Subsequence Runtime: 0 ms, faster than 100.00% of Java online submissions for Is Subsequence. Memory Usage: 49.6 MB, less than 100.00% of Java online submissions for Is Subsequence.\npublic boolean isSubsequence(String s, String t) { int idx = -1;\t// Set it to 0 to start check for 0th index for (char c: s.toCharArray()) {\t// For all the characters in String s idx = t.indexOf(c, idx+1);\t// Find it\u0026#39;s index in String t from index one more than the last index matched if (idx \u0026lt; 0)\t// idx \u0026lt; 0 means not found return false; } return true; } Minimum Absolute Difference in BST The idea is to use the Inorder traversal of a BST. We repeatively iterate over the left branch to find the minimum diff and then do the same for the right branch, but this time we already know that the parent of the right branch has to be its minimum, so first set it and then traverse the right branch to find the minimum difference.\nint res = Integer.MAX_VALUE;\t// Hold the minimum difference. int prev = Integer.MAX_VALUE;\t// Holds the minimum value observed for the right branch public int getMinimumDifference(TreeNode root) { traverse(root);\t// Start iterating from the root. return res; } private void traverse(TreeNode node) { if (node == null)\t// Null node, so stop recursion return; traverse(node.left);\t// Keep traversing till the end of the tree res = Math.min(Math.abs(node.val-prev), res);\t// Check if we have a minimum, if so set it. prev = node.val;\t// The smallest value for the right branch is it\u0026#39;s parent traverse(node.right);\t// Set it first and then traverse. } BST Tree to Greater Tree The idea is simple. In a BST, we know everything on the right side of a node is greater than it and it\u0026rsquo;s left side. So when we are at any node, it\u0026rsquo;s value would be its value + sum of everything on its right side. So, we first compute the node\u0026rsquo;s value and then notice that the value for the node on the left is nothing but its value + parents value. So the node\u0026rsquo;s value is computed, do the same thing for the left side, but this time, the starting sum would be the parent\u0026rsquo;s value.\npublic TreeNode convertBST(TreeNode root) { traverse(root, 0); return root; } private int traverse(TreeNode node, int sum) { if (node == null) return sum; node.val += traverse(node.right, sum); return traverse(node.left, node.val); } Student Attendance Record I public boolean checkRecord(String s) { int A = 0;\t// Count number of A\u0026#39;s seen int L = 0;\t// Count number of consecutive L\u0026#39;s seen for (char c: s.toCharArray()) {\t// Loop through each character if (c == \u0026#39;A\u0026#39;) {\t// If c is A, increment A A++; if (A \u0026gt; 1)\t// If A is more than 1, return false return false L = 0;\t// Always set L count to 0 } else if (c == \u0026#39;L\u0026#39;) {\t// If c is L, L++;\t// We might have consecutive L\u0026#39;s, so start counting if (L \u0026gt; 2) {\t// If we have more than 2 consecutive L\u0026#39;s return false;\t// return false } } else\t// Lastly, we might have a P, that will reset our L = 0;\t// consecutive L streak. } return true;\t// Everything passed, so return true. } Reverse Words in String III Runtime: 2 ms, faster than 99.34% of Java online submissions for Reverse Words in a String III.\nMemory Usage: 37.9 MB, less than 100.00% of Java online submissions for Reverse Words in a String III.\npublic String reverseWords(String s) { char[] arr = s.toCharArray(); int len = arr.length; int start = 0; int end; while (start \u0026lt; len) {\t// Check the whole string end = start;\t// find the index of the first whitespace while(end \u0026lt; len \u0026amp;\u0026amp; arr[end] != \u0026#39; \u0026#39;)\t// denoting end of the word end++; reverseWord(arr, start, end-1);\t// reverse that specific word start = end+1;\t// update start to the new word beginning } return new String(arr);\t// create a new string out of the array } /* Reverses a word in-place by iterating n/2 times where n = len of the word. Traverse upto the middle point of the word while swapping each word from start+offset to end-\toffset. **/ private void reverseWord(char[] arr, int start, int stop) { for (int i = 0; i \u0026lt;= (stop-start)/2; ++i) { char temp = arr[start+i]; arr[start+i] = arr[stop-i]; arr[stop-i] = temp; } } Quad Tree Intersection public Node intersect(Node qt1, Node qt2) { if (qt1.isLeaf)\t// If only a leaf, then return the one with true val return qt1.val ? qt1 : qt2; if (qt2.isLeaf) return qt2.val ? qt2 : qt1; Node n = new Node();\t// Prepare for recursion n.val = true;\t// By default, each level node is not a leaf with n.isLeaf = false;\t// value = true // Keep traversing all the way to a terminal node and then store it. n.topLeft = intersect(qt1.topLeft, qt2.topLeft); n.topRight = intersect(qt1.topRight, qt2.topRight); n.bottomLeft = intersect(qt1.bottomLeft, qt2.bottomLeft); n.bottomRight = intersect(qt1.bottomRight, qt2.bottomRight); // Check now if you\u0026#39;re at the base case. If n\u0026#39;s children are leaves and all their values are same, then make n a leaf and it\u0026#39;s value the same as it\u0026#39;s child. if (n.topLeft.isLeaf \u0026amp;\u0026amp; n.topRight.isLeaf \u0026amp;\u0026amp; n.bottomLeft.isLeaf \u0026amp;\u0026amp; n.bottomRight.isLeaf \u0026amp;\u0026amp; (n.topLeft.val == n.topRight.val \u0026amp;\u0026amp; n.topRight.val == n.bottomLeft.val \u0026amp;\u0026amp; n.bottomLeft.val == n.bottomRight.val)) { n.isLeaf = true; n.val = n.topLeft.val; } return n; } Long Pressed Name Runtime: 0 ms, faster than 100.00% of Java online submissions for Long Pressed Name.\nMemory Usage: 34.2 MB, less than 100.00% of Java online submissions for Long Pressed Name.\npublic boolean isLongPressedName(String name, String typed) { char[] n = name.toCharArray();\t// Arrays are much nicer to work with char[] t = typed.toCharArray();\t// Record start and stop points for both int startN = 0, endN = n.length, startT = 0, endT = t.length; while (startT \u0026lt; endT) {\t// While we haven\u0026#39;t looked at the whole string int temp = startN+1;\t// Let\u0026#39;s first count same consecutive letters int countN = 1;\t// in String name while (temp \u0026lt; endN \u0026amp;\u0026amp; n[startN] == n[temp]) { temp++; countN++; } int countT = 0;\t// Do the same for typed string while (startT \u0026lt; endT \u0026amp;\u0026amp; n[startN] == t[startT]) { startT++; countT++; }\t// If consecutive letters in typed string are if (countT \u0026lt; countN)\t// less than the ones in original name return false;\t// return false startN = temp;\t// Otherwise, prepare for next character } return startN == endN;\t// Lastly, check if we were able to match }\t// all character of the name string Binary Tree Zigzag Level Order Traversal The idea here is simple. We perform a BFS as usual using a Queue but I maintain a variable called dir to check which side do I add from. dir=1 means add from Right-\u0026gt;Left and dir=-1 means add from usual Left-\u0026gt;Right. I am also using LinkedList because of easy addition of elements in both direction. When I need to add from Right-\u0026gt;Left, I use the addFirst(E e) method of LinkedList to add to the head, otherwise normal add to the tail. One important thing to take care of at each iteration is to know how many nodes to dequeue, hence the usage of the variable children. This allows me to keep track of how many children were added to the queue at each stage so I dequeue exactly that many children in the next stage. Apart from that, everything is straightforward.\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; zigzagLevelOrder(TreeNode root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); if (root == null) return list; Queue\u0026lt;TreeNode\u0026gt; q = new LinkedList\u0026lt;\u0026gt;(); q.add(root);\t// Children = 1 because only root is added. int dir = 1, children = 1;\t// Added the root, so next time dir = 1 (Right-\u0026gt;Left) while(!q.isEmpty()) { int pushed = 0; LinkedList\u0026lt;Integer\u0026gt; l = new LinkedList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; children; ++i) {\t// Poll only those nodes that were queued in TreeNode u = q.poll();\t// the previous stage. if (dir == 1) l.add(u.val); else l.addFirst(u.val);\t// Left-\u0026gt;Right add if (u.left != null) {\t// Add children, notice I am counting here q.add(u.left);\t// how many children I am pushing/queuing ++pushed;\t// to the queue } if (u.right != null) {\t// Same thing for right child. q.add(u.right); ++pushed; } } list.add(l);\t// Add this list to main list children = pushed;\t// update # of children pushed dir = dir == 1 ? -1: 1;\t// update dir for next iteration } return list; } Array Partition I Runtime: 3 ms, faster than 99.90% of Java online submissions for Array Partition I.\nMemory Usage: 40.1 MB, less than 100.00% of Java online submissions for Array Partition I.\nI originally came up with the sorting solution where you sort the array and look at two numbers at a time and keep the smaller number out of them and add to the sum. It was way slower, so I checked the fastest submission and this one is pretty smart. The idea is really good. We know there are going to be 20,001 numbers, so reserve an array for it. Now let\u0026rsquo;s say we had duplicates in our array, ex [1,2,1,4,1,1], if we were to sort it, we would get [1,1,1,1,2,4]. Notice that those four 1\u0026rsquo;s don\u0026rsquo;t really matter because each of them pairs up with the other to give you a one 1. That is why we mark those particular indices as true and false. Notice that in our variable sum we would have counted them individually, making sum = 4 when in fact it should be 2 since we only take one of them from two pairs. If we have even occurrence of any number, they would be false, meaning we don\u0026rsquo;t need to account them in the diff calculation. Now coming to diff how do we compute it? First we have the seen array to know which elements we need to look at. If that particular index is true, then we check if it\u0026rsquo;s the first element of the pair which we maintain via the boolean value firstElemOfPair. If its true, then first becomes that value. Otherwise, we know that we\u0026rsquo;re looking at the second element so we update the diff which is basically that value subtract first. Notice that if we look at a pair in our example as (2,4), we would pick 2 and the diff would be 2. This needs to be subtracted from our sum, hence the reason to maintain both of them. At the end, we finally subtract sum and diff and divide the result by 2 because we were doubling our diff\u0026rsquo;s too.\npublic int arrayPairSum(int[] nums) { boolean[] seen = new boolean[20001]; int sum = 0; for (int n: nums) { seen[n + 10000] = !seen[n+10000]; sum += n; } int diff = 0; int first = 0; boolean firstElemOfPair = true; for (int i = 0; i \u0026lt; seen.length; ++i) { if (seen[i]) { if (firstElemOfPair) first = i; else diff += i-first; firstElemOfPair = !firstElemOfPair; } } return (sum-diff)/2; } Reshape the Matrix Runtime: 1 ms, faster than 100.00% of Java online submissions for Reshape the Matrix.\nMemory Usage: 38.4 MB, less than 100.00% of Java online submissions for Reshape the Matrix.\npublic int[][] matrixReshape(int[][] nums, int r, int c) { int numsR = nums.length;\t// Get rows and col of nums int numsC = nums[0].length; if (numsR * numsC != r*c || (numsR == r \u0026amp;\u0026amp; numsC == c))\t// If can\u0026#39;t reshape or problems return nums;\t// asks to reshape in the same dimensions, return the same array int[][] mat = new int[r][c];\t// New matrix to be returned int row = 0, col = 0, nR = 0, nC = 0; // To keep track of which element to consume and where to place it in the new matrix while (row != r) { mat[row][col++] = nums[nR][nC++];\t// Increment only the column value for both if (col == c) {\t// Check if we are at boundary, if so, increment row col = 0;\t// and set col to 0 for both cases. ++row; } if (nC == numsC) { nC = 0; ++nR; } } return mat; } Swap Nodes in Pairs Runtime: 0 ms, faster than 100.00% of Java online submissions for Swap Nodes in Pairs.\nMemory Usage: 34.5 MB, less than 100.00% of Java online submissions for Swap Nodes in Pairs.\nThe idea is simple. We add a dummy node in front for simplicity as it allows us to generalize the concept of getting two nodes at a time. We maintain a current pointer that points to the node in the actual LinkedList. Then, we get it\u0026rsquo;s next and it\u0026rsquo;s next.next and store it into n1 and n2. Now notice that before making n2\u0026rsquo;s next = n1, we need to store n2\u0026rsquo;s next into n1\u0026rsquo;s next. After we do that, we need to make sure that current\u0026rsquo;s next is n2 which is now working with the actual LinkedList. Then, we need to make sure that current.next.next is n1 which we just fixed and update current which is basically n1.\npublic ListNode swapPairs(ListNode head) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode curr = dummy; while (curr.next != null \u0026amp;\u0026amp; curr.next.next != null) { ListNode n1 = curr.next; ListNode n2 = n1.next; n1.next = n2.next; curr.next = n2; curr.next.next = n1; curr = curr.next.next; } return dummy.next; } Generate Parentheses Iterative Approach 1: This one is very slow.\nRuntime: 4 ms, faster than 8.87% of Java online submissions for Generate Parentheses.\nMemory Usage: 36.1 MB, less than 100.00% of Java online submissions for Generate Parentheses.\nThe idea is simple. We basically do a BFS and keep track of the parentheses combination we have obtained so far. Poll the queue and check if it\u0026rsquo;s length is 2*n (for a given n, we would have # of open brackets = # of closed brackets), add it to the list and check next combination. If not, then check if we can add an open bracket, add it and update number of open bracket count and add this combination to the queue. Then try to see if we can add a closed bracket, if you can add it, then update closed bracket count add that combination to the queue. Keep doing this until the queue becomes empty. This is the first approach I came up with which is naive as you can see since it\u0026rsquo;s doing an exhaustive search for all valid combination.\nprivate class Node { private String data; private int open; private int close; Node(String s, int o, int c) { data = s; open = o; close = c; } } public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; list = new LinkedList\u0026lt;\u0026gt;(); Queue\u0026lt;Node\u0026gt; q = new LinkedList\u0026lt;\u0026gt;(); q.add(new Node(\u0026#34;(\u0026#34;, 1, 0)); while (!q.isEmpty()) { Node u = q.poll(); if (u.data.length() == 2*n) list.add(u.data); else { Node n1 = new Node(u.data, u.open, u.close); Node n2 = new Node(u.data, u.open, u.close); if (n1.open \u0026lt; n) { n1.data = u.data + \u0026#39;(\u0026#39;; ++n1.open; q.add(n1); } if (n2.close \u0026lt; u.open) { n2.data = u.data + \u0026#39;)\u0026#39;; ++n2.close; q.add(n2); } } } return list; } Recursive Solution 2: This one is much more faster. I generalized the above idea into the fact that I am adding only valid combinations and any invalid combinations are automatically discarded. The logic is as follows: We know for a given n, the string length should be 2*n. So that forms our base case for recursion, if the length of String s is 2n, we want to add it to the list. Otherwise, we check if the number of open brackets we have so far is less than n. If so, we can add an open bracket. Then check if number of close bracket is less than open, if so that sequence would be valid and add a close bracket and recurse.\nRuntime: 1 ms, faster than 95.16% of Java online submissions for Generate Parentheses. Memory Usage: 36.1 MB, less than 100.00% of Java online submissions for Generate Parentheses.\npublic List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); helper(list, \u0026#34;(\u0026#34;, 1, 0, n); return list; } private void helper(List\u0026lt;String\u0026gt; list, String s, int open, int close, int n) { if (s.length() == 2*n) list.add(s); else { if (open \u0026lt; n) helper(list, s+\u0026#39;(\u0026#39;, open+1, close, n); if (close \u0026lt; open) helper(list, s+\u0026#39;)\u0026#39;, open, close+1, n); } } ###Distribute Candies\nPretty simple solution. We want to give maximize the number of unique candies to give to the sister. So we maintain a hashset to collect all the unique candies first. Both of them get half the candies, so let s = number of candies they get. Now, if the size of the set is greater than or equal to s, then the sister only gets s candies out of it. Otherwise, the maximum amount of unique candies she can get is equal to the set size.\npublic int distributeCandies(int[] candies) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(candies.length); for (int i: candies) set.add(i); int share = candies.length/2; return set.size() \u0026gt;= share ? share: set.size(); } Maximum subproduct subarray Credits for this simplistic solution to LeetCode user mzchen. The approach is very clever. Notice that if this problem was about finding maximum sum subarray, then a negative number would break the contiguous array. Here, what it does is that it makes our maximum product minimum when we see a negative number and vice versa. We keep track of maximum and minimum we have so far and check if we have a negative number. If so swap our max and min. Then, find the local maximum and minimum between current number and multiplying that number with our current max or min. After that, update our global max value and keep doing this for all values in the array.\npublic int maxProduct(int[] nums) { int max = nums[0]; for (int i = 1, imax = max, imin = max; i \u0026lt; nums.length; ++i) { if (nums[i] \u0026lt; 0) { int temp = imax; imax = imin; imin = temp; } imax = Math.max(nums[i], imax * nums[i]); imin = Math.min(nums[i], imin * nums[i]); max = Math.max(max, imax); } return max; } Binary Tree Right Side View Runtime: 1 ms, faster than 95.45% of Java online submissions for Binary Tree Right Side View.\nMemory Usage: 36.3 MB, less than 100.00% of Java online submissions for Binary Tree Right Side View.\nThis is an interesting problem cause initially, I thought we would always have a complete binary tree and I made my initial solution oriented towards it. But then I saw that it doesn\u0026rsquo;t say that anywhere and it could be any kind of binary tree. So it got me thinking towards a more generalized approach. Notice that to get a right side view of the binary tree, we only need the last value at any given level and put it into the list. So we maintain a queue and also the number of elements we enqueue at each stage. Initially, we put the root node in our queue and our enqueue count is 1. We dequeue exactly that many elements and again enqueue each of those dequeued node\u0026rsquo;s children. Notice that I am using the variable newEnqueued to keep track of newly enqueued elements. Lastly, we need to check if we dequeued the last element. If so, that must be a part of the solution since it has to be the rightmost element at that level, so I add it to the list. Update enqueued to the new value and repeat until our queue isn\u0026rsquo;t empty.\npublic List\u0026lt;Integer\u0026gt; rightSideView(TreeNode root) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); if (root == null) return list; Queue\u0026lt;TreeNode\u0026gt; q = new LinkedList\u0026lt;\u0026gt;(); q.add(root); int enqueued = 1; while (!q.isEmpty()) { int newEnqueued = 0; for (int i = 0; i \u0026lt; enqueued; ++i) { TreeNode u = q.poll(); if (u.left != null) { q.add(u.left); ++newEnqueued; } if (u.right != null) { q.add(u.right); ++newEnqueued; } if (i == enqueued-1) list.add(u.val); } enqueued = newEnqueued; } return list; } Find Minimum in Rotated Sorted Array Runtime: 0 ms, faster than 100.00% of Java online submissions for Find Minimum in Rotated Sorted Array.\nMemory Usage: 38.6 MB, less than 77.27% of Java online submissions for Find Minimum in Rotated Sorted Array.\npublic int findMin(int[] nums) { if (nums.length == 1)\t// Base case. return nums[0]; int left = 0; int right = nums.length-1; int mid; while (nums[left] \u0026gt; nums[right]) {\t// While we are in the ascending order half, mid = (left + right)/2;\t// Find the middle element if (nums[mid] \u0026gt;= nums[left])\t// If mid element \u0026gt;= left element, then our min left = mid + 1;\t// must be in the right half. else right = mid;\t// otherwise min in the left half. } return nums[left];\t// left points to minimum element. } Binary Search Tree Iterator Runtime: 15 ms, faster than 99.74% of Java online submissions for Binary Search Tree Iterator.\nMemory Usage: 49.9 MB, less than 93.83% of Java online submissions for Binary Search Tree Iterator.\nLogic is same as your In-Order traversal of any Binary Tree, but store the node values you visit in any data structure. Here I am using an ArrayList for storing each of the visited node\u0026rsquo;s value. Maintain idx value to keep track of which value to return. hasNext() method returns true as long as idx \u0026lt; list.size().\nclass BSTIterator { private List\u0026lt;Integer\u0026gt; list; private int idx = 0; public BSTIterator(TreeNode root) { list = new ArrayList\u0026lt;\u0026gt;(); traverse(root); } private void traverse(TreeNode node) { if (node == null) return; traverse(node.left); list.add(node.val); traverse(node.right); } /** @return the next smallest number */ public int next() { return list.get(idx++); } /** @return whether we have a next smallest number */ public boolean hasNext() { return idx != list.size(); } } Find Peak Element This question was asked to me for my internship at Yahoo! The idea is simple, we want any one of the peak. So to achieve O(log n) time, we have to mimic binary search algorithm. We look at the middle element and check it\u0026rsquo;s neighbor, if it\u0026rsquo;s greater than the middle element, then we know we will have atleast one peak on the right side. Why? Think what could happen. We know that the element next to middle is greater than it, so there are two possibilities on the right side, either elements keep increasing to the right of the middle\u0026rsquo;s next element or we might go up till a particular index and then go down. So in any case, we will have a peak on the right side. On the other case, if the element on the right side is smaller than the middle, then we know that the left half including the middle will have the peak cause middle is already greater than middle\u0026rsquo;s right, so we might have middle as the peak itself.\npublic int findPeakElement(int[] nums) { if (nums.length == 1) return 0; int low = nums[0], high = nums.length - 1, mid; while (low \u0026lt; high) { mid = (low + high)/2; if (nums[mid] \u0026lt; nums[mid+1]) low = mid+1; else high = mid; } return low; } Next Permutation Runtime: 0 ms, faster than 100.00% of Java online submissions for Next Permutation.\nMemory Usage: 40.3 MB, less than 47.00% of Java online submissions for Next Permutation.\nThis one was quite interesting in the sense it seems difficult but is very simple once you try out a few example. If we want to find the next lexicographical greater number, then we need to find a particular index from the right side of the array such that the number after it is greater than itself, because by swapping them would give us a next larger number. So what I first do is find the index of the number such that num[idx] \u0026gt; num[idx-1]. We know at this point that all the numbers after that index are reverse sorted, so we need to fix it and sort them in increasing order because lexicographical order demands all the numbers in increasing manner. Example, say nums = [2,3,1,4,2,1,0]. You can see that that the next number should be [2,3,2,0,1,1,4]. Notice that I replaced the number at index 2 with the first number which is greater than it if the array after index 2 was sorted. This gaurantees us a larger lexicographical number. So the first while loop finds us that index number and then I reverse the array after it. Once you reverse it, we should expect the nums array to look like [2,3,1,0,1,2,4]. Note that now we need to find the number larger than the number at index 2, which is 1 in this case. The first number greater than 1 is 2, so the second while loop finds it and then we simply swap them to give us the next larger lexicographically greater number =\u0026gt; [2,3,2,0,1,1,4].\npublic void nextPermutation(int[] nums) { if (nums.length \u0026lt; 2) return; int idx = nums.length-1; while (idx \u0026gt; 0 \u0026amp;\u0026amp; nums[idx] \u0026lt;= nums[idx-1]) --idx; reverse(nums, idx); if (idx == 0) return; int val = nums[idx-1]; int i = idx; while (i \u0026lt; nums.length \u0026amp;\u0026amp; nums[i] \u0026lt;= val) ++i; swap(nums, i, idx-1); } private void swap(int[] arr, int idx1, int idx2) { int temp = arr[idx1]; arr[idx1] = arr[idx2]; arr[idx2] = temp; } private void reverse(int[] arr, int start) { int end = arr.length-1; while (start \u0026lt; end) swap(arr, start++, end--); } Search in Rotated Sorted Array The idea is same as binary search except you need to keep track of which half to stay in. We compute the middle index and the value at that index. If the middle value is the target, then return that index. Otherwise, find the correct half. If the number on the left side is \u0026lt; middle value then we know that between the left and middle index, values are increasing. We only need to now check if target is \u0026lt; middle value, if so we need to adjust our right pointer otherwise adjust the left pointer. If left value is not \u0026lt; middle value then we are at a shift where the array is pivoted. We again need to confirm now which half to take. There would be some index i such that nums[left] \u0026gt; nums[i] \u0026lt; nums[mid] and value are increasing upto i and shifts from index i onwards. In this case, we again need to adjust our index pointers and we repeat this loop until left \u0026lt;= right\npublic int search(int[] nums, int target) { int left = 0, right = nums.length-1; while (left \u0026lt;= right) { int mid = (left + right)/2; int midVal = nums[mid]; if (target == midVal) return mid; else if (nums[left] \u0026lt;= midVal) { if (target \u0026lt; midVal \u0026amp;\u0026amp; target \u0026gt;= nums[left]) right = mid - 1; else left = mid + 1; } else { if (target \u0026gt; midVal \u0026amp;\u0026amp; target \u0026lt;= nums[right]) left = mid+1; else right = mid - 1; } } return -1; } Transpose Matrix Pretty straightforward. Create matrix B of opposite dimensions to those of A. We maintain br and bc which tracks row and columns of B. We iterate over each element of A and put it in B[br][bc] and then ideally we would increment bc for an exact copy, but since we want transpose, we increment br and then reset it to 0 if we fill all the values in a row and increment column count, giving us the tranpose of the matrix.\npublic int[][] transpose(int[][] A) { int[][] B = new int[A[0].length][A.length]; int br = 0, bc = 0; for (int i = 0; i \u0026lt; A.length; ++i) { for (int j = 0; j \u0026lt; A[0].length; ++j) { B[br][bc] = A[i][j]; if (++br == B.length) { br = 0; ++bc; } } } return B; } Merge K Sorted Lists This was an onsite interview question at ThousandEyes. The idea is simple. Basically, we have multiple sorted lists so we have access to one value at a time, that is head of the lists initially and the consecutive nodes. So we need to fetch the minimum element out of all of them in constant time. The easiest way for us to do this is to use a PriorityQueue and define the logic of comparision of two ListNodes. Then, we add all the nodes inside the PQ and build our resulting List. Fetch the minimum valued ListNode and add it to our list. Then we also need to update that particular list\u0026rsquo;s head, so we add that list\u0026rsquo;s next in the PQ so the next time it is fetched, we fetch the correct node of the list. Repeat this until the list is empty and return dummy\u0026rsquo;s next node.\npublic ListNode mergeKLists(ListNode[] lists) { if (lists.length == 0) return null; PriorityQueue\u0026lt;ListNode\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;(lists.length, (n1, n2) -\u0026gt; n1.val - n2.val); for (ListNode ln: lists) if (ln != null) pq.add(ln); if (pq.isEmpty()) return null; ListNode node = new ListNode(-1); ListNode ret = node; while (!pq.isEmpty()) { node.next = pq.poll(); node = node.next; if (node.next != null) pq.add(node.next); } return ret.next; } ","permalink":"https://samirpaul1.github.io/blog/posts/leetcode-solutions-cheatsheet/","summary":"Leetcode Solutions Cheatsheet","title":"Leetcode Solutions Cheatsheet"},{"content":" A curated list of awesome Python frameworks, libraries, software and resources.\nAwesome Python Admin Panels Algorithms and Design Patterns ASGI Servers Asynchronous Programming Audio Authentication Build Tools Built-in Classes Enhancement Caching ChatOps Tools CMS Code Analysis Command-line Interface Development Command-line Tools Compatibility Computer Vision Concurrency and Parallelism Configuration Cryptography Data Analysis Data Validation Data Visualization Database Drivers Database Date and Time Debugging Tools Deep Learning DevOps Tools Distributed Computing Distribution Documentation Downloader E-commerce Editor Plugins and IDEs Email Enterprise Application Integrations Environment Management Files Foreign Function Interface Forms Functional Programming Game Development Geolocation GUI Development Hardware HTML Manipulation HTTP Clients Image Processing Implementations Interactive Interpreter Internationalization Job Scheduler Logging Machine Learning Miscellaneous Natural Language Processing Network Virtualization News Feed ORM Package Management Package Repositories Penetration testing Permissions Processes Recommender Systems Refactoring RESTful API Robotics RPC Servers Science Search Serialization Serverless Frameworks Shell Specific Formats Processing Static Site Generator Tagging Task Queues Template Engine Testing Text Processing Third-party APIs URL Manipulation Video Web Asset Management Web Content Extracting Web Crawling Web Frameworks WebSocket WSGI Servers Resources Books Newsletters Podcasts Websites Contributing Admin Panels Libraries for administrative interfaces.\najenti - The admin panel your servers deserve. django-grappelli - A jazzy skin for the Django Admin-Interface. django-jet - Modern responsive template for the Django admin interface with improved functionality. django-suit - Alternative Django Admin-Interface (free only for Non-commercial use). django-xadmin - Drop-in replacement of Django admin comes with lots of goodies. flask-admin - Simple and extensible administrative interface framework for Flask. flower - Real-time monitor and web admin for Celery. jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django). wooey - A Django app which creates automatic web UIs for Python scripts. Algorithms and Design Patterns Python implementation of data structures, algorithms and design patterns. Also see awesome-algorithms.\nAlgorithms algorithms - Minimal examples of data structures and algorithms. python-ds - A collection of data structure and algorithms for coding interviews. sortedcontainers - Fast and pure-Python implementation of sorted collections. TheAlgorithms - All Algorithms implemented in Python. Design Patterns PyPattyrn - A simple yet effective library for implementing common design patterns. python-patterns - A collection of design patterns in Python. transitions - A lightweight, object-oriented finite state machine implementation. ASGI Servers ASGI-compatible web servers.\ndaphne - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP. uvicorn - A lightning-fast ASGI server implementation, using uvloop and httptools. Asynchronous Programming asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks. awesome-asyncio trio - A friendly library for async concurrency and I/O. Twisted - An event-driven networking engine. uvloop - Ultra fast asyncio event loop. Audio Libraries for manipulating audio and its metadata.\nAudio audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding. dejavu - Audio fingerprinting and recognition. kapre - Keras Audio Preprocessors. librosa - Python library for audio and music analysis. matchering - A library for automated reference audio mastering. mingus - An advanced music theory and notation package with MIDI file and playback support. pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications. pydub - Manipulate audio with a simple and easy high level interface. TimeSide - Open web audio processing framework. Metadata beets - A music library manager and MusicBrainz tagger. eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata. mutagen - A Python module to handle audio metadata. tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files. Authentication Libraries for implementing authentications schemes.\nOAuth authlib - JavaScript Object Signing and Encryption draft implementation. django-allauth - Authentication app for Django that \u0026ldquo;just works.\u0026rdquo; django-oauth-toolkit - OAuth 2 goodies for Django. oauthlib - A generic and thorough implementation of the OAuth request-signing logic. python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers. python-social-auth - An easy-to-setup social authentication mechanism. JWT pyjwt - JSON Web Token implementation in Python. python-jose - A JOSE implementation in Python. python-jwt - A module for generating and verifying JSON Web Tokens. Build Tools Compile software from source code.\nBitBake - A make-like build tool for embedded Linux. buildout - A build system for creating, assembling and deploying applications from multiple parts. PlatformIO - A console tool to build code with different development platforms. pybuilder - A continuous build tool written in pure Python. SCons - A software construction tool. Built-in Classes Enhancement Libraries for enhancing Python built-in classes.\nattrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions. bidict - Efficient, Pythonic bidirectional map data structures and related functionality.. Box - Python dictionaries with advanced dot notation access. dataclasses - (Python standard library) Data classes. DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation. CMS Content Management Systems.\ndjango-cms - An Open source enterprise CMS based on the Django. feincms - One of the most advanced Content Management Systems built on Django. indico - A feature-rich event management system, made @ CERN. Kotti - A high-level, Pythonic web application framework built on Pyramid. mezzanine - A powerful, consistent, and flexible content management platform. plone - A CMS built on top of the open source application server Zope. quokka - Flexible, extensible, small CMS powered by Flask and MongoDB. wagtail - A Django content management system. Caching Libraries for caching data.\nbeaker - A WSGI middleware for sessions and caching. django-cache-machine - Automatic caching and invalidation for Django models. django-cacheops - A slick ORM cache with automatic granular event-driven invalidation. dogpile.cache - dogpile.cache is a next generation replacement for Beaker made by the same authors. HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention. pylibmc - A Python wrapper around the libmemcached interface. python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis. ChatOps Tools Libraries for chatbot development.\nerrbot - The easiest and most popular chatbot to implement ChatOps. Code Analysis Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis.\nCode Analysis coala - Language independent and easily extendable code analysis application. code2flow - Turn your Python and JavaScript code into DOT flowcharts. prospector - A tool to analyse Python code. pycallgraph - A library that visualises the flow (call graph) of your Python application. vulture - A tool for finding and analysing dead Python code. Code Linters flake8 - A wrapper around pycodestyle, pyflakes and McCabe. awesome-flake8-extensions pylama - A code audit tool for Python and JavaScript. pylint - A fully customizable source code analyzer. wemake-python-styleguide - The strictest and most opinionated python linter ever. Code Formatters black - The uncompromising Python code formatter. isort - A Python utility / library to sort imports. yapf - Yet another Python code formatter from Google. Static Type Checkers, also see awesome-python-typing mypy - Check variable types during compile time. pyre-check - Performant type checking. typeshed - Collection of library stubs for Python, with static types. Static Type Annotations Generators MonkeyType - A system for Python that generates static type annotations by collecting runtime types. pyannotate - Auto-generate PEP-484 annotations. pytype - Pytype checks and infers types for Python code - without requiring type annotations. Command-line Interface Development Libraries for building command-line applications.\nCommand-line Application Development cement - CLI Application Framework for Python. click - A package for creating beautiful command line interfaces in a composable way. cliff - A framework for creating command-line programs with multi-level commands. docopt - Pythonic command line arguments parser. python-fire - A library for creating command line interfaces from absolutely any Python object. python-prompt-toolkit - A library for building powerful interactive command lines. Terminal Rendering alive-progress - A new kind of Progress Bar, with real-time throughput, eta and very cool animations. asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations). bashplotlib - Making basic plots in the terminal. colorama - Cross-platform colored terminal text. rich - Python library for rich text and beautiful formatting in the terminal. Also provides a great RichHandler log handler. tqdm - Fast, extensible progress bar for loops and CLI. Command-line Tools Useful CLI-based tools for productivity.\nProductivity Tools copier - A library and command-line utility for rendering projects templates. cookiecutter - A command-line utility that creates projects from cookiecutters (project templates). doitlive - A tool for live presentations in the terminal. howdoi - Instant coding answers via the command line. Invoke - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. PathPicker - Select files out of bash output. percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX. thefuck - Correcting your previous console command. tmuxp - A tmux session manager. try - A dead simple CLI to try out python packages - it\u0026rsquo;s never been easier. CLI Enhancements httpie - A command line HTTP client, a user-friendly cURL replacement. iredis - Redis CLI with autocompletion and syntax highlighting. kube-shell - An integrated shell for working with the Kubernetes CLI. litecli - SQLite CLI with autocompletion and syntax highlighting. mycli - MySQL CLI with autocompletion and syntax highlighting. pgcli - PostgreSQL CLI with autocompletion and syntax highlighting. saws - A Supercharged aws-cli. Compatibility Libraries for migrating from Python 2 to 3.\npython-future - The missing compatibility layer between Python 2 and Python 3. modernize - Modernizes Python code for eventual Python 3 migration. six - Python 2 and 3 compatibility utilities. Computer Vision Libraries for Computer Vision.\nEasyOCR - Ready-to-use OCR with 40+ languages supported. Face Recognition - Simple facial recognition library. Kornia - Open Source Differentiable Computer Vision Library for PyTorch. OpenCV - Open Source Computer Vision Library. pytesseract - A wrapper for Google Tesseract OCR. SimpleCV - An open source framework for building computer vision applications. tesserocr - Another simple, Pillow-friendly, wrapper around the tesseract-ocr API for OCR. Concurrency and Parallelism Libraries for concurrent and parallel execution. Also see awesome-asyncio.\nconcurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables. eventlet - Asynchronous framework with WSGI support. gevent - A coroutine-based Python networking library that uses greenlet. multiprocessing - (Python standard library) Process-based parallelism. scoop - Scalable Concurrent Operations in Python. uvloop - Ultra fast implementation of asyncio event loop on top of libuv. Configuration Libraries for storing and parsing configuration options.\nconfigobj - INI file parser with validation. configparser - (Python standard library) INI file parser. hydra - Hydra is a framework for elegantly configuring complex applications. profig - Config from multiple formats with value conversion. python-decouple - Strict separation of settings from code. Cryptography cryptography - A package designed to expose cryptographic primitives and recipes to Python developers. paramiko - The leading native Python SSHv2 protocol library. passlib - Secure password storage/hashing library, very high level. pynacl - Python binding to the Networking and Cryptography (NaCl) library. Data Analysis Libraries for data analyzing.\nAWS Data Wrangler - Pandas on AWS. Blaze - NumPy and Pandas interface to Big Data. Open Mining - Business Intelligence (BI) in Pandas interface. Optimus - Agile Data Science Workflows made easy with PySpark. Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts. Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools. Data Validation Libraries for validating data. Used for forms in many cases.\nCerberus - A lightweight and extensible data validation library. colander - Validating and deserializing data obtained via XML, JSON, an HTML form post. jsonschema - An implementation of JSON Schema for Python. schema - A library for validating Python data structures. Schematics - Data Structure Validation. valideer - Lightweight extensible data validation and adaptation library. voluptuous - A Python data validation library. Data Visualization Libraries for visualizing data. Also see awesome-javascript.\nAltair - Declarative statistical visualization library for Python. Bokeh - Interactive Web Plotting for Python. bqplot - Interactive Plotting Library for the Jupyter Notebook. Cartopy - A cartographic python library with matplotlib support. Dash - Built on top of Flask, React and Plotly aimed at analytical web applications. awesome-dash diagrams - Diagram as Code. Matplotlib - A Python 2D plotting library. plotnine - A grammar of graphics for Python based on ggplot2. Pygal - A Python SVG Charts Creator. PyGraphviz - Python interface to Graphviz. PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets. Seaborn - Statistical data visualization using Matplotlib. VisPy - High-performance scientific visualization based on OpenGL. Database Databases implemented in Python.\npickleDB - A simple and lightweight key-value store for Python. tinydb - A tiny, document-oriented database. ZODB - A native object database for Python. A key-value and object graph database. Database Drivers Libraries for connecting and operating databases.\nMySQL - awesome-mysql mysqlclient - MySQL connector with Python 3 support (mysql-python fork). PyMySQL - A pure Python MySQL driver compatible to mysql-python. PostgreSQL - awesome-postgres psycopg2 - The most popular PostgreSQL adapter for Python. queries - A wrapper of the psycopg2 library for interacting with PostgreSQL. SQlite - awesome-sqlite sqlite3 - (Python standard library) SQlite interface compliant with DB-API 2.0 SuperSQLite - A supercharged SQLite library built on top of apsw. Other Relational Databases pymssql - A simple database interface to Microsoft SQL Server. clickhouse-driver - Python driver with native interface for ClickHouse. NoSQL Databases cassandra-driver - The Python Driver for Apache Cassandra. happybase - A developer-friendly library for Apache HBase. kafka-python - The Python client for Apache Kafka. py2neo - A client library and toolkit for working with Neo4j. pymongo - The official Python client for MongoDB. redis-py - The Python client for Redis. Asynchronous Clients motor - The async Python driver for MongoDB. Date and Time Libraries for working with dates and times.\nArrow - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. Chronyk - A Python 3 library for parsing human-written times and dates. dateutil - Extensions to the standard Python datetime module. delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes. maya - Datetimes for Humans. moment - A Python library for dealing with dates/times. Inspired by Moment.js. Pendulum - Python datetimes made easy. PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string. pytz - World timezone definitions, modern and historical. Brings the tz database into Python. when.py - Providing user-friendly functions to help perform common date and time actions. Debugging Tools Libraries for debugging code.\npdb-like Debugger ipdb - IPython-enabled pdb. pdb++ - Another drop-in replacement for pdb. pudb - A full-screen, console-based Python debugger. wdb - An improbable web debugger through WebSockets. Tracing lptrace - strace for Python programs. manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt. pyringe - Debugger capable of attaching to and injecting code into Python processes. python-hunter - A flexible code tracing toolkit. Profiler line_profiler - Line-by-line profiling. memory_profiler - Monitor Memory usage of Python code. py-spy - A sampling profiler for Python programs. Written in Rust. pyflame - A ptracing profiler For Python. vprof - Visual Python profiler. Others django-debug-toolbar - Display various debug information for Django. django-devserver - A drop-in replacement for Django\u0026rsquo;s runserver. flask-debugtoolbar - A port of the django-debug-toolbar to flask. icecream - Inspect variables, expressions, and program execution with a single, simple function call. pyelftools - Parsing and analyzing ELF files and DWARF debugging information. Deep Learning Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning.\ncaffe - A fast open framework for deep learning.. keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano. mxnet - A deep learning framework designed for both efficiency and flexibility. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. SerpentAI - Game agent framework. Use any video game as a deep learning sandbox. tensorflow - The most popular Deep Learning framework created by Google. Theano - A library for fast numerical computation. DevOps Tools Software and libraries for DevOps.\nConfiguration Management ansible - A radically simple IT automation platform. cloudinit - A multi-distribution package that handles early initialization of a cloud instance. OpenStack - Open source software for building private and public clouds. pyinfra - A versatile CLI tools and python libraries to automate infrastructure. saltstack - Infrastructure automation and management system. SSH-style Deployment cuisine - Chef-like functionality for Fabric. fabric - A simple, Pythonic tool for remote execution and deployment. fabtools - Tools for writing awesome Fabric files. Process Management honcho - A Python clone of Foreman, for managing Procfile-based applications. supervisor - Supervisor process control system for UNIX. Monitoring psutil - A cross-platform process and system utilities module. Backup BorgBackup - A deduplicating archiver with compression and encryption. Others docker-compose - Fast, isolated development environments using Docker. Distributed Computing Frameworks and libraries for Distributed Computing.\nBatch Processing dask - A flexible parallel computing library for analytic computing. luigi - A module that helps you build complex pipelines of batch jobs. mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services. PySpark - Apache Spark Python API. Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem. Stream Processing faust - A stream processing library, porting the ideas from Kafka Streams to Python. streamparse - Run Python code against real-time streams of data via Apache Storm. Distribution Libraries to create packaged executables for release distribution.\ndh-virtualenv - Build and distribute a virtualenv as a Debian package. Nuitka - Compile scripts, modules, packages to an executable or extension module. py2app - Freezes Python scripts (Mac OS X). py2exe - Freezes Python scripts (Windows). pyarmor - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts. PyInstaller - Converts Python programs into stand-alone executables (cross-platform). pynsist - A tool to build Windows installers, installers bundle Python itself. shiv - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included. Documentation Libraries for generating project documentation.\nsphinx - Python Documentation generator. awesome-sphinxdoc pdoc - Epydoc replacement to auto generate API documentation for Python libraries. pycco - The literate-programming-style documentation generator. Downloader Libraries for downloading.\nakshare - A financial data interface library, built for human beings! s3cmd - A command line tool for managing Amazon S3 and CloudFront. s4cmd - Super S3 command line tool, good for higher performance. you-get - A YouTube/Youku/Niconico video downloader written in Python 3. youtube-dl - A small command-line program to download videos from YouTube. E-commerce Frameworks and libraries for e-commerce and payments.\nalipay - Unofficial Alipay API for Python. Cartridge - A shopping cart app built using the Mezzanine. django-oscar - An open-source e-commerce framework for Django. django-shop - A Django based shop system. forex-python - Foreign exchange rates, Bitcoin price index and currency conversion. merchant - A Django app to accept payments from various payment processors. money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange. python-currencies - Display money format and its filthy currencies. saleor - An e-commerce storefront for Django. shoop - An open source E-Commerce platform based on Django. Editor Plugins and IDEs Emacs elpy - Emacs Python Development Environment. Sublime Text anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE. SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi. Vim jedi-vim - Vim bindings for the Jedi auto-completion library for Python. python-mode - An all in one plugin for turning Vim into a Python IDE. YouCompleteMe - Includes Jedi-based completion engine for Python. Visual Studio PTVS - Python Tools for Visual Studio. Visual Studio Code Python - The official VSCode extension with rich support for Python. IDE PyCharm - Commercial Python IDE by JetBrains. Has free community edition available. spyder - Open Source Python IDE. Email Libraries for sending and parsing email.\nMail Servers modoboa - A mail hosting and management platform including a modern Web UI. salmon - A Python Mail Server. Clients imbox - Python IMAP for Humans. yagmail - Yet another Gmail/SMTP client. Others flanker - An email address and Mime parsing library. mailer - High-performance extensible mail delivery framework. Enterprise Application Integrations Platforms and tools for systems integrations in enterprise environments\nZato - ESB, SOA, REST, APIs and Cloud Integrations in Python. Environment Management Libraries for Python version and virtual environment management.\npyenv - Simple Python version management. virtualenv - A tool to create isolated Python environments. Files Libraries for file manipulation and MIME type detection.\nmimetypes - (Python standard library) Map filenames to MIME types. path.py - A module wrapper for os.path. pathlib - (Python standard library) An cross-platform, object-oriented path library. PyFilesystem2 - Python\u0026rsquo;s filesystem abstraction layer. python-magic - A Python interface to the libmagic file type identification library. Unipath - An object-oriented approach to file/directory operations. watchdog - API and shell utilities to monitor file system events. Foreign Function Interface Libraries for providing foreign function interface.\ncffi - Foreign Function Interface for Python calling C code. ctypes - (Python standard library) Foreign Function Interface for Python calling C code. PyCUDA - A Python wrapper for Nvidia\u0026rsquo;s CUDA API. SWIG - Simplified Wrapper and Interface Generator. Forms Libraries for working with forms.\nDeform - Python HTML form generation library influenced by the formish form generation library. django-bootstrap3 - Bootstrap 3 integration with Django. django-bootstrap4 - Bootstrap 4 integration with Django. django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way. django-remote-forms - A platform independent Django form serializer. WTForms - A flexible forms validation and rendering library. Functional Programming Functional Programming with Python.\nCoconut - A variant of Python built for simple, elegant, Pythonic functional programming. CyToolz - Cython implementation of Toolz: High performance functional utilities. fn.py - Functional programming in Python: implementation of missing features to enjoy FP. funcy - A fancy and practical functional tools. more-itertools - More routines for operating on iterables, beyond itertools. returns - A set of type-safe monads, transformers, and composition utilities. Toolz - A collection of functional utilities for iterators, functions, and dictionaries. GUI Development Libraries for working with graphical user interface applications.\ncurses - Built-in wrapper for ncurses used to create terminal GUI applications. Eel - A library for making simple Electron-like offline HTML/JS GUI apps. enaml - Creating beautiful user-interfaces with Declarative Syntax like QML. Flexx - Flexx is a pure Python toolkit for creating GUI\u0026rsquo;s, that uses web technology for its rendering. Gooey - Turn command line programs into a full GUI application with one line. kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS. pyglet - A cross-platform windowing and multimedia library for Python. PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3). PyQt - Python bindings for the Qt cross-platform application and UI framework. PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi. pywebview - A lightweight cross-platform native wrapper around a webview component. Tkinter - Tkinter is Python\u0026rsquo;s de-facto standard GUI package. Toga - A Python native, OS native GUI toolkit. urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc. wxPython - A blending of the wxWidgets C++ class library with the Python. DearPyGui - A Simple GPU accelerated Python GUI framework GraphQL Libraries for working with GraphQL.\ngraphene - GraphQL framework for Python. tartiflette-aiohttp - An aiohttp-based wrapper for Tartiflette to expose GraphQL APIs over HTTP. tartiflette-asgi - ASGI support for the Tartiflette GraphQL engine. tartiflette - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio. Game Development Awesome game development libraries.\nArcade - Arcade is a modern Python framework for crafting games with compelling graphics and sound. Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. Harfang3D - Python framework for 3D, VR and game development. Panda3D - 3D game engine developed by Disney. Pygame - Pygame is a set of Python modules designed for writing games. PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D. PyOpenGL - Python ctypes bindings for OpenGL and it\u0026rsquo;s related APIs. PySDL2 - A ctypes based wrapper for the SDL2 library. RenPy - A Visual Novel engine. Geolocation Libraries for geocoding addresses and working with latitudes and longitudes.\ndjango-countries - A Django app that provides a country field for models and forms. GeoDjango - A world-class geographic web framework. GeoIP - Python API for MaxMind GeoIP Legacy Database. geojson - Python bindings and utilities for GeoJSON. geopy - Python Geocoding Toolbox. HTML Manipulation Libraries for working with HTML and XML.\nBeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML. bleach - A whitelist-based HTML sanitization and text linkification library. cssutils - A CSS library for Python. html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments. lxml - A very fast, easy-to-use and versatile library for handling HTML and XML. MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python. pyquery - A jQuery-like library for parsing HTML. untangle - Converts XML documents to Python objects for easy access. WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF. xmldataset - Simple XML Parsing. xmltodict - Working with XML feel like you are working with JSON. HTTP Clients Libraries for working with HTTP.\ngrequests - requests + gevent for asynchronous HTTP requests. httplib2 - Comprehensive HTTP client library. httpx - A next generation HTTP client for Python. requests - HTTP Requests for Humans. treq - Python requests like API built on top of Twisted\u0026rsquo;s HTTP client. urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly. Hardware Libraries for programming with hardware.\nino - Command line toolkit for working with Arduino. keyboard - Hook and simulate global keyboard events on Windows and Linux. mouse - Hook and simulate global mouse events on Windows and Linux. Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc. PyUserInput - A module for cross-platform control of the mouse and keyboard. scapy - A brilliant packet manipulation library. Image Processing Libraries for manipulating images.\nhmap - Image histogram remapping. imgSeek - A project for searching a collection of images using visual similarity. nude.py - Nudity detection. pagan - Retro identicon (Avatar) generation based on input string and hash. pillow - Pillow is the friendly PIL fork. python-barcode - Create barcodes in Python with no extra dependencies. pygram - Instagram-like image filters. PyMatting - A library for alpha matting. python-qrcode - A pure Python QR Code generator. pywal - A tool that generates color schemes from images. pyvips - A fast image processing library with low memory needs. Quads - Computer art based on quadtrees. scikit-image - A Python library for (scientific) image processing. thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images. wand - Python bindings for MagickWand, C API for ImageMagick. Implementations Implementations of Python.\nCLPython - Implementation of the Python programming language written in Common Lisp. CPython - Default, most widely used implementation of the Python programming language written in C. Cython - Optimizing Static Compiler for Python. Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha). IronPython - Implementation of the Python programming language written in C#. Jython - Implementation of Python programming language written in Java for the JVM. MicroPython - A lean and efficient Python programming language implementation. Numba - Python JIT compiler to LLVM aimed at scientific Python. PeachPy - x86-64 assembler embedded in Python. Pyjion - A JIT for Python based upon CoreCLR. PyPy - A very fast and compliant implementation of the Python language. Pyston - A Python implementation using JIT techniques. Stackless Python - An enhanced version of the Python programming language. Interactive Interpreter Interactive Python interpreters (REPL).\nbpython - A fancy interface to the Python interpreter. Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively. awesome-jupyter ptpython - Advanced Python REPL built on top of the python-prompt-toolkit. Internationalization Libraries for working with i18n.\nBabel - An internationalization library for Python. PyICU - A wrapper of International Components for Unicode C++ library (ICU). Job Scheduler Libraries for scheduling jobs.\nAirflow - Airflow is a platform to programmatically author, schedule and monitor workflows. APScheduler - A light but powerful in-process task scheduler that lets you schedule functions. django-schedule - A calendaring app for Django. doit - A task runner and build tool. gunnery - Multipurpose task execution tool for distributed systems with web-based interface. Joblib - A set of tools to provide lightweight pipelining in Python. Plan - Writing crontab file in Python like a charm. Prefect - A modern workflow orchestration framework that makes it easy to build, schedule and monitor robust data pipelines. schedule - Python job scheduling for humans. Spiff - A powerful workflow engine implemented in pure Python. TaskFlow - A Python library that helps to make task execution easy, consistent and reliable. Logging Libraries for generating and working with logs.\nlogbook - Logging replacement for Python. logging - (Python standard library) Logging facility for Python. loguru - Library which aims to bring enjoyable logging in Python. sentry-python - Sentry SDK for Python. structlog - Structured logging made easy. Machine Learning Libraries for Machine Learning. Also see awesome-machine-learning.\ngym - A toolkit for developing and comparing reinforcement learning algorithms. H2O - Open Source Fast Scalable Machine Learning Platform. Metrics - Machine learning evaluation metrics. NuPIC - Numenta Platform for Intelligent Computing. scikit-learn - The most popular Python library for Machine Learning. Spark ML - Apache Spark\u0026rsquo;s scalable Machine Learning library. vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit. xgboost - A scalable, portable, and distributed gradient boosting library. MindsDB - MindsDB is an open source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using standard queries. Microsoft Windows Python programming on Microsoft Windows.\nPython(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder. pythonlibs - Unofficial Windows binaries for Python extension packages. PythonNet - Python Integration with the .NET Common Language Runtime (CLR). PyWin32 - Python Extensions for Windows. WinPython - Portable development environment for Windows 7/8. Miscellaneous Useful libraries or tools that don\u0026rsquo;t fit in the categories above.\nblinker - A fast Python in-process signal/event dispatching system. boltons - A set of pure-Python utilities. itsdangerous - Various helpers to pass trusted data to untrusted environments. magenta - A tool to generate music and art using artificial intelligence. pluginbase - A simple but flexible plugin system for Python. tryton - A general purpose business framework. Natural Language Processing Libraries for working with human languages.\nGeneral gensim - Topic Modeling for Humans. langid.py - Stand-alone language identification system. nltk - A leading platform for building Python programs to work with human language data. pattern - A web mining module. polyglot - Natural language pipeline supporting hundreds of languages. pytext - A natural language modeling framework based on PyTorch. PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research. spacy - A library for industrial-strength natural language processing in Python and Cython. Stanza - The Stanford NLP Group\u0026rsquo;s official Python library, supporting 60+ languages. Chinese funNLP - A collection of tools and datasets for Chinese NLP. jieba - The most popular Chinese text segmentation library. pkuseg-python - A toolkit for Chinese word segmentation in various domains. snownlp - A library for processing Chinese text. Network Virtualization Tools and libraries for Virtual Networking and SDN (Software Defined Networking).\nmininet - A popular network emulator and API written in Python. napalm - Cross-vendor API to manipulate network devices. pox - A Python-based SDN control applications, such as OpenFlow SDN controllers. News Feed Libraries for building user\u0026rsquo;s activities.\ndjango-activity-stream - Generating generic activity streams from the actions on your site. Stream Framework - Building news feed and notification systems using Cassandra and Redis. ORM Libraries that implement Object-Relational Mapping or data mapping techniques.\nRelational Databases Django Models - The Django ORM. SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper. awesome-sqlalchemy dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL. orator - The Orator ORM provides a simple yet beautiful ActiveRecord implementation. orm - An async ORM. peewee - A small, expressive ORM. pony - ORM that provides a generator-oriented interface to SQL. pydal - A pure Python Database Abstraction Layer. NoSQL Databases hot-redis - Rich Python data types for Redis. mongoengine - A Python Object-Document-Mapper for working with MongoDB. PynamoDB - A Pythonic interface for Amazon DynamoDB. redisco - A Python Library for Simple Models and Containers Persisted in Redis. Package Management Libraries for package and dependency management.\npip - The package installer for Python. pip-tools - A set of tools to keep your pinned Python dependencies fresh. PyPI conda - Cross-platform, Python-agnostic binary package manager. poetry - Python dependency management and packaging made easy. Package Repositories Local PyPI repository server and proxies.\nbandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA). devpi - PyPI server and packaging/testing/release tool. localshop - Local PyPI server (custom packages and auto-mirroring of pypi). warehouse - Next generation Python Package Repository (PyPI). Penetration Testing Frameworks and tools for penetration testing.\nfsociety - A Penetration testing framework. setoolkit - A toolkit for social engineering. sqlmap - Automatic SQL injection and database takeover tool. Permissions Libraries that allow or deny users access to data or functionality.\ndjango-guardian - Implementation of per object permissions for Django 1.2+ django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database. Processes Libraries for starting and communicating with OS processes.\ndelegator.py - Subprocesses for Humans 2.0. sarge - Yet another wrapper for subprocess. sh - A full-fledged subprocess replacement for Python. Recommender Systems Libraries for building recommender systems.\nannoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage. fastFM - A library for Factorization Machines. implicit - A fast Python implementation of collaborative filtering for implicit datasets. libffm - A library for Field-aware Factorization Machine (FFM). lightfm - A Python implementation of a number of popular recommendation algorithms. spotlight - Deep recommender models using PyTorch. Surprise - A scikit for building and analyzing recommender systems. tensorrec - A Recommendation Engine Framework in TensorFlow. Refactoring Refactoring tools and libraries for Python\nBicycle Repair Man - Bicycle Repair Man, a refactoring tool for Python. Bowler - Safe code refactoring for modern Python. Rope - Rope is a python refactoring library. RESTful API Libraries for building RESTful APIs.\nDjango django-rest-framework - A powerful and flexible toolkit to build web APIs. django-tastypie - Creating delicious APIs for Django apps. Flask eve - REST API framework powered by Flask, MongoDB and good intentions. flask-api - Browsable Web APIs for Flask. flask-restful - Quickly building REST APIs for Flask. Pyramid cornice - A RESTful framework for Pyramid. Framework agnostic apistar - A smart Web API framework, designed for Python 3. falcon - A high-performance framework for building cloud APIs and web app backends. fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints. hug - A Python 3 framework for cleanly exposing APIs. sandman2 - Automated REST APIs for existing database-driven systems. sanic - A Python 3.6+ web server and web framework that\u0026rsquo;s written to go fast. vibora - Fast, efficient and asynchronous Web framework inspired by Flask. Robotics Libraries for robotics.\nPythonRobotics - This is a compilation of various robotics algorithms with visualizations. rospy - This is a library for ROS (Robot Operating System). RPC Servers RPC-compatible servers.\nRPyC (Remote Python Call) - A transparent and symmetric RPC library for Python zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack. Science Libraries for scientific computing. Also see Python-for-Scientists.\nastropy - A community Python library for Astronomy. bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis. bccb - Collection of useful code related to biological analysis. Biopython - Biopython is a set of freely available tools for biological computation. cclib - A library for parsing and interpreting the results of computational chemistry packages. Colour - Implementing a comprehensive number of colour theory transformations and algorithms. Karate Club - Unsupervised machine learning toolbox for graph structured data. NetworkX - A high-productivity software for complex networks. NIPY - A collection of neuroimaging toolkits. NumPy - A fundamental package for scientific computing with Python. ObsPy - A Python toolbox for seismology. Open Babel - A chemical toolbox designed to speak the many languages of chemical data. PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion. PyMC - Markov Chain Monte Carlo sampling toolkit. QuTiP - Quantum Toolbox in Python. RDKit - Cheminformatics and Machine Learning Software. SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering. SimPy - A process-based discrete-event simulation framework. statsmodels - Statistical modeling and econometrics in Python. SymPy - A Python library for symbolic mathematics. Zipline - A Pythonic algorithmic trading library. Search Libraries and software for indexing and performing search queries on data.\ndjango-haystack - Modular search for Django. elasticsearch-dsl-py - The official high-level Python client for Elasticsearch. elasticsearch-py - The official low-level Python client for Elasticsearch. pysolr - A lightweight Python wrapper for Apache Solr. whoosh - A fast, pure Python search engine library. Serialization Libraries for serializing complex data types\nmarshmallow - A lightweight library for converting complex objects to and from simple Python datatypes. pysimdjson - A Python bindings for simdjson. python-rapidjson - A Python wrapper around RapidJSON. ultrajson - A fast JSON decoder and encoder written in C with Python bindings. Serverless Frameworks Frameworks for developing serverless Python code.\npython-lambda - A toolkit for developing and deploying Python code in AWS Lambda. Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway. Shell Shells based on Python.\nxonsh - A Python-powered, cross-platform, Unix-gazing shell language and command prompt. Specific Formats Processing Libraries for parsing and manipulating specific text formats.\nGeneral tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML. Office docxtpl - Editing a docx document by jinja2 template openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files. pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files. python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files. python-pptx - Python library for creating and updating PowerPoint (.pptx) files. unoconv - Convert between any document format supported by LibreOffice/OpenOffice. XlsxWriter - A Python module for creating Excel .xlsx files. xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa. xlwt / xlrd - Writing and reading data and formatting information from Excel files. PDF PDFMiner - A tool for extracting information from PDF documents. PyPDF2 - A library capable of splitting, merging and transforming PDF pages. ReportLab - Allowing Rapid creation of rich PDF documents. Markdown Mistune - Fastest and full featured pure Python parsers of Markdown. Python-Markdown - A Python implementation of John Gruber’s Markdown. YAML PyYAML - YAML implementations for Python. CSV csvkit - Utilities for converting to and working with CSV. Archive unp - A command line tool that can unpack archives easily. Static Site Generator Static site generator is a software that takes some text + templates as input and produces HTML files on the output.\nlektor - An easy to use static CMS and blog engine. mkdocs - Markdown friendly documentation generator. makesite - Simple, lightweight, and magic-free static site/blog generator (\u0026lt; 130 lines). nikola - A static website and blog generator. pelican - Static site generator that supports Markdown and reST syntax. Tagging Libraries for tagging items.\ndjango-taggit - Simple tagging for Django. Task Queues Libraries for working with task queues.\ncelery - An asynchronous task queue/job queue based on distributed message passing. dramatiq - A fast and reliable background task processing library for Python 3. huey - Little multi-threaded task queue. mrq - A distributed worker task queue in Python using Redis \u0026amp; gevent. rq - Simple job queues for Python. Template Engine Libraries and tools for templating and lexing.\nGenshi - Python templating toolkit for generation of web-aware output. Jinja2 - A modern and designer friendly templating language. Mako - Hyperfast and lightweight templating for the Python platform. Testing Libraries for testing codebases and generating test data.\nTesting Frameworks hypothesis - Hypothesis is an advanced Quickcheck style property based testing library. nose2 - The successor to nose, based on `unittest2. pytest - A mature full-featured Python testing tool. Robot Framework - A generic test automation framework. unittest - (Python standard library) Unit testing framework. Test Runners green - A clean, colorful test runner. mamba - The definitive testing tool for Python. Born under the banner of BDD. tox - Auto builds and tests distributions in multiple Python versions GUI / Web Testing locust - Scalable user load testing tool written in Python. PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings. Schemathesis - A tool for automatic property-based testing of web applications built with Open API / Swagger specifications. Selenium - Python bindings for Selenium WebDriver. sixpack - A language-agnostic A/B Testing framework. splinter - Open source tool for testing web applications. Mock doublex - Powerful test doubles framework for Python. freezegun - Travel through time by mocking the datetime module. httmock - A mocking library for requests for Python 2.6+ and 3.2+. httpretty - HTTP request mock tool for Python. mock - (Python standard library) A mocking and patching library. mocket - A socket mock framework with gevent/asyncio/SSL support. responses - A utility library for mocking out the requests Python library. VCR.py - Record and replay HTTP interactions on your tests. Object Factories factory_boy - A test fixtures replacement for Python. mixer - Another fixtures replacement. Supports Django, Flask, SQLAlchemy, Peewee and etc. model_mommy - Creating random fixtures for testing in Django. Code Coverage coverage - Code coverage measurement. Fake Data fake2db - Fake database generator. faker - A Python package that generates fake data. mimesis - is a Python library that help you generate fake data. radar - Generate random datetime / time. Text Processing Libraries for parsing and manipulating plain texts.\nGeneral chardet - Python 2/3 compatible character encoding detector. difflib - (Python standard library) Helpers for computing deltas. ftfy - Makes Unicode text less broken and more consistent automagically. fuzzywuzzy - Fuzzy String Matching. Levenshtein - Fast computation of Levenshtein distance and string similarity. pangu.py - Paranoid text spacing. pyfiglet - An implementation of figlet written in Python. pypinyin - Convert Chinese hanzi (漢字) to pinyin (拼音). textdistance - Compute distance between sequences with 30+ algorithms. unidecode - ASCII transliterations of Unicode text. Slugify awesome-slugify - A Python slugify library that can preserve unicode. python-slugify - A Python slugify library that translates unicode to ASCII. unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency. Unique identifiers hashids - Implementation of hashids in Python. shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs. Parser ply - Implementation of lex and yacc parsing tools for Python. pygments - A generic syntax highlighter. pyparsing - A general purpose framework for generating parsers. python-nameparser - Parsing human names into their individual components. python-phonenumbers - Parsing, formatting, storing and validating international phone numbers. python-user-agents - Browser user agent parser. sqlparse - A non-validating SQL parser. Third-party APIs Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries.\napache-libcloud - One Python library for all clouds. boto3 - Python interface to Amazon Web Services. django-wordpress - WordPress models and views for Django. facebook-sdk - Facebook Platform Python SDK. google-api-python-client - Google APIs Client Library for Python. gspread - Google Spreadsheets Python API. twython - A Python wrapper for the Twitter API. URL Manipulation Libraries for parsing URLs.\nfurl - A small Python library that makes parsing and manipulating URLs easy. purl - A simple, immutable URL class with a clean API for interrogation and manipulation. pyshorteners - A pure Python URL shortening lib. webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks. Video Libraries for manipulating video and GIFs.\nmoviepy - A module for script-based movie editing with many formats, including animated GIFs. scikit-video - Video processing routines for SciPy. vidgear - Most Powerful multi-threaded Video Processing framework. Web Asset Management Tools for managing, compressing and minifying website assets.\ndjango-compressor - Compresses linked and inline JavaScript or CSS into a single cached file. django-pipeline - An asset packaging library for Django. django-storages - A collection of custom storage back ends for Django. fanstatic - Packages, optimizes, and serves static file dependencies as Python packages. fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP. flask-assets - Helps you integrate webassets into your Flask app. webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources. Web Content Extracting Libraries for extracting web contents.\nhtml2text - Convert HTML to Markdown-formatted text. lassie - Web Content Retrieval for Humans. micawber - A small library for extracting rich content from URLs. newspaper - News extraction, article extraction and content curation in Python. python-readability - Fast Python port of arc90\u0026rsquo;s readability tool. requests-html - Pythonic HTML Parsing for Humans. sumy - A module for automatic summarization of text documents and HTML pages. textract - Extract text from any document, Word, PowerPoint, PDFs, etc. toapi - Every web site provides APIs. Web Crawling Libraries to automate web scraping.\ncola - A distributed crawling framework. feedparser - Universal feed parser. grab - Site scraping framework. MechanicalSoup - A Python library for automating interaction with websites. portia - Visual scraping for Scrapy. pyspider - A powerful spider system. robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser. scrapy - A fast high-level screen scraping and web crawling framework. Web Frameworks Traditional full stack web frameworks. Also see RESTful API.\nSynchronous Django - The most popular web framework in Python. awesome-django awesome-django Flask - A microframework for Python. awesome-flask Pyramid - A small, fast, down-to-earth, open source Python web framework. awesome-pyramid Masonite - The modern and developer centric Python web framework. Asynchronous Tornado - A web framework and asynchronous networking library. WebSocket Libraries for working with WebSocket.\nautobahn-python - WebSocket \u0026amp; WAMP for Python on Twisted and asyncio. channels - Developer-friendly asynchrony for Django. websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity. WSGI Servers WSGI-compatible web servers.\nbjoern - Asynchronous, very fast and written in C. gunicorn - Pre-forked, ported from Ruby\u0026rsquo;s Unicorn project. uWSGI - A project aims at developing a full stack for building hosting services, written in C. waitress - Multi-threaded, powers Pyramid. werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects. Resources Where to discover learning resources or new Python libraries.\nBooks Fluent Python Think Python Websites Tutorials Full Stack Python Python Cheatsheet Real Python The Hitchhiker’s Guide to Python Ultimate Python study guide Libraries Awesome Python @LibHunt Others Python ZEEF Pythonic News What the f*ck Python! Newsletters Awesome Python Newsletter Pycoder\u0026rsquo;s Weekly Python Tricks Python Weekly Podcasts Django Chat Podcast.__init__ Python Bytes Running in Production Talk Python To Me Test and Code The Real Python Podcast Contributing Your contributions are always welcome! Please take a look at the contribution guidelines first.\nI will keep some pull requests open if I\u0026rsquo;m not sure whether those libraries are awesome, you could vote for them by adding 👍 to them. Pull requests will be merged when their votes reach 20.\nIf you have any question about this opinionated list, do not hesitate to contact me @VintaChen on Twitter or open an issue on GitHub.\n","permalink":"https://samirpaul1.github.io/blog/posts/list-of-python-frameworks-libraries-software-and-resources/","summary":"List of Python Frameworks Libraries Software and Resources","title":"List of Python Frameworks Libraries Software and Resources"},{"content":" A curated list of lists of technical interview questions.\nTable of Contents Programming Languages/Frameworks/Platforms\nAndroid AngularJS Angular BackboneJS C++ C C♯ .NET Clojure CSS Cucumber Django Docker Elastic EmberJS Erlang Golang GraphQl HTML Ionic iOS Java JavaScript jQuery Front-end build tools KnockoutJS Less Lisp NodeJS Objective-C PHP Python ReactJS Rails Ruby Rust Sass Scala Shell Spark Swift Vue.js Wordpress TypeScript Database technologies\nCassandra Microsoft Access MongoDB MySQL Neo4j Oracle Postgres SQL SQL Lite Caching technologies\nMemcached Redis OS\nLinux Windows Algorithms\nBlockchain\nCoding exercises\nComprehensive lists\nDesign patterns\nData structures\nNetworks\nSecurity\nData Science\nProgramming Languages/Frameworks/Platforms Android 10 Android interview question answers for Freshers 20 Essential Android Interview Questions from Toptal 25 Essential Android Interview Questions from Adeva A couple of Android questions posted by Quora users A great list of Android interview questions covering all the aspects of this career Collection of Android and Java related questions and topics, including general developer questions, Java core, Data structures, Build Tools, Programming Paradigms, Core Android, Databases and etc Collection of Android and Java questions divided by experience RocketSkill App Android Interview Questions Android cheat sheet: Coding program, Data structure, Android and Java interview questions with answers and categorized by topics Android Interview Questions And Answers From Beginner To Advanced Interview Questions for Senior Android Developers 35+ Android Interview Questions AngularJS 12 Essential AngularJS Interview Questions from Toptal An AngularJS exam with questions from beginner to expert by @gdi2290 from @AngularClass 29 AngularJS Interview Questions – Can You Answer Them All? Great Article from Codementor AngularJS interview questions and answers for experienced developers AngularJS Interview Questions which have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of AngularJS This article discusses the top 50 Most occurred AngularJS interview question with answers Top 25 Angularjs Interview Questions and Quiz 100 AngularJS Interview Questions - Quick Refresher Angular A list of helpful Angular related questions you can use to interview potential candidates, test yourself or completely ignore Angular 2 Interview Questions List of 300 Angular Interview Questions and Answers Angular Interview Questions (2020) Top Angular Interview Questions and Answers in 2021 BackboneJS 8 Essential Backbonejs Interview Questions from Toptal Backbonejs Interview Questions And Answers from web technology experts notes Top 25 Backbone.js interview questions C++ 1000+ Multiple Choice Questions \u0026amp; Answers in C++ with explanations 200 C++ interview questions and answers 24 Essential C++ Interview Questions from Toptal C++ Interview Questions from GeekInterview C++ Programming Q\u0026amp;A and quizzes from computer science portal for geeks C++ Programming Questions and Answers related to such topics as OOPs concepts, Object and Classes, Functions, Constructors and Destructors, Inheritance and etc LeetCode Problems\u0026rsquo; Solutions written in C++ C Basic C language technical frequently asked interview questions and answers It includes data structures, pointers interview questions and answers for experienced C Programming Interview Questions and Answers for such topics as Bits and Bytes, Preprocessors, Functions, Strings, Language basics and etc C Programming Interview Questions have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of C Programming First set of commonly asked C programming interview questions from computer science portal for geeks Second set of commonly asked C programming interview questions from computer science portal for geeks 9 Essential C Interview Questions with answers Top C Interview Questions and Answers C# 15 Essential C# Interview Question from Toptal C# interview questions from dotnetfunda.com Top 50 C# Interview Questions \u0026amp; Answers 50 C# Coding Interview Questions and Answers 20 C# OOPS Interview Questions and Answers 30+ C# Interview Questions .NET 300 ASPNET interview questions and answers ASP.NET Core Interview Questions Great list of NET interview questions covering all the NET platform topics NET Interview Questions and Answers for Beginners which consists of the most frequently asked questions in NET This list of 100+ questions and answers gauge your familiarity with the NET platform Questions gathered by community of the StackOverflow What Great NET Developers Ought To Know (More NET Interview Questions) Clojure Classic \u0026lsquo;Fizz Buzz\u0026rsquo; interview question for Clojure developers Clojure Interview Questions for experienced devs Coding exercises in Clojure, handy practice for technical interview questions Experience and questions from Clojure developer interview collected by Reddit users Interview cake Clojure solutions CSS CSS interview questions and answers for freshers and experienced candidates Also there you can find CSS online practice tests to fight written tests and certification exams on CSS Development hiring managers and potential interviewees may find there sample CSS proficiency interview Q\u0026amp;As and code snippets useful Interview Questions and Exercises About CSS Top 50 CSS(Cascading Style Sheet) Interview Questions covering the most of tricky CSS moments Front End Interview Handbook - CSS Questions and Answers Cucumber Cucumber Web Application BDD Sample Interview Questions Guide to building a simple Cucumber + Watir page object pattern framework Django Some abstract interview questions for Python/Django developers Some Django basic interview questions to establish the basic level of the candidates Top 16 Django Interview Questions for both freshers and experienced developers Docker Docker Interview Questions Top Docker Interview Questions You Must Prepare In 2019 Top Docker Interview Questions And Answers DOCKER (SOFTWARE) INTERVIEW QUESTIONS \u0026amp; ANSWERS 30 Docker Interview Questions and Answers in 2019 Docker Interview Questions \u0026amp; Answers Top 50 Docker Interview Questions \u0026amp; Answers Top 50+ Docker Interview Questions and Answers in 2021 Elastic Top Elastic Stack Interview Questions EmberJS 8 Essential Emberjs Interview Questions from Toptal Top 25 Emberjs Interview Questions for both freshers and experienced developers Erlang Top 22 Erlang Interview Questions for both freshers and experienced developers Golang Solutions for Elements of Programming Interviews problems written in Golang Solutions for some basic coding interview tasks written in Go Top 20 GO Programming Interview Questions for both freshers and experienced developers GraphQl 8 GraphQl Interview Questions To Know How to GraphQl - Common Questions HTML 10 Typical HTML Interview Exercises from SitePoint.com 16 Essential HTML5 Interview Questions from Toptal 40 important HTML 5 Interview questions with answers HTML interview questions and answers for freshers and experienced candidates Also find HTML online practice tests to fight written tests and certification exams on HTML Top 50 HTML Interview Questions for both freshers and experienced developers Common HTML interview questions for freshers Front End Interview Handbook - HTML Questions and Answers 30 HTML Interview Questions and Answers 30+ HTML Interview Questions (2021) Ionic 23 Beginner Level Ionic Framework Questions 12 Essential Ionic Interview Questions 45 Ionic Interview Questions Most Asked Ionic Interview Questions iOS 14 Essential iOS Interview Questions from Toptal 20 iOS Developer Interview Questions and Answers for getting you ready for your interview 25 Essential iOS Interview Questions from Adeva A small guide to help those looking to hire a developer or designer for iOS work While tailored for iOS, many questions could be used for Android developers or designers as well A great self-test if you\u0026rsquo;re looking to keep current or practice for your own interview All you need to know about iOS technical interview including some tips for preparing, questions and some coding exercises Interview Questions for iOS and Mac Developers from the CEO of Black Pixel iOS Interview Questions and Answers including such topics as Development Basics, App states and multitasking, App states, Core app objects iOS Interview Questions For Senior Developers 50 iOS Interview Questions And Answers 1 50 iOS Interview Questions And Answers Part 2 50 iOS Interview Questions And Answers Part 3 50 iOS Interview Questions And Answers Part 4 50 iOS Interview Questions And Answers Part 5 10 iOS interview questions and answers iOS Developer and Designer Interview Questions IOS Interview Questions and Answers iOS Interview Questions For Beginners Babylon iOS Interview Questions RocketSkill App iOS Interview Questions iOS Static vs Dynamic Dispatch Java List of Java programs for interview Categoriwise 115 Java Interview Questions and Answers – The ULTIMATE List 37 Java Interview Questions to Practice With from Codementor 21 Essential Java Interview Questions Top 30 Core Java Interview Questions 29 Essential Java Interview Questions from Adeva A collection of Java interview questions and answers to them Data Structures and Algorithms in Java which can be useful in interview process Java Interview Questions: How to crack the TOP 15 questions 300 Core Java Interview Questions Top 10 Tricky Java interview questions and Answers Top 25 Most Frequently Asked Interview Core Java Interview Questions And Answers Top 40 Core Java Interview Questions Answers from Telephonic Round Top 50 Spring Interview Questions You Must Prepare For In 2020 Spring Interview Questions And Answers Interview Cake Java Interview Questions Java Interview Questions \u0026amp; Quizzes Essetial Java Interview Questions Fundamental Java Interview Questions JavaScript Practice common algorithms using JavaScript 10 Interview Questions Every JavaScript Developer Should Know 21 Essential JavaScript Interview Questions from best mentors all over the world 20 Essential JavaScript Interview Questions from Adeva 37 Essential JavaScript Interview Questions from Toptal 5 More JavaScript Interview Exercises 5 Typical JavaScript Interview Exercises Development hiring managers and potential interviewees may find these sample JavaScript proficiency interview Q\u0026amp;As and code snippets useful 123 Essential JavaScript Interview Question JavaScript Interview Questions have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of JavaScript JS: Basics and Tricky Questions JS: Interview Algorithm Some basic javascript coding challenges and interview questions Some JavaScript interview exercises Ten Questions I\u0026rsquo;ve Been Asked, Most More Than Once, Over Six Technical JavaScript / Front-End Engineer Job Interviews. Top 85 JavaScript Interview Questions Interview Cake JavaScript Interview Questions The Best Frontend JavaScript Interview Questions (written by a Frontend Engineer) 10 JavaScript Concepts You Need to Know for Interviews Front End Interview Handbook - JavaScript Questions and Answers JavaScript Interview Questions - Quick Refresher The MEGA Interview Guide Javascript Interview Questions and Answers (2020) JavaScript Modern Interview Code Challenges 2021 70 JavaScript Interview Questions jQuery Top 50 jquery interview questions 17 Essential jQuery Interview Questions From Toptal Top JQuery Interview Questions and Answers Front-end build tools Webpack interview questions \u0026amp; answers Gulp js interview questions Grunt js interview questions for beginners Grunt js interview questions KnockoutJS 15 interview questions from CodeSample.com 20 questions you might be asked about KnockoutJS in an interview for both freshers and experienced developers Less Top 25 LESS Interview Questions Lisp 10 LISP Questions \u0026amp; Answers Top 18 Lisp Interview Questions from Career Guru NodeJS 25 Essential Node.js Interview Questions from Adeva 8 Essential Nodejs Interview Questions from Toptal Node.JS Interview Questions have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of Node.JS Node.js Interview Questions and Answers Top 25 Nodejs Interview Questions \u0026amp; Answers from Career Guru Top 30 Node.Js Interview Questions With Answers Top Nodejs Interview Questions \u0026amp; Answers Node.js Interview Questions in Chinese Node.js Interview Questions by learning-zone Objective-C Interview Qs for Objective-C and Swift iOS Interview Questions For Beginners PHP 100 PHP interview questions and answers from CareerRide.com 21 Essential PHP Interview Questions from Toptal 20 Common PHP Job Interview Questions and Answers 25 Essential PHP Interview Questions from Adeva PHP interview questions and answers for freshers Top 100 PHP Interview Questions \u0026amp; Answers from CareerGuru 25 PHP Interview Questions 26 Essential PHP Interview Questions for 2018 Cracking PHP Interviews Questions ebook 300+ Q\u0026amp;A PHP Interview Questions - Quick Refresher 30+ PHP Interview Questions Python 26 Essential Python Interview Questions from Adeva 20 Python interview questions and answers 11 Essential Python Interview Questions from Toptal A listing of questions that could potentially be asked for a python job listing Interview Questions for both beginners and experts Interview Cake Python Interview Questions Python Frequently Asked Questions (Programming) Python interview questions collected by Reddit users Top 25 Python Interview Questions from Career Guru Python Interview 10 questions from Corey Schafer Python interview questions. Part I. Junior Python interview questions. Part II. Middle Python interview questions. Part III. Senior Python Interview Questions and Answers (2019) 100 Python Interview Questions - Quick Refresher Top 100 Python Interview Questions from Edureka (2021) Ruby on Rails 20 Ruby on Rails interview questions and answers from CareerRide.com 9 Essential Ruby on Rails Interview Questions from Toptal High-level Ruby on Rails Interview Questions Ruby And Ruby On Rails interview Q\u0026amp;A Some of the most frequently asked Ruby on Rails questions and how to answer them confidently 11 Ruby on Rails Interview Practice Questions Top 53 Ruby on Rails Interview Questions \u0026amp; Answers 10 Ruby on Rails interview questions and answers ReactJS Reddit users share their expectations from ReactJS interview 5 Essential React.js Interview Questions React Interview Questions Toptal\u0026rsquo;s 21 Essential React.js Interview Questions 19 Essential ReactJs Interview Questions React Interview Questions \u0026amp; Answers Ruby 21 Essential Ruby Interview Questions from Toptal 15 Questions to Ask During a Ruby Interview A list of questions about Ruby programming you can use to quiz yourself The Art of Ruby Technical Interview Interview Cake Ruby Interview Questions Frequently Asked Ruby Interview Questions Rust Top 250+ Rust Programming Language Interview Questions Rust Programming Interview Questions and Answers rust-exam: A set of questions about the Rust programming language Best Rust Programming Language Interview Questions and answers Sass Top 17 Sass Interview Questions from Career Guru Top 10 Sass Interview Questions from educba Scala 4 Interview Questions for Scala Developers A list of Frequently Asked Questions and their answers, sorted by category A list of helpful Scala related questions you can use to interview potential candidates How Scala Developers Are Being Interviewed Top 25 Scala Interview Questions \u0026amp; Answers from Toptal SharePoint Sharepoint Interview Question For Developer Top SharePoint Interview Questions and Answers Shell Top 50 Shell Scripting Interview Questions from Career Guru Spark Carefully Curated 70 Spark Questions with Additional Optimization Guides (First in the series) Swift 10 Essential Swift Interview Questions from Toptal Get prepared for your next iOS job interview by studying high quality LeetCode solutions in Swift 5 Swift Interview Questions and Answers Swift Programming Language Interview Questions And Answers from mycodetips.com Your top 10 Swift questions answered Swift interview questions and answers on Swift 5 by Raywenderlich Dynamic keyword in Swift Vue.js List of 300 VueJS Interview Questions WordPress Top 45 WordPress interview questions 10 Essential WordPress Interview Questions TypeScript Typescript Interview Questions Top 10 TypeScript Interview Questions and Answers for Beginner Web Developers 2019 Database technologies Cassandra Top 23 Cassandra Interview Questions from Career Guru Microsoft Access Top 16 Microsoft Access Database Interview Questions from Career Guru MongoDB 28 MongoDB NoSQL Database Interview Questions and Answers\nMongoDB frequently Asked Questions by expert members with experience in MongoDB These questions and answers will help you strengthen your technical skills, prepare for the new job test and quickly revise the concepts\nMongoDB Interview Questions from JavaTPointcom\nMongoDB Interview Questions that have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of MongoDB\nTop 20 MongoDB interview questions from Career Guru\nMySQL 10 MySQL Database Interview Questions for Beginners and Intermediates 100 MySQL interview questions 15 Basic MySQL Interview Questions for Database Administrators 28 MySQL interview questions from JavaTPoint.com 40 Basic MySQL Interview Questions with Answers Top 50 MySQL Interview Questions \u0026amp; Answers from Career Guru Neo4j Top 20 Neo4j Interview Questions from Career Guru Oracle General Oracle Interview Questions \u0026amp; Answers Postgres 13 PostgreSQL Interview Q\u0026amp;A Frequently Asked Basic PostgreSQL Interview Questions and Answers PostgreSQL Interview Preparation Guide PostgreSQL Interview Q\u0026amp;A from CoolInterview.com SQL 10 Frequently asked SQL Query Interview Questions 45 Essential SQL Interview Questions from Toptal Common Interview Questions and Answers General Interview Questions and Answers Schema, Questions \u0026amp; Solutions for SQL Exercising SQL Interview Questions that have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of SQL SQL Interview Questions CHEAT SHEET SQLite Top 20 SQLITE Interview Questions from Career Guru Caching technologies Memcached Memcached Interview Questions from Javapoint Memcached Interview Questions from Wisdomjobs Redis Redis Interview Questions from Javapoint Redis Interview Questions from Wisdomjobs Redis Interview Questions from Career Guru OS Linux 10 Job Interview Questions for Linux System Administrators from Linux.com 10 Useful Random Linux Interview Questions and Answers 11 Basic Linux Interview Questions and Answers 11 Essential Linux Interview Questions from Toptal Top 30 Linux System Admin Interview Questions \u0026amp; Answers Top 50 Linux Interview Questions from Career Guru 278 Test Questions and Answers for *nix System Administrators Linux Interview Questions - Quick Refresher Windows Top 10 Interview Questions for Windows Administrators Top 22 Windows Server Interview Questions from Career Guru Windows Admin Interview Questions \u0026amp; Answers DevOps Linux System Administrator/DevOps Interview Questions Top DevOps Interview Questions You Must Prepare In 2021 Top 60+ DevOps Interview Questions \u0026amp; Answers in 2021 DevOps Interview Questions \u0026amp; Answers Algorithms Comprehensive list of interview questions of top tech companies A great list of Java interview questions Algorithms playground for common interview questions written in Ruby EKAlgorithms contains some well known CS algorithms \u0026amp; data structures Top 10 Algorithms for Coding Interview Top 15 Data Structures and Algorithm Interview Questions for Java programmer Tech Interview Handbook Best Practice Questions Daily Coding Interview Practice Blockchain Top 55 Blockchain Interview Questions You Must Prepare In 2018 Blockchain Interview Questions Top Blockchain Interview Questions Blockchain Developer Interview Questions and Answers 10 Essential Blockchain Interview Questions Top 30 Blockchain Interview Questions – For Freshers to Experienced Most Frequently Asked Blockchain Interview Questions Coding exercises Common interview questions and puzzles solved in several languages Interactive, test-driven Python coding challenges (algorithms and data structures) typically found in coding interviews or coding competitions Interview questions solved in python 7 Swift Coding Challenges to Practice Your Skills Comprehensive lists A list of helpful front-end related questions you can use to interview potential candidates, test yourself or completely ignore Front End Developer Interview Questions Front End Interview Handbook Some simple questions to interview potential backend candidates Design Patterns Design Pattern Interview Questions that have been designed specially to get you acquainted with the nature of questions you may encounter during your interview for the subject of Design Pattern Design Patterns for Humans™ - An ultra-simplified explanation Design Patterns implemented in Java Design Patterns implemented in DotNet Data structures Top 15 Data Structures and Algorithm Interview Questions for Java programmer Top 50 Data Structure Interview Questions from Career Guru What is Data Structure? | Top 40 Data Structure Interview Questions Networks Top 100 Networking Interview Questions \u0026amp; Answers from Career Guru Networking Interview Questions Security 101 IT Security Interview Questions How to prepare for an information security job interview? Information Security Interview Questions from Daniel Miessler Top 50 Information Security Interview Questions for freshers and experts Data Science Data Science Interview Questions for Top Tech Companies 66 Job Interview Questions for Data Scientists Top 45 Data Science Interview Questions You Must Prepare In 2021 Top 30 data science interview questions Top 100 Data science interview questions Data Science Interview Questions 160+ Data Science Interview Questions Top Data Science Interview Questions ","permalink":"https://samirpaul1.github.io/blog/posts/lists-of-technical-interview-questions/","summary":"Lists of Technical Interview Questions","title":"Lists of Technical Interview Questions"},{"content":"About The Project: An online PDF file compression tool to reduce the size of a .pdf file. Python Flask is used to upload the file to a temporary location on the server. In the backend, using the PDFNetPython library that file gets reduced and saved to its final location. After download, the files are automatically deleted from the server after 1 hour. Technologies used in this project: Python3, Flask, C, Shell, Nix, Replit, Git, HTML, CSS, JavaScript.\nLive Demo 🚀 Video Demo: Landing Page: Flask File Uploading: In HTML form, the enctype property is set to \u0026quot;multipart/form-data\u0026quot; to publish the file to the URL.The URL handler extracts the file from the request.files [] object and saves it to the required location. The path to the upload folder is defined as app.config['UPLOAD_FOLDER'] and maximum size (in bytes) as maximum size (in bytes). The server-side flask script fetches the file from the request object using name = request.files['file'].filename. On successfully uploading the file, it is saved to the desired location on the server. Here’s the Python code for the Flask application.\nfrom flask import Flask, render_template, request from werkzeug import secure_filename app = Flask(__name__) @app.route(\u0026#39;/upload\u0026#39;) def upload_file(): return render_template(\u0026#39;upload.html\u0026#39;) @app.route(\u0026#39;/uploader\u0026#39;, methods = [\u0026#39;GET\u0026#39;, \u0026#39;POST\u0026#39;]) def upload_file(): if request.method == \u0026#39;POST\u0026#39;: f = request.files[\u0026#39;file\u0026#39;] f.save(secure_filename(f.filename)) return \u0026#39;file uploaded successfully\u0026#39; if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug = True) How PDF is compressed in backend: import os import sys from PDFNetPython3.PDFNetPython import PDFDoc, Optimizer, SDFDoc, PDFNet def compress_file(input_file: str, output_file: str): if not output_file: output_file = input_file try: PDFNet.Initialize() doc = PDFDoc(input_file) doc.InitSecurityHandler() Optimizer.Optimize(doc) doc.Save(output_file, SDFDoc.e_linearized) doc.Close() except Exception as e: doc.Close() return False return True if __name__ == \u0026#34;__main__\u0026#34;: input_file = sys.argv[1] output_file = sys.argv[2] compress_file(input_file, output_file) File Download: function downloadFile(filename) { if(response !== null) { fname = response.filename; var url = \u0026#34;static/resource/\u0026#34; + fname.toString(2); console.log(url); fetch(url) .then(response =\u0026gt; response.blob()) .then(blob =\u0026gt; { const link = document.createElement(\u0026#34;a\u0026#34;); link.href = URL.createObjectURL(blob); link.download = fname; link.click(); }) .catch(console.error); } } 🤔 How to contribute Fork this repository; Create a branch with your feature: git checkout -b my-feature; Commit your changes: git commit -m \u0026quot;feat: my new feature\u0026quot;; Push to your branch: git push origin my-feature. ","permalink":"https://samirpaul1.github.io/blog/posts/online-pdf-compression-tool/","summary":"Online PDF Compression Tool","title":"Online PDF Compression Tool"},{"content":"System Design Course Learn how to design systems at scale and prepare for system design interviews.\nDownload PDF Table of contents Getting Started\nWhat is system design? Chapter I\nIP OSI Model TCP and UDP Domain Name System (DNS) Load Balancing Clustering Caching Content Delivery Network (CDN) Proxy Availability Scalability Storage Chapter II\nDatabases and DBMS SQL databases NoSQL databases SQL vs NoSQL databases Database Replication Indexes Normalization and Denormalization ACID and BASE consistency models CAP theorem PACELC Theorem Transactions Distributed Transactions Sharding Consistent Hashing Database Federation Chapter III\nN-tier architecture Message Brokers Message Queues Publish-Subscribe Enterprise Service Bus (ESB) Monoliths and Microservices Event-Driven Architecture (EDA) Event Sourcing Command and Query Responsibility Segregation (CQRS) API Gateway REST, GraphQL, gRPC Long polling, WebSockets, Server-Sent Events (SSE) Chapter IV\nGeohashing and Quadtrees Circuit breaker Rate Limiting Service Discovery SLA, SLO, SLI Disaster recovery Virtual Machines (VMs) and Containers OAuth 2.0 and OpenID Connect (OIDC) Single Sign-On (SSO) SSL, TLS, mTLS Chapter V\nSystem Design Interviews URL Shortener Whatsapp Twitter Netflix Uber Appendix\nNext Steps References What is system design? Before we start this course, let\u0026rsquo;s talk about what even is system design.\nSystem design is the process of defining the architecture, interfaces, and data for a system that satisfies specific requirements. System design meets the needs of your business or organization through coherent and efficient systems. It requires a systematic approach to building and engineering systems. A good system design requires us to think about everything, from infrastructure all the way down to the data and how it\u0026rsquo;s stored.\nWhy is System Design so important? System design helps us define a solution that meets the business requirements. It is one of the earliest decisions we can make when building a system. Often it is essential to think from a high level as these decisions are very difficult to correct later. It also makes it easier to reason about and manage architectural changes as the system evolves.\nIP An IP address is a unique address that identifies a device on the internet or a local network. IP stands for \u0026ldquo;Internet Protocol\u0026rdquo;, which is the set of rules governing the format of data sent via the internet or local network.\nIn essence, IP addresses are the identifier that allows information to be sent between devices on a network. They contain location information and make devices accessible for communication. The internet needs a way to differentiate between different computers, routers, and websites. IP addresses provide a way of doing so and form an essential part of how the internet works.\nVersions Now, let\u0026rsquo;s learn about the different versions of IP addresses:\nIPv4 The original Internet Protocol is IPv4 which uses a 32-bit numeric dot-decimal notation that only allows for around 4 billion IP addresses. Initially, it was more than enough but as internet adoption grew we needed something better.\nExample: 102.22.192.181\nIPv6 IPv6 is a new protocol that was introduced in 1998. Deployment commenced in the mid-2000s and since the internet users have grown exponentially, it is still ongoing.\nThis new protocol uses 128-bit alphanumeric hexadecimal notation. This means that IPv6 can provide about ~340e+36 IP addresses. That\u0026rsquo;s more than enough to meet the growing demand for years to come.\nExample: 2001:0db8:85a3:0000:0000:8a2e:0370:7334\nTypes Let\u0026rsquo;s discuss types of IP addresses:\nPublic A public IP address is an address where one primary address is associated with your whole network. In this type of IP address, each of the connected devices has the same IP address.\nExample: IP address provided to your router by the ISP.\nPrivate A private IP address is a unique IP number assigned to every device that connects to your internet network, which includes devices like computers, tablets, and smartphones, which are used in your household.\nExample: IP addresses generated by your home router for your devices.\nStatic A static IP address does not change and is one that was manually created, as opposed to having been assigned. These addresses are usually more expensive but are more reliable.\nExample: They are usually used for important things like reliable geo-location services, remote access, server hosting, etc.\nDynamic A dynamic IP address changes from time to time and is not always the same. It has been assigned by a Dynamic Host Configuration Protocol (DHCP) server. Dynamic IP addresses are the most common type of internet protocol addresses. They are cheaper to deploy and allow us to reuse IP addresses within a network as needed.\nExample: They are more commonly used for consumer equipment and personal use.\nOSI Model The OSI Model is a logical and conceptual model that defines network communication used by systems open to interconnection and communication with other systems. The Open System Interconnection (OSI Model) also defines a logical network and effectively describes computer packet transfer by using various layers of protocols.\nThe OSI Model can be seen as a universal language for computer networking. It\u0026rsquo;s based on the concept of splitting up a communication system into seven abstract layers, each one stacked upon the last.\nWhy does the OSI model matter? The Open System Interconnection (OSI) model has defined the common terminology used in networking discussions and documentation. This allows us to take a very complex communications process apart and evaluate its components.\nWhile this model is not directly implemented in the TCP/IP networks that are most common today, it can still help us do so much more, such as:\nMake troubleshooting easier and help identify threats across the entire stack. Encourage hardware manufacturers to create networking products that can communicate with each other over the network. Essential for developing a security-first mindset. Separate a complex function into simpler components. Layers The seven abstraction layers of the OSI model can be defined as follows, from top to bottom:\nApplication This is the only layer that directly interacts with data from the user. Software applications like web browsers and email clients rely on the application layer to initiate communication. But it should be made clear that client software applications are not part of the application layer, rather the application layer is responsible for the protocols and data manipulation that the software relies on to present meaningful data to the user. Application layer protocols include HTTP as well as SMTP.\nPresentation The presentation layer is also called the Translation layer. The data from the application layer is extracted here and manipulated as per the required format to transmit over the network. The functions of the presentation layer are translation, encryption/decryption, and compression.\nSession This is the layer responsible for opening and closing communication between the two devices. The time between when the communication is opened and closed is known as the session. The session layer ensures that the session stays open long enough to transfer all the data being exchanged, and then promptly closes the session in order to avoid wasting resources. The session layer also synchronizes data transfer with checkpoints.\nTransport The transport layer (also known as layer 4) is responsible for end-to-end communication between the two devices. This includes taking data from the session layer and breaking it up into chunks called segments before sending it to the Network layer (layer 3). It is also responsible for reassembling the segments on the receiving device into data the session layer can consume.\nNetwork The network layer is responsible for facilitating data transfer between two different networks. The network layer breaks up segments from the transport layer into smaller units, called packets, on the sender\u0026rsquo;s device, and reassembles these packets on the receiving device. The network layer also finds the best physical path for the data to reach its destination this is known as routing. If the two devices communicating are on the same network, then the network layer is unnecessary.\nData Link The data link layer is very similar to the network layer, except the data link layer facilitates data transfer between two devices on the same network. The data link layer takes packets from the network layer and breaks them into smaller pieces called frames.\nPhysical This layer includes the physical equipment involved in the data transfer, such as the cables and switches. This is also the layer where the data gets converted into a bit stream, which is a string of 1s and 0s. The physical layer of both devices must also agree on a signal convention so that the 1s can be distinguished from the 0s on both devices.\nTCP and UDP TCP Transmission Control Protocol (TCP) is connection-oriented, meaning once a connection has been established, data can be transmitted in both directions. TCP has built-in systems to check for errors and to guarantee data will be delivered in the order it was sent, making it the perfect protocol for transferring information like still images, data files, and web pages.\nBut while TCP is instinctively reliable, its feedback mechanisms also result in a larger overhead, translating to greater use of the available bandwidth on the network.\nUDP User Datagram Protocol (UDP) is a simpler, connectionless internet protocol in which error-checking and recovery services are not required. With UDP, there is no overhead for opening a connection, maintaining a connection, or terminating a connection. Data is continuously sent to the recipient, whether or not they receive it.\nIt is largely preferred for real-time communications like broadcast or multicast network transmission. We should use UDP over TCP when we need the lowest latency and late data is worse than the loss of data.\nTCP vs UDP TCP is a connection-oriented protocol, whereas UDP is a connectionless protocol. A key difference between TCP and UDP is speed, as TCP is comparatively slower than UDP. Overall, UDP is a much faster, simpler, and more efficient protocol, however, retransmission of lost data packets is only possible with TCP.\nTCP provides ordered delivery of data from user to server (and vice versa), whereas UDP is not dedicated to end-to-end communications, nor does it check the readiness of the receiver.\nFeature TCP UDP Connection Requires an established connection Connectionless protocol Guaranteed delivery Can guarantee delivery of data Cannot guarantee delivery of data Re-transmission Re-transmission of lost packets is possible No re-transmission of lost packets Speed Slower than UDP Faster than TCP Broadcasting Does not support broadcasting Supports broadcasting Use cases HTTPS, HTTP, SMTP, POP, FTP, etc Video streaming, DNS, VoIP, etc Domain Name System (DNS) Earlier we learned about IP addresses that enable every machine to connect with other machines. But as we know humans are more comfortable with names than numbers. It\u0026rsquo;s easier to remember a name like google.com than something like 122.250.192.232.\nThis brings us to Domain Name System (DNS) which is a hierarchical and decentralized naming system used for translating human-readable domain names to IP addresses.\nHow DNS works Video: https://youtu.be/vhfRArT11jc\nDNS lookup involves the following eight steps:\nA client types example.com into a web browser, the query travels to the internet and is received by a DNS resolver. The resolver then recursively queries a DNS root nameserver. The root server responds to the resolver with the address of a Top Level Domain (TLD). The resolver then makes a request to the .com TLD. The TLD server then responds with the IP address of the domain\u0026rsquo;s nameserver, example.com. Lastly, the recursive resolver sends a query to the domain\u0026rsquo;s nameserver. The IP address for example.com is then returned to the resolver from the nameserver. The DNS resolver then responds to the web browser with the IP address of the domain requested initially. Once the IP address has been resolved, the client should be able to request content from the resolved IP address. For example, the resolved IP may return a webpage to be rendered in the browser\nServer types Now, let\u0026rsquo;s look at the four key groups of servers that make up the DNS infrastructure.\nDNS Resolver A DNS resolver (also known as a DNS recursive resolver) is the first stop in a DNS query. The recursive resolver acts as a middleman between a client and a DNS nameserver. After receiving a DNS query from a web client, a recursive resolver will either respond with cached data, or send a request to a root nameserver, followed by another request to a TLD nameserver, and then one last request to an authoritative nameserver. After receiving a response from the authoritative nameserver containing the requested IP address, the recursive resolver then sends a response to the client.\nDNS root server A root server accepts a recursive resolver\u0026rsquo;s query which includes a domain name, and the root nameserver responds by directing the recursive resolver to a TLD nameserver, based on the extension of that domain (.com, .net, .org, etc.). The root nameservers are overseen by a nonprofit called the Internet Corporation for Assigned Names and Numbers (ICANN).\nThere are 13 DNS root nameservers known to every recursive resolver. Note that while there are 13 root nameservers, that doesn\u0026rsquo;t mean that there are only 13 machines in the root nameserver system. There are 13 types of root nameservers, but there are multiple copies of each one all over the world, which use Anycast routing to provide speedy responses.\nTLD nameserver A TLD nameserver maintains information for all the domain names that share a common domain extension, such as .com, .net, or whatever comes after the last dot in a URL.\nManagement of TLD nameservers is handled by the Internet Assigned Numbers Authority (IANA), which is a branch of ICANN. The IANA breaks up the TLD servers into two main groups:\nGeneric top-level domains: These are domains like .com, .org, .net, .edu, and .gov. Country code top-level domains: These include any domains that are specific to a country or state. Examples include .uk, .us, .ru, and .jp. Authoritative DNS server The authoritative nameserver is usually the resolver\u0026rsquo;s last step in the journey for an IP address. The authoritative nameserver contains information specific to the domain name it serves (e.g. google.com) and it can provide a recursive resolver with the IP address of that server found in the DNS A record, or if the domain has a CNAME record (alias) it will provide the recursive resolver with an alias domain, at which point the recursive resolver will have to perform a whole new DNS lookup to procure a record from an authoritative nameserver (often an A record containing an IP address). If it cannot find the domain, returns the NXDOMAIN message.\nQuery Types Video: https://youtu.be/BZISxpdl4lQ\nThere are three types of queries in a DNS system:\nRecursive In a recursive query, a DNS client requires that a DNS server (typically a DNS recursive resolver) will respond to the client with either the requested resource record or an error message if the resolver can\u0026rsquo;t find the record.\nIterative In an iterative query, a DNS client provides a hostname, and the DNS Resolver returns the best answer it can. If the DNS resolver has the relevant DNS records in its cache, it returns them. If not, it refers the DNS client to the Root Server or another Authoritative Name Server that is nearest to the required DNS zone. The DNS client must then repeat the query directly against the DNS server it was referred.\nNon-recursive A non-recursive query is a query in which the DNS Resolver already knows the answer. It either immediately returns a DNS record because it already stores it in a local cache, or queries a DNS Name Server which is authoritative for the record, meaning it definitely holds the correct IP for that hostname. In both cases, there is no need for additional rounds of queries (like in recursive or iterative queries). Rather, a response is immediately returned to the client.\nRecords Types DNS records (aka zone files) are instructions that live in authoritative DNS servers and provide information about a domain including what IP address is associated with that domain and how to handle requests for that domain.\nThese records consist of a series of text files written in what is known as DNS syntax. DNS syntax is just a string of characters used as commands that tell the DNS server what to do. All DNS records also have a \u0026ldquo;TTL\u0026rdquo;, which stands for time-to-live, and indicates how often a DNS server will refresh that record.\nThere are more record types but for now, let\u0026rsquo;s look at some of the most commonly used ones:\nA (Address record): This is the record that holds the IP address of a domain. AAAA (IP Version 6 Address record): The record that contains the IPv6 address for a domain (as opposed to A records, which stores the IPv4 address). CNAME (Canonical Name record): Forwards one domain or subdomain to another domain, does NOT provide an IP address. MX (Mail exchanger record): Directs mail to an email server. TXT (Text Record): This record lets an admin store text notes in the record. These records are often used for email security. NS (Name Server records): Stores the name server for a DNS entry. SOA (Start of Authority): Stores admin information about a domain. SRV (Service Location record): Specifies a port for specific services. PTR (Reverse-lookup Pointer records): Provides a domain name in reverse lookups. CERT (Certificate record): Stores public key certificates. Subdomains A subdomain is an additional part of our main domain name. It is commonly used to logically separate a website into sections. We can create multiple subdomains or child domains on the main domain.\nFor example, blog.example.com where blog is the subdomain, example is the primary domain and .com is the top-level domain (TLD). Similar examples can be support.example.com or careers.example.com.\nDNS Zones A DNS zone is a distinct part of the domain namespace which is delegated to a legal entity like a person, organization, or company, who is responsible for maintaining the DNS zone. A DNS zone is also an administrative function, allowing for granular control of DNS components, such as authoritative name servers.\nDNS Caching A DNS cache (sometimes called a DNS resolver cache) is a temporary database, maintained by a computer\u0026rsquo;s operating system, that contains records of all the recent visits and attempted visits to websites and other internet domains. In other words, a DNS cache is just a memory of recent DNS lookups that our computer can quickly refer to when it\u0026rsquo;s trying to figure out how to load a website.\nThe Domain Name System implements a time-to-live (TTL) on every DNS record. TTL specifies the number of seconds the record can be cached by a DNS client or server. When the record is stored in a cache, whatever TTL value came with it gets stored as well. The server continues to update the TTL of the record stored in the cache, counting down every second. When it hits zero, the record is deleted or purged from the cache. At that point, if a query for that record is received, the DNS server has to start the resolution process.\nReverse DNS A reverse DNS lookup is a DNS query for the domain name associated with a given IP address. This accomplishes the opposite of the more commonly used forward DNS lookup, in which the DNS system is queried to return an IP address. The process of reverse resolving an IP address uses PTR records. If the server does not have a PTR record, it cannot resolve a reverse lookup.\nReverse lookups are commonly used by email servers. Email servers check and see if an email message came from a valid server before bringing it onto their network. Many email servers will reject messages from any server that does not support reverse lookups or from a server that is highly unlikely to be legitimate.\nNote: Reverse DNS lookups are not universally adopted as they are not critical to the normal function of the internet.\nExamples These are some widely used managed DNS solutions:\nRoute53 Cloudflare DNS Google Cloud DNS Azure DNS NS1 Load Balancing Load balancing lets us distribute incoming network traffic across multiple resources ensuring high availability and reliability by sending requests only to resources that are online. This provides the flexibility to add or subtract resources as demand dictates.\nFor additional scalability and redundancy, we can try to load balance at each layer of our system:\nBut why? Modern high-traffic websites must serve hundreds of thousands, if not millions, of concurrent requests from users or clients. To cost-effectively scale to meet these high volumes, modern computing best practice generally requires adding more servers.\nA load balancer can sit in front of the servers and route client requests across all servers capable of fulfilling those requests in a manner that maximizes speed and capacity utilization. This ensures that no single server is overworked, which could degrade performance. If a single server goes down, the load balancer redirects traffic to the remaining online servers. When a new server is added to the server group, the load balancer automatically starts sending requests to it.\nWorkload distribution This is the core functionality provided by a load balancer and has several common variations:\nHost-based: Distributes requests based on the requested hostname. Path-based: Using the entire URL to distribute requests as opposed to just the hostname. Content-based: Inspects the message content of a request. This allows distribution based on content such as the value of a parameter. Layers Generally speaking, load balancers operate at one of the two levels:\nNetwork layer This is the load balancer that works at the network\u0026rsquo;s transport layer, also known as layer 4. This performs routing based on networking information such as IP addresses and is not able to perform content-based routing. These are often dedicated hardware devices that can operate at high speed.\nApplication layer This is the load balancer that operates at the application layer, also known as layer 7. Load balancers can read requests in their entirety and perform content-based routing. This allows the management of load based on a full understanding of traffic.\nTypes Let\u0026rsquo;s look at different types of load balancers:\nSoftware Software load balancers usually are easier to deploy than hardware versions. They also tend to be more cost-effective and flexible, and they are used in conjunction with software development environments. The software approach gives us the flexibility of configuring the load balancer to our environment\u0026rsquo;s specific needs. The boost in flexibility may come at the cost of having to do more work to set up the load balancer. Compared to hardware versions, which offer more of a closed-box approach, software balancers give us more freedom to make changes and upgrades.\nSoftware load balancers are widely used and are available either as installable solutions that require configuration and management or as a managed cloud service.\nHardware As the name implies, a hardware load balancer relies on physical, on-premises hardware to distribute application and network traffic. These devices can handle a large volume of traffic but often carry a hefty price tag and are fairly limited in terms of flexibility.\nHardware load balancers include proprietary firmware that requires maintenance and updates as new versions and security patches are released.\nDNS DNS load balancing is the practice of configuring a domain in the Domain Name System (DNS) such that client requests to the domain are distributed across a group of server machines.\nUnfortunately, DNS load balancing has inherent problems limiting its reliability and efficiency. Most significantly, DNS does not check for server and network outages, or errors. It always returns the same set of IP addresses for a domain even if servers are down or inaccessible.\nRouting Algorithms Now, let\u0026rsquo;s discuss commonly used routing algorithms:\nRound-robin: Requests are distributed to application servers in rotation. Weighted Round-robin: Builds on the simple Round-robin technique to account for differing server characteristics such as compute and traffic handling capacity using weights that can be assigned via DNS records by the administrator. Least Connections: A new request is sent to the server with the fewest current connections to clients. The relative computing capacity of each server is factored into determining which one has the least connections. Least Response Time: Sends requests to the server selected by a formula that combines the fastest response time and fewest active connections. Least Bandwidth: This method measures traffic in megabits per second (Mbps), sending client requests to the server with the least Mbps of traffic. Hashing: Distributes requests based on a key we define, such as the client IP address or the request URL. Advantages Load balancing also plays a key role in preventing downtime, other advantages of load balancing include the following:\nScalability Redundancy Flexibility Efficiency Redundant load balancers As you must\u0026rsquo;ve already guessed, the load balancer itself can be a single point of failure. To overcome this, a second or N number of load balancers can be used in a cluster mode.\nAnd, if there\u0026rsquo;s a failure detection and the active load balancer fails, another passive load balancer can take over which will make our system more fault-tolerant.\nFeatures Here are some commonly desired features of load balancers:\nAutoscaling: Starting up and shutting down resources in response to demand conditions. Sticky sessions: The ability to assign the same user or device to the same resource in order to maintain the session state on the resource. Healthchecks: The ability to determine if a resource is down or performing poorly in order to remove the resource from the load balancing pool. Persistence connections: Allowing a server to open a persistent connection with a client such as a WebSocket. Encryption: Handling encrypted connections such as TLS and SSL. Certificates: Presenting certificates to a client and authentication of client certificates. Compression: Compression of responses. Caching: An application-layer load balancer may offer the ability to cache responses. Logging: Logging of request and response metadata can serve as an important audit trail or source for analytics data. Request tracing: Assigning each request a unique id for the purposes of logging, monitoring, and troubleshooting. Redirects: The ability to redirect an incoming request based on factors such as the requested path. Fixed response: Returning a static response for a request such as an error message. Examples Following are some of the load balancing solutions commonly used in the industry:\nAmazon Elastic Load Balancing Azure Load Balancing GCP Load Balancing DigitalOcean Load Balancer Nginx HAProxy Clustering At a high level, a computer cluster is a group of two or more computers, or nodes, that run in parallel to achieve a common goal. This allows workloads consisting of a high number of individual, parallelizable tasks to be distributed among the nodes in the cluster. As a result, these tasks can leverage the combined memory and processing power of each computer to increase overall performance.\nTo build a computer cluster, the individual nodes should be connected to a network to enable internode communication. The software can then be used to join the nodes together and form a cluster. It may have a shared storage device and/or local storage on each node.\nTypically, at least one node is designated as the leader node and acts as the entry point to the cluster. The leader node may be responsible for delegating incoming work to the other nodes and, if necessary, aggregating the results and returning a response to the user.\nIdeally, a cluster functions as if it were a single system. A user accessing the cluster should not need to know whether the system is a cluster or an individual machine. Furthermore, a cluster should be designed to minimize latency and prevent bottlenecks in node-to-node communication.\nTypes Computer clusters can generally be categorized into three types:\nHighly available or fail-over Load balancing High-performance computing Configurations The two most commonly used high availability (HA) clustering configurations are active-active and active-passive.\nActive-Active An active-active cluster is typically made up of at least two nodes, both actively running the same kind of service simultaneously. The main purpose of an active-active cluster is to achieve load balancing. A load balancer distributes workloads across all nodes to prevent any single node from getting overloaded. Because there are more nodes available to serve, there will also be an improvement in throughput and response times.\nActive-Passive Like the active-active cluster configuration, an active-passive cluster also consists of at least two nodes. However, as the name active-passive implies, not all nodes are going to be active. For example, in the case of two nodes, if the first node is already active, then the second node must be passive or on standby.\nAdvantages Four key advantages of cluster computing are as follows:\nHigh availability Scalability Performance Cost-effective Load balancing vs Clustering Load balancing shares some common traits with clustering, but they are different processes. Clustering provides redundancy and boosts capacity and availability. Servers in a cluster are aware of each other and work together toward a common purpose. But with load balancing, servers are not aware of each other. Instead, they react to the requests they receive from the load balancer.\nWe can employ load balancing in conjunction with clustering but it also is applicable in cases involving independent servers that share a common purpose such as to run a website, business application, web service, or some other IT resource.\nChallenges The most obvious challenge clustering presents is the increased complexity of installation and maintenance. An operating system, the application, and its dependencies must each be installed and updated on every node.\nThis becomes even more complicated if the nodes in the cluster are not homogeneous. Resource utilization for each node must also be closely monitored, and logs should be aggregated to ensure that the software is behaving correctly.\nAdditionally, storage becomes more difficult to manage, a shared storage device must prevent nodes from overwriting one another and distributed data stores have to be kept in sync.\nExamples Clustering is commonly used in the industry, and often many technologies offer some sort of clustering mode. For example:\nContainers (eg. Kubernetes, Amazon ECS) Databases (eg. Cassandra, MongoDB) Cache (eg. Redis) Caching \u0026ldquo;There are only two hard things in Computer Science: cache invalidation and naming things.\u0026rdquo; - Phil Karlton\nA cache\u0026rsquo;s primary purpose is to increase data retrieval performance by reducing the need to access the underlying slower storage layer. Trading off capacity for speed, a cache typically stores a subset of data transiently, in contrast to databases whose data is usually complete and durable.\nCaches take advantage of the locality of reference principle \u0026ldquo;recently requested data is likely to be requested again\u0026rdquo;.\nCaching and Memory Similar to a computer\u0026rsquo;s memory, a cache is a compact, fast-performing memory that stores data in a hierarchy of levels, starting at level one, and progressing from there sequentially. They are labeled as L1, L2, L3, and so on. A cache also gets written if requested, such as when there has been an update and new content needs to be saved to the cache, replacing the older content that was saved.\nNo matter whether the cache is read or written, it\u0026rsquo;s done one block at a time. Each block also has a tag that includes the location where the data was stored in the cache. When data is requested from the cache, a search occurs through the tags to find the specific content that\u0026rsquo;s needed in level one (L1) of the memory. If the correct data isn\u0026rsquo;t found, more searches are conducted in L2.\nIf the data isn\u0026rsquo;t found there, searches are continued in L3, then L4, and so on until it has been found, then, it\u0026rsquo;s read and loaded. If the data isn\u0026rsquo;t found in the cache at all, then it\u0026rsquo;s written into it for quick retrieval the next time.\nCache hit and Cache miss Cache hit A cache hit describes the situation where content is successfully served from the cache. The tags are searched in the memory rapidly, and when the data is found and read, it\u0026rsquo;s considered a cache hit.\nCold, Warm, and Hot Caches\nA cache hit can also be described as cold, warm, or hot. In each of these, the speed at which the data is read is described.\nA hot cache is an instance where data was read from the memory at the fastest possible rate. This happens when the data is retrieved from L1.\nA cold cache is the slowest possible rate for data to be read, though, it\u0026rsquo;s still successful so it\u0026rsquo;s still considered a cache hit. The data is just found lower in the memory hierarchy such as in L3, or lower.\nA warm cache is used to describe data that\u0026rsquo;s found in L2 or L3. It\u0026rsquo;s not as fast as a hot cache, but it\u0026rsquo;s still faster than a cold cache. Generally, calling a cache warm is used to express that it\u0026rsquo;s slower and closer to a cold cache than a hot one.\nCache miss A cache miss refers to the instance when the memory is searched and the data isn\u0026rsquo;t found. When this happens, the content is transferred and written into the cache.\nCache Invalidation Cache invalidation is a process where the computer system declares the cache entries as invalid and removes or replaces them. If the data is modified, it should be invalidated in the cache, if not, this can cause inconsistent application behavior. There are three kinds of caching systems:\nWrite-through cache Data is written into the cache and the corresponding database simultaneously.\nPro: Fast retrieval, complete data consistency between cache and storage.\nCon: Higher latency for write operations.\nWrite-around cache Where write directly goes to the database or permanent storage, bypassing the cache.\nPro: This may reduce latency.\nCon: It increases cache misses because the cache system has to read the information from the database in case of a cache miss. As a result, this can lead to higher read latency in the case of applications that write and re-read the information quickly. Read happen from slower back-end storage and experiences higher latency.\nWrite-back cache Where the write is only done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the database.\nPro: This would lead to reduced latency and high throughput for write-intensive applications.\nCon: There is a risk of data loss in case the caching layer crashes. We can improve this by having more than one replica acknowledging the write in the cache.\nEviction policies Following are some of the most common cache eviction policies:\nFirst In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before. Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before. Least Recently Used (LRU): Discards the least recently used items first. Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first. Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first. Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. Distributed Cache A distributed cache is a system that pools together the random-access memory (RAM) of multiple networked computers into a single in-memory data store used as a data cache to provide fast access to data. While most caches are traditionally in one physical server or hardware component, a distributed cache can grow beyond the memory limits of a single computer by linking together multiple computers.\nGlobal Cache As the name suggests, we will have a single shared cache that all the application nodes will use. When the requested data is not found in the global cache, it\u0026rsquo;s the responsibility of the cache to find out the missing piece of data from the underlying data store.\nUse cases Caching can have many real-world use cases such as:\nDatabase Caching Content Delivery Network (CDN) Domain Name System (DNS) Caching API Caching When not to use caching?\nLet\u0026rsquo;s also look at some scenarios where we should not use cache:\nCaching isn\u0026rsquo;t helpful when it takes just as long to access the cache as it does to access the primary data store. Caching doesn\u0026rsquo;t work as well when requests have low repetition (higher randomness), because caching performance comes from repeated memory access patterns. Caching isn\u0026rsquo;t helpful when the data changes frequently, as the cached version gets out of sync, and the primary data store must be accessed every time. It\u0026rsquo;s important to note that a cache should not be used as permanent data storage. They are almost always implemented in volatile memory because it is faster, and thus should be considered transient.\nAdvantages Below are some advantages of caching:\nImproves performance Reduce latency Reduce load on the database Reduce network cost Increase Read Throughput Examples Here are some commonly used technologies for caching:\nRedis Memcached Amazon Elasticache Aerospike Content Delivery Network (CDN) A content delivery network (CDN) is a geographically distributed group of servers that work together to provide fast delivery of internet content. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN.\nWhy use a CDN? Content Delivery Network (CDN) increases content availability and redundancy while reducing bandwidth costs and improving security. Serving content from CDNs can significantly improve performance as users receive content from data centers close to them and our servers do not have to serve requests that the CDN fulfills.\nHow does a CDN work? In a CDN, the origin server contains the original versions of the content while the edge servers are numerous and distributed across various locations around the world.\nTo minimize the distance between the visitors and the website\u0026rsquo;s server, a CDN stores a cached version of its content in multiple geographical locations known as edge locations. Each edge location contains a number of caching servers responsible for content delivery to visitors within its proximity.\nOnce the static assets are cached on all the CDN servers for a particular location, all subsequent website visitor requests for static assets will be delivered from these edge servers instead of the origin, thus reducing origin load and improving scalability.\nFor example, when someone in the UK requests our website which might be hosted in the USA, they will be served from the closest edge location such as the London edge location. This is much quicker than having the visitor make a complete request to the origin server which will increase the latency.\nTypes CDNs are generally divided into two types:\nPush CDNs Push CDNs receive new content whenever changes occur on the server. We take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. We can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.\nSites with a small amount of traffic or sites with content that isn\u0026rsquo;t often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals.\nPull CDNs In a Pull CDN situation, the cache is updated based on request. When the client sends a request that requires static assets to be fetched from the CDN if the CDN doesn\u0026rsquo;t have it, then it will fetch the newly updated assets from the origin server and populate its cache with this new asset, and then send this new cached asset to the user.\nContrary to the Push CDN, this requires less maintenance because cache updates on CDN nodes are performed based on requests from the client to the origin server. Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.\nDisadvantages As we all know good things come with extra costs, so let\u0026rsquo;s discuss some disadvantages of CDNs:\nExtra charges: It can be expensive to use a CDN, especially for high-traffic services. Restrictions: Some organizations and countries have blocked the domains or IP addresses of popular CDNs. Location: If most of our audience is located in a country where the CDN has no servers, the data on our website may have to travel further than without using any CDN. Examples Here are some widely used CDNs:\nAmazon CloudFront Google Cloud CDN Cloudflare CDN Fastly Proxy A proxy server is an intermediary piece of hardware/software sitting between the client and the backend server. It receives requests from clients and relays them to the origin servers. Typically, proxies are used to filter requests, log requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or compression).\nTypes There are two types of proxies:\nForward Proxy A forward proxy, often called a proxy, proxy server, or web proxy is a server that sits in front of a group of client machines. When those computers make requests to sites and services on the internet, the proxy server intercepts those requests and then communicates with web servers on behalf of those clients, like a middleman.\nAdvantages\nHere are some advantages of a forward proxy:\nBlock access to certain content Allows access to geo-restricted content Provides anonymity Avoid other browsing restrictions Although proxies provide the benefits of anonymity, they can still track our personal information. Setup and maintenance of a proxy server can be costly and requires configurations.\nReverse Proxy A reverse proxy is a server that sits in front of one or more web servers, intercepting requests from clients. When clients send requests to the origin server of a website, those requests are intercepted by the reverse proxy server.\nThe difference between a forward and reverse proxy is subtle but important. A simplified way to sum it up would be to say that a forward proxy sits in front of a client and ensures that no origin server ever communicates directly with that specific client. On the other hand, a reverse proxy sits in front of an origin server and ensures that no client ever communicates directly with that origin server.\nIntroducing reverse proxy results in increased complexity. A single reverse proxy is a single point of failure, configuring multiple reverse proxies (i.e. a failover) further increases complexity.\nAdvantages\nHere are some advantages of using a reverse proxy:\nImproved security Caching SSL encryption Load balancing Scalability and flexibility Load balancer vs Reverse Proxy Wait, isn\u0026rsquo;t reverse proxy similar to a load balancer? Well, no as a load balancer is useful when we have multiple servers. Often, load balancers route traffic to a set of servers serving the same function, while, reverse proxies can be useful even with just one web server or application server. A reverse proxy can also act as a load balancer but not the other way around.\nExamples Below are some commonly used proxy technologies:\nNginx HAProxy Traefik Envoy Availability Availability is the time a system remains operational to perform its required function in a specific period. It is a simple measure of the percentage of time that a system, service, or machine remains operational under normal conditions.\nThe Nine\u0026rsquo;s of availability Availability is often quantified by uptime (or downtime) as a percentage of time the service is available. It is generally measured in the number of 9s.\n$$ Availability = \\frac{Uptime}{(Uptime + Downtime)} $$\nIf availability is 99.00% available, it is said to have \u0026ldquo;2 nines\u0026rdquo; of availability, and if it is 99.9%, it is called \u0026ldquo;3 nines\u0026rdquo;, and so on.\nAvailability (Percent) Downtime (Year) Downtime (Month) Downtime (Week) 90% (one nine) 36.53 days 72 hours 16.8 hours 99% (two nines) 3.65 days 7.20 hours 1.68 hours 99.9% (three nines) 8.77 hours 43.8 minutes 10.1 minutes 99.99% (four nines) 52.6 minutes 4.32 minutes 1.01 minutes 99.999% (five nines) 5.25 minutes 25.9 seconds 6.05 seconds 99.9999% (six nines) 31.56 seconds 2.59 seconds 604.8 milliseconds 99.99999% (seven nines) 3.15 seconds 263 milliseconds 60.5 milliseconds 99.999999% (eight nines) 315.6 milliseconds 26.3 milliseconds 6 milliseconds 99.9999999% (nine nines) 31.6 milliseconds 2.6 milliseconds 0.6 milliseconds Availability in Sequence vs Parallel If a service consists of multiple components prone to failure, the service\u0026rsquo;s overall availability depends on whether the components are in sequence or in parallel.\nSequence Overall availability decreases when two components are in sequence.\n$$ Availability \\space (Total) = Availability \\space (Foo) * Availability \\space (Bar) $$\nFor example, if both Foo and Bar each had 99.9% availability, their total availability in sequence would be 99.8%.\nParallel Overall availability increases when two components are in parallel.\n$$ Availability \\space (Total) = 1 - (1 - Availability \\space (Foo)) * (1 - Availability \\space (Bar)) $$\nFor example, if both Foo and Bar each had 99.9% availability, their total availability in parallel would be 99.9999%.\nAvailability vs Reliability If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. In other words, high reliability contributes to high availability, but it is possible to achieve high availability even with an unreliable system.\nHigh availability vs Fault Tolerance Both high availability and fault tolerance apply to methods for providing high uptime levels. However, they accomplish the objective differently.\nA fault-tolerant system has no service interruption but a significantly higher cost, while a highly available system has minimal service interruption. Fault-tolerance requires full hardware redundancy as if the main system fails, with no loss in uptime, another system should take over.\nScalability Scalability is the measure of how well a system responds to changes by adding or removing resources to meet demands.\nLet\u0026rsquo;s discuss different types of scaling:\nVertical scaling Vertical scaling (also known as scaling up) expands a system\u0026rsquo;s scalability by adding more power to an existing machine. In other words, vertical scaling refers to improving an application\u0026rsquo;s capability via increasing hardware capacity.\nAdvantages Simple to implement Easier to manage Data consistent Disadvantages Risk of high downtime Harder to upgrade Can be a single point of failure Horizontal scaling Horizontal scaling (also known as scaling out) expands a system\u0026rsquo;s scale by adding more machines. It improves the performance of the server by adding more instances to the existing pool of servers, allowing the load to be distributed more evenly.\nAdvantages Increased redundancy Better fault tolerance Flexible and efficient Easier to upgrade Disadvantages Increased complexity Data inconsistency Increased load on downstream services Storage Storage is a mechanism that enables a system to retain data, either temporarily or permanently. This topic is mostly skipped over in the context of system design, however, it is important to have a basic understanding of some common types of storage techniques that can help us fine-tune our storage components. Let\u0026rsquo;s discuss some important storage concepts:\nRAID RAID (Redundant Array of Independent Disks) is a way of storing the same data on multiple hard disks or solid-state drives (SSDs) to protect data in the case of a drive failure.\nThere are different RAID levels, however, and not all have the goal of providing redundancy. Let\u0026rsquo;s discuss some commonly used RAID levels:\nRAID 0: Also known as striping, data is split evenly across all the drives in the array. RAID 1: Also known as mirroring, at least two drives contains the exact copy of a set of data. If a drive fails, others will still work. RAID 5: Striping with parity. Requires the use of at least 3 drives, striping the data across multiple drives like RAID 0, but also has a parity distributed across the drives. RAID 6: Striping with double parity. RAID 6 is like RAID 5, but the parity data are written to two drives. RAID 10: Combines striping plus mirroring from RAID 0 and RAID 1. It provides security by mirroring all data on secondary drives while using striping across each set of drives to speed up data transfers. Comparison Let\u0026rsquo;s compare all the features of different RAID levels:\nFeatures RAID 0 RAID 1 RAID 5 RAID 6 RAID 10 Description Striping Mirroring Striping with Parity Striping with double parity Striping and Mirroring Minimum Disks 2 2 3 4 4 Read Performance High High High High High Write Performance High Medium High High Medium Cost Low High Low Low High Fault Tolerance None Single-drive failure Single-drive failure Two-drive failure Up to one disk failure in each sub-array Capacity Utilization 100% 50% 67%-94% 50%-80% 50% Volumes Volume is a fixed amount of storage on a disk or tape. The term volume is often used as a synonym for the storage itself, but it is possible for a single disk to contain more than one volume or a volume to span more than one disk.\nFile storage File storage is a solution to store data as files and present it to its final users as a hierarchical directories structure. The main advantage is to provide a user-friendly solution to store and retrieve files. To locate a file in file storage, the complete path of the file is required. It is economical and easily structured and is usually found on hard drives, which means that they appear exactly the same for the user and on the hard drive.\nExample: Amazon EFS, Azure files, Google Cloud Filestore, etc.\nBlock storage Block storage divides data into blocks (chunks) and stores them as separate pieces. Each block of data is given a unique identifier, which allows a storage system to place the smaller pieces of data wherever it is most convenient.\nBlock storage also decouples data from user environments, allowing that data to be spread across multiple environments. This creates multiple paths to the data and allows the user to retrieve it quickly. When a user or application requests data from a block storage system, the underlying storage system reassembles the data blocks and presents the data to the user or application\nExample: Amazon EBS.\nObject Storage Object storage, which is also known as object-based storage, breaks data files up into pieces called objects. It then stores those objects in a single repository, which can be spread out across multiple networked systems.\nExample: Amazon S3, Azure Blob Storage, Google Cloud Storage, etc.\nNAS A NAS (Network Attached Storage) is a storage device connected to a network that allows storage and retrieval of data from a central location for authorized network users. NAS devices are flexible, meaning that as we need additional storage, we can add to what we have. It\u0026rsquo;s faster, less expensive, and provides all the benefits of a public cloud on-site, giving us complete control.\nHDFS The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. It has many similarities with existing distributed file systems.\nHDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks, all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance.\nDatabases and DBMS What is a Database? A database is an organized collection of structured information, or data, typically stored electronically in a computer system. A database is usually controlled by a Database Management System (DBMS). Together, the data and the DBMS, along with the applications that are associated with them, are referred to as a database system, often shortened to just database.\nWhat is DBMS? A database typically requires a comprehensive database software program known as a Database Management System (DBMS). A DBMS serves as an interface between the database and its end-users or programs, allowing users to retrieve, update, and manage how the information is organized and optimized. A DBMS also facilitates oversight and control of databases, enabling a variety of administrative operations such as performance monitoring, tuning, and backup and recovery.\nComponents Here are some common components found across different databases:\nSchema The role of a schema is to define the shape of a data structure, and specify what kinds of data can go where. Schemas can be strictly enforced across the entire database, loosely enforced on part of the database, or they might not exist at all.\nTable Each table contains various columns just like in a spreadsheet. A table can have as meager as two columns and upwards of a hundred or more columns, depending upon the kind of information being put in the table.\nColumn A column contains a set of data values of a particular type, one value for each row of the database. A column may contain text values, numbers, enums, timestamps, etc.\nRow Data in a table is recorded in rows. There can be thousands or millions of rows in a table having any particular information.\nTypes Below are different types of databases:\nSQL NoSQL Document Key-value Graph Timeseries Wide column Multi-model SQL and NoSQL databases are broad topics and will be discussed separately in SQL databases and NoSQL databases. Learn how they compare to each other in SQL vs NoSQL databases.\nChallenges Some common challenges faced while running databases at scale:\nAbsorbing significant increases in data volume: The explosion of data coming in from sensors, connected machines, and dozens of other sources. Ensuring data security: Data breaches are happening everywhere these days, it\u0026rsquo;s more important than ever to ensure that data is secure but also easily accessible to users. Keeping up with demand: Companies need real-time access to their data to support timely decision-making and to take advantage of new opportunities. Managing and maintaining the database and infrastructure: As databases become more complex and data volumes grow, companies are faced with the expense of hiring additional talent to manage their databases. Removing limits on scalability: A business needs to grow if it\u0026rsquo;s going to survive, and its data management must grow along with it. But it\u0026rsquo;s very difficult to predict how much capacity the company will need, particularly with on-premises databases. Ensuring data residency, data sovereignty, or latency requirements: Some organizations have use cases that are better suited to run on-premises. In those cases, engineered systems that are pre-configured and pre-optimized for running the database are ideal. SQL databases A SQL (or relational) database is a collection of data items with pre-defined relationships between them. These items are organized as a set of tables with columns and rows. Tables are used to hold information about the objects to be represented in the database. Each column in a table holds a certain kind of data and a field stores the actual value of an attribute. The rows in the table represent a collection of related values of one object or entity.\nEach row in a table could be marked with a unique identifier called a primary key, and rows among multiple tables can be made related using foreign keys. This data can be accessed in many different ways without re-organizing the database tables themselves. SQL databases usually follow the ACID consistency model.\nMaterialized views A materialized view is a pre-computed data set derived from a query specification and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view. This performance difference can be significant when a query is run frequently or is sufficiently complex.\nIt also enables data subsetting and improves the performance of complex queries that run on large data sets which reduces network loads. There are other uses of materialized views, but they are mostly used for performance and replication.\nN+1 query problem The N+1 query problem happens when the data access layer executes N additional SQL statements to fetch the same data that could have been retrieved when executing the primary SQL query. The larger the value of N, the more queries will be executed, the larger the performance impact.\nThis is commonly seen in GraphQL and ORM (Object-Relational Mapping) tools and can be addressed by optimizing the SQL query or using a dataloader that batches consecutive requests and makes a single data request under the hood.\nAdvantages Let\u0026rsquo;s look at some advantages of using relational databases:\nSimple and accurate Accessibility Data consistency Flexibility Disadvantages Below are the disadvantages of relational databases:\nExpensive to maintain Difficult schema evolution Performance hits (join, denormalization, etc.) Difficult to scale due to poor horizontal scalability Examples Here are some commonly used relational databases:\nPostgreSQL MySQL MariaDB Amazon Aurora NoSQL databases NoSQL is a broad category that includes any database that doesn\u0026rsquo;t use SQL as its primary data access language. These types of databases are also sometimes referred to as non-relational databases. Unlike in relational databases, data in a NoSQL database doesn\u0026rsquo;t have to conform to a pre-defined schema. NoSQL databases follow BASE consistency model.\nBelow are different types of NoSQL databases:\nDocument A document database (also known as a document-oriented database or a document store) is a database that stores information in documents. They are general-purpose databases that serve a variety of use cases for both transactional and analytical applications.\nAdvantages\nIntuitive and flexible Easy horizontal scaling Schemaless Disadvantages\nSchemaless Non-relational Examples\nMongoDB Amazon DocumentDB CouchDB Key-value One of the simplest types of NoSQL databases, key-value databases save data as a group of key-value pairs made up of two data items each. They\u0026rsquo;re also sometimes referred to as a key-value store.\nAdvantages\nSimple and performant Highly scalable for high volumes of traffic Session management Optimized lookups Disadvantages\nBasic CRUD Values can\u0026rsquo;t be filtered Lacks indexing and scanning capabilities Not optimized for complex queries Examples\nRedis Memcached Amazon DynamoDB Aerospike Graph A graph database is a NoSQL database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data instead of tables or documents.\nThe graph relates the data items in the store to a collection of nodes and edges, the edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly and, in many cases, retrieved with one operation.\nAdvantages\nQuery speed Agile and flexible Explicit data representation Disadvantages\nComplex No standardized query language Use cases\nFraud detection Recommendation engines Social networks Network mapping Examples\nNeo4j ArangoDB Amazon Neptune JanusGraph Time series A time-series database is a database optimized for time-stamped, or time series, data.\nAdvantages\nFast insertion and retrieval Efficient data storage Use cases\nIoT data Metrics analysis Application monitoring Understand financial trends Examples\nInfluxDB Apache Druid Wide column Wide column databases, also known as wide column stores, are schema-agnostic. Data is stored in column families, rather than in rows and columns.\nAdvantages\nHighly scalable, can handle petabytes of data Ideal for real-time big data applications Disadvantages\nExpensive Increased write time Use cases\nBusiness analytics Attribute-based data storage Examples\nBigTable Apache Cassandra ScyllaDB Multi-model Multi-model databases combine different database models (i.e. relational, graph, key-value, document, etc.) into a single, integrated backend. This means they can accommodate various data types, indexes, queries, and store data in more than one model.\nAdvantages\nFlexibility Suitable for complex projects Data consistent Disadvantages\nComplex Less mature Examples\nArangoDB Azure Cosmos DB Couchbase SQL vs NoSQL databases In the world of databases, there are two main types of solutions, SQL (relational) and NoSQL (non-relational) databases. Both of them differ in the way they were built, the kind of information they store, and how they store it. Relational databases are structured and have predefined schemas while non-relational databases are unstructured, distributed, and have a dynamic schema.\nHigh-level differences Here are some high-level differences between SQL and NoSQL:\nStorage SQL stores data in tables, where each row represents an entity and each column represents a data point about that entity.\nNoSQL databases have different data storage models such as key-value, graph, document, etc.\nSchema In SQL, each record conforms to a fixed schema, meaning the columns must be decided and chosen before data entry and each row must have data for each column. The schema can be altered later, but it involves modifying the database using migrations.\nWhereas in NoSQL, schemas are dynamic. Fields can be added on the fly, and each record (or equivalent) doesn\u0026rsquo;t have to contain data for each field.\nQuerying SQL databases use SQL (structured query language) for defining and manipulating the data, which is very powerful.\nIn a NoSQL database, queries are focused on a collection of documents. Different databases have different syntax for querying.\nScalability In most common situations, SQL databases are vertically scalable, which can get very expensive. It is possible to scale a relational database across multiple servers, but this is a challenging and time-consuming process.\nOn the other hand, NoSQL databases are horizontally scalable, meaning we can add more servers easily to our NoSQL database infrastructure to handle large traffic. Any cheap commodity hardware or cloud instances can host NoSQL databases, thus making it a lot more cost-effective than vertical scaling. A lot of NoSQL technologies also distribute data across servers automatically.\nReliability The vast majority of relational databases are ACID compliant. So, when it comes to data reliability and a safe guarantee of performing transactions, SQL databases are still the better bet.\nMost of the NoSQL solutions sacrifice ACID compliance for performance and scalability.\nReasons As always we should always pick the technology that fits the requirements better. So, let\u0026rsquo;s look at some reasons for picking SQL or NoSQL based database:\nFor SQL\nStructured data with strict schema Relational data Need for complex joins Transactions Lookups by index are very fast For NoSQL\nDynamic or flexible schema Non-relational data No need for complex joins Very data-intensive workload Very high throughput for IOPS Database Replication Replication is a process that involves sharing information to ensure consistency between redundant resources such as multiple databases, to improve reliability, fault-tolerance, or accessibility.\nMaster-Slave Replication The master serves reads and writes, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.\nAdvantages Backups of the entire database of relatively no impact on the master. Applications can read from the slave(s) without impacting the master. Slaves can be taken offline and synced back to the master without any downtime. Disadvantages Replication adds more hardware and additional complexity. Downtime and possibly loss of data when a master fails. All writes also have to be made to the master in a master-slave architecture. The more read slaves, the more we have to replicate, which will increase replication lag. Master-Master Replication Both masters serve reads/writes and coordinate with each other. If either master goes down, the system can continue to operate with both reads and writes.\nAdvantages Applications can read from both masters. Distributes write load across both master nodes. Simple, automatic, and quick failover. Disadvantages Not as simple as master-slave to configure and deploy. Either loosely consistent or have increased write latency due to synchronization. Conflict resolution comes into play as more write nodes are added and as latency increases. Synchronous vs Asynchronous replication The primary difference between synchronous and asynchronous replication is how the data is written to the replica. In synchronous replication, data is written to primary storage and the replica simultaneously. As such, the primary copy and the replica should always remain synchronized.\nIn contrast, asynchronous replication copies the data to the replica after the data is already written to the primary storage. Although the replication process may occur in near-real-time, it is more common for replication to occur on a scheduled basis and it is more cost-effective.\nIndexes Indexes are well known when it comes to databases, they are used to improve the speed of data retrieval operations on the data store. An index makes the trade-offs of increased storage overhead, and slower writes (since we not only have to write the data but also have to update the index) for the benefit of faster reads. Indexes are used to quickly locate data without having to examine every row in a database table. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access to ordered records.\nAn index is a data structure that can be perceived as a table of contents that points us to the location where actual data lives. So when we create an index on a column of a table, we store that column and a pointer to the whole row in the index. Indexes are also used to create different views of the same data. For large data sets, this is an excellent way to specify different filters or sorting schemes without resorting to creating multiple additional copies of the data.\nOne quality that database indexes can have is that they can be dense or sparse. Each of these index qualities comes with its own trade-offs. Let\u0026rsquo;s look at how each index type would work:\nDense Index In a dense index, an index record is created for every row of the table. Records can be located directly as each record of the index holds the search key value and the pointer to the actual record.\nDense indexes require more maintenance than sparse indexes at write-time. Since every row must have an entry, the database must maintain the index on inserts, updates, and deletes. Having an entry for every row also means that dense indexes will require more memory. The benefit of a dense index is that values can be quickly found with just a binary search. Dense indexes also do not impose any ordering requirements on the data.\nSparse Index In a sparse index, records are created only for some of the records.\nSparse indexes require less maintenance than dense indexes at write-time since they only contain a subset of the values. This lighter maintenance burden means that inserts, updates, and deletes will be faster. Having fewer entries also means that the index will use less memory. Finding data is slower since a scan across the page typically follows the binary search. Sparse indexes are also optional when working with ordered data.\nNormalization and Denormalization Terms Before we go any further, let\u0026rsquo;s look at some commonly used terms in normalization and denormalization.\nKeys Primary key: Column or group of columns that can be used to uniquely identify every row of the table.\nComposite key: A primary key made up of multiple columns.\nSuper key: Set of all keys that can uniquely identify all the rows present in a table.\nCandidate key: Attributes that identify rows uniquely in a table.\nForeign key: It is a reference to a primary key of another table.\nAlternate key: Keys that are not primary keys are known as alternate keys.\nSurrogate key: A system-generated value that uniquely identifies each entry in a table when no other column was able to hold properties of a primary key.\nDependencies Partial dependency: Occurs when the primary key determines some other attributes.\nFunctional dependency: It is a relationship that exists between two attributes, typically between the primary key and non-key attribute within a table.\nTransitive functional dependency: Occurs when some non-key attribute determines some other attribute.\nAnomalies Database anomaly happens when there is a flaw in the database due to incorrect planning or storing everything in a flat database. This is generally addressed by the process of normalization.\nThere are three types of database anomalies:\nInsertion anomaly: Occurs when we are not able to insert certain attributes in the database without the presence of other attributes.\nUpdate anomaly: Occurs in case of data redundancy and partial update. In other words, a correct update of the database needs other actions such as addition, deletion, or both.\nDeletion anomaly: Occurs where deletion of some data requires deletion of other data.\nExample\nLet\u0026rsquo;s consider the following table which is not normalized:\nID Name Role Team 1 Peter Software Engineer A 2 Brian DevOps Engineer B 3 Hailey Product Manager C 4 Hailey Product Manager C 5 Steve Frontend Engineer D Let\u0026rsquo;s imagine, we hired a new person \u0026ldquo;John\u0026rdquo; but they might not be assigned a team immediately. This will cause an insertion anomaly as the team attribute is not yet present.\nNext, let\u0026rsquo;s say Hailey from Team C got promoted, to reflect that change in the database, we will need to update 2 rows to maintain consistency which can cause an update anomaly.\nFinally, we would like to remove Team B but to do that we will also need to remove additional information such as name and role, this is an example of a deletion anomaly.\nNormalization Normalization is the process of organizing data in a database. This includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating redundancy and inconsistent dependency.\nWhy do we need normalization? The goal of normalization is to eliminate redundant data and ensure data is consistent. A fully normalized database allows its structure to be extended to accommodate new types of data without changing the existing structure too much. As a result, applications interacting with the database are minimally affected.\nNormal forms Normal forms are a series of guidelines to ensure that the database is normalized. Let\u0026rsquo;s discuss some essential normal forms:\n1NF\nFor a table to be in the first normal form (1NF), it should follow the following rules:\nRepeating groups are not permitted. Identify each set of related data with a primary key. Set of related data should have a separate table. Mixing data types in the same column is not permitted. 2NF\nFor a table to be in the second normal form (2NF), it should follow the following rules:\nSatisfies the first normal form (1NF). Should not have any partial dependency. 3NF\nFor a table to be in the third normal form (3NF), it should follow the following rules:\nSatisfies the second normal form (2NF). Transitive functional dependencies are not permitted. BCNF\nBoyce-Codd normal form (or BCNF) is a slightly stronger version of the third normal form (3NF) used to address certain types of anomalies not dealt with by 3NF as originally defined. Sometimes it is also known as the 3.5 normal form (3.5NF).\nFor a table to be in the Boyce-Codd normal form (BCNF), it should follow the following rules:\nSatisfied the third normal form (3NF). For every functional dependency X → Y, X should be the super key. There are more normal forms such as 4NF, 5NF, and 6NF but we won\u0026rsquo;t discuss them here. Check out this amazing video that goes into detail.\nIn a relational database, a relation is often described as \u0026ldquo;normalized\u0026rdquo; if it meets the third normal form. Most 3NF relations are free of insertion, update, and deletion anomalies.\nAs with many formal rules and specifications, real-world scenarios do not always allow for perfect compliance. If you decide to violate one of the first three rules of normalization, make sure that your application anticipates any problems that could occur, such as redundant data and inconsistent dependencies.\nAdvantages Here are some advantages of normalization:\nReduces data redundancy. Better data design. Increases data consistency. Enforces referential integrity. Disadvantages Let\u0026rsquo;s look at some disadvantages of normalization:\nData design is complex. Slower performance. Maintenance overhead. Require more joins. Denormalization Denormalization is a database optimization technique in which we add redundant data to one or more tables. This can help us avoid costly joins in a relational database. It attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins.\nOnce data becomes distributed with techniques such as federation and sharding, managing joins across the network further increases complexity. Denormalization might circumvent the need for such complex joins.\nNote: Denormalization does not mean reversing normalization.\nAdvantages Let\u0026rsquo;s look at some advantages of denormalization:\nRetrieving data is faster. Writing queries is easier. Reduction in number of tables. Convenient to manage. Disadvantages Below are some disadvantages of denormalization:\nExpensive inserts and updates. Increases complexity of database design. Increases data redundancy. More chances of data inconsistency. ACID and BASE consistency models Let\u0026rsquo;s discuss the ACID and BASE consistency models.\nACID The term ACID stands for Atomicity, Consistency, Isolation, and Durability. ACID properties are used for maintaining data integrity during transaction processing.\nIn order to maintain consistency before and after a transaction relational databases follow ACID properties. Let us understand these terms:\nAtomic All operations in a transaction succeed or every operation is rolled back.\nConsistent On the completion of a transaction, the database is structurally sound.\nIsolated Transactions do not contend with one another. Contentious access to data is moderated by the database so that transactions appear to run sequentially.\nDurable Once the transaction has been completed and the writes and updates have been written to the disk, it will remain in the system even if a system failure occurs.\nBASE With the increasing amount of data and high availability requirements, the approach to database design has also changed dramatically. To increase the ability to scale and at the same time be highly available, we move the logic from the database to separate servers. In this way, the database becomes more independent and focused on the actual process of storing data.\nIn the NoSQL database world, ACID transactions are less common as some databases have loosened the requirements for immediate consistency, data freshness, and accuracy in order to gain other benefits, like scale and resilience.\nBASE properties are much looser than ACID guarantees, but there isn\u0026rsquo;t a direct one-for-one mapping between the two consistency models. Let us understand these terms:\nBasic Availability The database appears to work most of the time.\nSoft-state Stores don\u0026rsquo;t have to be write-consistent, nor do different replicas have to be mutually consistent all the time.\nEventual consistency The data might not be consistent immediately but eventually, it becomes consistent. Reads in the system are still possible even though they may not give the correct response due to inconsistency.\nACID vs BASE Trade-offs There\u0026rsquo;s no right answer to whether our application needs an ACID or a BASE consistency model. Both the models have been designed to satisfy different requirements. While choosing a database we need to keep the properties of both the models and the requirements of our application in mind.\nGiven BASE\u0026rsquo;s loose consistency, developers need to be more knowledgeable and rigorous about consistent data if they choose a BASE store for their application. It\u0026rsquo;s essential to be familiar with the BASE behavior of the chosen database and work within those constraints.\nOn the other hand, planning around BASE limitations can sometimes be a major disadvantage when compared to the simplicity of ACID transactions. A fully ACID database is the perfect fit for use cases where data reliability and consistency are essential.\nCAP Theorem Video: https://youtu.be/8UryASGBiR4\nCAP theorem states that a distributed system can deliver only two of the three desired characteristics Consistency, Availability, and Partition tolerance (CAP).\nConsistency In a consistent system, all nodes see the same data simultaneously. If we perform a read operation on a consistent system, it should return the value of the most recent write operation. The read should cause all nodes to return the same data. All users see the same data at the same time, regardless of the node they connect to. When data is written to a single node, it is then replicated across the other nodes in the system. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated across all the nodes in the system before the write is deemed \u0026ldquo;successful\u0026rdquo;.\nFinancial data is a good example. When a user logs in to their banking institution, they do not want to see an error that no data is returned, or that the value is higher or lower than it actually is. Banking apps should return the exact value of a user’s account information. In this case, banks would rely on consistent databases.\nExamples of a consistent database include:\nBank account balances Text messages Database options for consistency:\nMongoDB Redis HBase Availability When availability is present in a distributed system, it means that the system remains operational all of the time. Every request will get a response regardless of the individual state of the nodes. This means that the system will operate even if there are multiple nodes down. Unlike a consistent system, there’s no guarantee that the response will be the most recent write operation.\nExample of a highly available database:\nOn YouTube and social media like Facebook and Instagram, we can ignore consistency in views or likes count but the availability of videos and posts is essential. In e-commerce businesses. Online stores want to make their store and the functions of the shopping cart available 24/7 so shoppers can make purchases exactly when they need. Database options for availability:\nCassandra DynamoDB Cosmos DB Partition tolerance When a distributed system encounters a partition, it means that there’s a break in communication between nodes. If a system is partition-tolerant, the system does not fail, regardless of whether messages are dropped or delayed between nodes within the system. To have partition tolerance, the system must replicate records across combinations of nodes and networks.\nCAP theorem NoSQL databases NoSQL databases can be classified based on whether they support high availability or high consistency. NoSQL databases are great for distributed networks. They allow for horizontal scaling, and they can quickly scale across multiple nodes. When deciding which NoSQL database to use, it’s important to keep the CAP theorem in mind. NoSQL databases can be classified based on the two CAP features they support.\nConsistency-Availability Tradeoff We live in a physical world and can\u0026rsquo;t guarantee the stability of a network, so distributed databases must choose Partition Tolerance (P). This implies a tradeoff between Consistency (C) and Availability (A).\nCA database Relational databases, such as PostgreSQL, allow for consistency and availability if the systems are vertically scale on a single machine, we can avoid fault tolerance. A CA database delivers consistency and availability across all nodes. It can\u0026rsquo;t do this if there is a partition between any two nodes in the system, and therefore can\u0026rsquo;t deliver fault tolerance.\nExample: PostgreSQL, MariaDB.\nCP database CP databases enable consistency and partition tolerance, but not availability. When a partition occurs, the system has to turn off inconsistent nodes until the partition can be fixed. That\u0026rsquo;s why they are not 100% available. MongoDB is an example of a CP database. It’s a NoSQL database management system (DBMS) that uses documents for data storage. It’s considered schema-less, which means that it doesn’t require a defined database schema. It’s commonly used in big data and applications running in different locations. The CP system is structured so that there’s only one primary node that receives all of the write requests in a given replica set. Secondary nodes replicate the data in the primary nodes, so if the primary node fails, a secondary node can stand-in. Example: MongoDB, Apache HBase.\nAP database AP databases enable availability and partition tolerance, but not consistency. In the event of a partition, all nodes are available, but they’re not all updated. For example, if a user tries to access data from a bad node, they won’t receive the most up-to-date version of the data. When the partition is eventually resolved, most AP databases will sync the nodes to ensure consistency across them. Apache Cassandra is an example of an AP database. It’s a NoSQL database with no primary node, meaning that all of the nodes remain available. Cassandra allows for eventual consistency because users can resync their data right after a partition is resolved.\nExample: Apache Cassandra, CouchDB.\nCAP theorem and microservices Microservices are defined as loosely coupled services that can be independently developed, deployed, and maintained. They include their own stack, database, and database model, and communicate with each other through a network. Microservices have become especially popular in hybrid cloud and multi-cloud environments, and they are also widely used in on-premises data centers. If you want to create a microservices application, you can use the CAP theorem to help you determine a database that will best fit your needs.\nPACELC Theorem The PACELC theorem is an extension of the CAP theorem. The CAP theorem states that in the case of network partitioning (P) in a distributed system, one has to choose between Availability (A) and Consistency (C).\nPACELC extends the CAP theorem by introducing latency (L) as an additional attribute of a distributed system. The theorem states that else (E), even when the system is running normally in the absence of partitions, one has to choose between latency (L) and consistency (C).\nThe PACELC theorem was first described by Daniel J. Abadi.\nPACELC theorem was developed to address a key limitation of the CAP theorem as it makes no provision for performance or latency.\nFor example, according to the CAP theorem, a database can be considered Available if a query returns a response after 30 days. Obviously, such latency would be unacceptable for any real-world application.\nTransactions A transaction is a series of database operations that are considered to be a \u0026ldquo;single unit of work\u0026rdquo;. The operations in a transaction either all succeed, or they all fail. In this way, the notion of a transaction supports data integrity when part of a system fails. Not all databases choose to support ACID transactions, usually because they are prioritizing other optimizations that are hard or theoretically impossible to implement together.\nUsually, relational databases support ACID transactions, and non-relational databases don\u0026rsquo;t (there are exceptions).\nStates A transaction in a database can be in one of the following states:\nActive In this state, the transaction is being executed. This is the initial state of every transaction.\nPartially Committed When a transaction executes its final operation, it is said to be in a partially committed state.\nCommitted If a transaction executes all its operations successfully, it is said to be committed. All its effects are now permanently established on the database system.\nFailed The transaction is said to be in a failed state if any of the checks made by the database recovery system fails. A failed transaction can no longer proceed further.\nAborted If any of the checks fail and the transaction has reached a failed state, then the recovery manager rolls back all its write operations on the database to bring the database back to its original state where it was prior to the execution of the transaction. Transactions in this state are aborted.\nThe database recovery module can select one of the two operations after a transaction aborts:\nRestart the transaction Kill the transaction Terminated If there isn\u0026rsquo;t any roll-back or the transaction comes from the committed state, then the system is consistent and ready for a new transaction and the old transaction is terminated.\nDistributed Transactions A distributed transaction is a set of operations on data that is performed across two or more databases. It is typically coordinated across separate nodes connected by a network, but may also span multiple databases on a single server.\nWhy do we need distributed transactions? Unlike an ACID transaction on a single database, a distributed transaction involves altering data on multiple databases. Consequently, distributed transaction processing is more complicated, because the database must coordinate the committing or rollback of the changes in a transaction as a self-contained unit.\nIn other words, all the nodes must commit, or all must abort and the entire transaction rolls back. This is why we need distributed transactions.\nNow, let\u0026rsquo;s look at some popular solutions for distributed transactions:\nTwo-Phase commit The two-phase commit (2PC) protocol is a distributed algorithm that coordinates all the processes that participate in a distributed transaction on whether to commit or abort (roll back) the transaction.\nThis protocol achieves its goal even in many cases of temporary system failure and is thus widely used. However, it is not resilient to all possible failure configurations, and in rare cases, manual intervention is needed to remedy an outcome.\nThis protocol requires a coordinator node, which basically coordinates and oversees the transaction across different nodes. The coordinator tries to establish the consensus among a set of processes in two phases, hence the name.\nPhases Two-phase commit consists of the following phases:\nPrepare phase\nThe prepare phase involves the coordinator node collecting consensus from each of the participant nodes. The transaction will be aborted unless each of the nodes responds that they\u0026rsquo;re prepared.\nCommit phase\nIf all participants respond to the coordinator that they are prepared, then the coordinator asks all the nodes to commit the transaction. If a failure occurs, the transaction will be rolled back.\nProblems Following problems may arise in the two-phase commit protocol:\nWhat if one of the nodes crashes? What if the coordinator itself crashes? It is a blocking protocol. Three-phase commit Three-phase commit (3PC) is an extension of the two-phase commit where the commit phase is split into two phases. This helps with the blocking problem that occurs in the two-phase commit protocol.\nPhases Three-phase commit consists of the following phases:\nPrepare phase\nThis phase is the same as the two-phase commit.\nPre-commit phase\nCoordinator issues the pre-commit message and all the participating nodes must acknowledge it. If a participant fails to receive this message in time, then the transaction is aborted.\nCommit phase\nThis step is also similar to the two-phase commit protocol.\nWhy is the Pre-commit phase helpful? The pre-commit phase accomplishes the following:\nIf the participant nodes are found in this phase, that means that every participant has completed the first phase. The completion of prepare phase is guaranteed. Every phase can now time out and avoid indefinite waits. Sagas A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.\nCoordination There are two common implementation approaches:\nChoreography: Each local transaction publishes domain events that trigger local transactions in other services. Orchestration: An orchestrator tells the participants what local transactions to execute. Problems The Saga pattern is particularly hard to debug. There\u0026rsquo;s a risk of cyclic dependency between saga participants. Lack of participant data isolation imposes durability challenges. Testing is difficult because all services must be running to simulate a transaction. Sharding Before we discuss sharding, let\u0026rsquo;s talk about data partitioning:\nData Partitioning Data partitioning is a technique to break up a database into many smaller parts. It is the process of splitting up a database or a table across multiple machines to improve the manageability, performance, and availability of a database.\nMethods There are many different ways one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular methods used by various large-scale applications:\nHorizontal Partitioning (or Sharding)\nIn this strategy, we split the table data horizontally based on the range of values defined by the partition key. It is also referred to as database sharding.\nVertical Partitioning\nIn vertical partitioning, we partition the data vertically based on columns. We divide tables into relatively smaller tables with few elements, and each part is present in a separate partition.\nIn this tutorial, we will specifically focus on sharding.\nWhat is sharding? Sharding is a database architecture pattern related to horizontal partitioning, which is the practice of separating one table\u0026rsquo;s rows into multiple different tables, known as partitions or shards. Each partition has the same schema and columns, but also a subset of the shared data. Likewise, the data held in each is unique and independent of the data held in other partitions.\nThe justification for data sharding is that, after a certain point, it is cheaper and more feasible to scale horizontally by adding more machines than to scale it vertically by adding powerful servers. Sharding can be implemented at both application or the database level.\nPartitioning criteria There are a large number of criteria available for data partitioning. Some most commonly used criteria are:\nHash-Based This strategy divides the rows into different partitions based on a hashing algorithm rather than grouping database rows based on continuous indexes.\nThe disadvantage of this method is that dynamically adding/removing database servers becomes expensive.\nList-Based In list-based partitioning, each partition is defined and selected based on the list of values on a column rather than a set of contiguous ranges of values.\nRange Based Range partitioning maps data to various partitions based on ranges of values of the partitioning key. In other words, we partition the table in such a way that each partition contains rows within a given range defined by the partition key.\nRanges should be contiguous but not overlapping, where each range specifies a non-inclusive lower and upper bound for a partition. Any partitioning key values equal to or higher than the upper bound of the range are added to the next partition.\nComposite As the name suggests, composite partitioning partitions the data based on two or more partitioning techniques. Here we first partition the data using one technique, and then each partition is further subdivided into sub-partitions using the same or some other method.\nAdvantages But why do we need sharding? Here are some advantages:\nAvailability: Provides logical independence to the partitioned database, ensuring the high availability of our application. Here individual partitions can be managed independently. Scalability: Proves to increase scalability by distributing the data across multiple partitions. Security: Helps improve the system\u0026rsquo;s security by storing sensitive and non-sensitive data in different partitions. This could provide better manageability and security to sensitive data. Query Performance: Improves the performance of the system. Instead of querying the whole database, now the system has to query only a smaller partition. Data Manageability: Divides tables and indexes into smaller and more manageable units. Disadvantages Complexity: Sharding increases the complexity of the system in general. Joins across shards: Once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span multiple database shards. Such joins will not be performance efficient since data has to be retrieved from multiple servers. Rebalancing: If the data distribution is not uniform or there is a lot of load on a single shard, in such cases we have to rebalance our shards so that the requests are as equally distributed among the shards as possible. When to use sharding? Here are some reasons where sharding might be the right choice:\nLeveraging existing hardware instead of high-end machines. Maintain data in distinct geographic regions. Quickly scale by adding more shards. Better performance as each machine is under less load. When more concurrent connections are required. Consistent Hashing Let\u0026rsquo;s first understand the problem we\u0026rsquo;re trying to solve.\nWhy do we need this? In traditional hashing-based distribution methods, we use a hash function to hash our partition keys (i.e. request ID or IP). Then if we use the modulo against the total number of nodes (server or databases). This will give us the node where we want to route our request.\n$$ \\begin{align*} \u0026amp; Hash(key_1) \\to H_1 \\bmod N = Node_0 \\ \u0026amp; Hash(key_2) \\to H_2 \\bmod N = Node_1 \\ \u0026amp; Hash(key_3) \\to H_3 \\bmod N = Node_2 \\ \u0026amp; \u0026hellip; \\ \u0026amp; Hash(key_n) \\to H_n \\bmod N = Node_{n-1} \\end{align*} $$\nWhere,\nkey: Request ID or IP.\nH: Hash function result.\nN: Total number of nodes.\nNode: The node where the request will be routed.\nThe problem with this is if we add or remove a node, it will cause N to change, meaning our mapping strategy will break as the same requests will now map to a different server. As a consequence, the majority of requests will need to be redistributed which is very inefficient.\nWe want to uniformly distribute requests among different nodes such that we should be able to add or remove nodes with minimal effort. Hence, we need a distribution scheme that does not depend directly on the number of nodes (or servers), so that, when adding or removing nodes, the number of keys that need to be relocated is minimized.\nConsistent hashing solves this horizontal scalability problem by ensuring that every time we scale up or down, we do not have to re-arrange all the keys or touch all the servers.\nNow that we understand the problem, let\u0026rsquo;s discuss consistent hashing in detail.\nHow does it work Consistent Hashing is a distributed hashing scheme that operates independently of the number of nodes in a distributed hash table by assigning them a position on an abstract circle, or hash ring. This allows servers and objects to scale without affecting the overall system.\nUsing consistent hashing, only K/N data would require re-distributing.\n$$ R = K/N $$\nWhere,\nR: Data that would require re-distribution.\nK: Number of partition keys.\nN: Number of nodes.\nThe output of the hash function is a range let\u0026rsquo;s say 0...m-1 which we can represent on our hash ring. We hash the requests and distribute them on the ring depending on what the output was. Similarly, we also hash the node and distribute them on the same ring as well.\n$$ \\begin{align*} \u0026amp; Hash(key_1) = P_1 \\ \u0026amp; Hash(key_2) = P_2 \\ \u0026amp; Hash(key_3) = P_3 \\ \u0026amp; \u0026hellip; \\ \u0026amp; Hash(key_n) = P_{m-1} \\end{align*} $$\nWhere,\nkey: Request/Node ID or IP.\nP: Position on the hash ring.\nm: Total range of the hash ring.\nNow, when the request comes in we can simply route it to the closest node in a clockwise (can be counterclockwise as well) manner. This means that if a new node is added or removed, we can use the nearest node and only a fraction of the requests need to be re-routed.\nIn theory, consistent hashing should distribute the load evenly however it doesn\u0026rsquo;t happen in practice. Usually, the load distribution is uneven and one server may end up handling the majority of the request becoming a hotspot, essentially a bottleneck for the system. We can fix this by adding extra nodes but that can be expensive.\nLet\u0026rsquo;s see how we can address these issues.\nVirtual Nodes In order to ensure a more evenly distributed load, we can introduce the idea of a virtual node, sometimes also referred to as a VNode.\nInstead of assigning a single position to a node, the hash range is divided into multiple smaller ranges, and each physical node is assigned several of these smaller ranges. Each of these subranges is considered a VNode. Hence, virtual nodes are basically existing physical nodes mapped multiple times across the hash ring to minimize changes to a node\u0026rsquo;s assigned range.\nFor this, we can use k number of hash functions.\n$$ \\begin{align*} \u0026amp; Hash_1(key_1) = P_1 \\ \u0026amp; Hash_2(key_2) = P_2 \\ \u0026amp; Hash_3(key_3) = P_3 \\ \u0026amp; . . . \\ \u0026amp; Hash_k(key_n) = P_{m-1} \\end{align*} $$\nWhere,\nkey: Request/Node ID or IP.\nk: Number of hash functions.\nP: Position on the hash ring.\nm: Total range of the hash ring.\nAs VNodes help spread the load more evenly across the physical nodes on the cluster by diving the hash ranges into smaller subranges, this speeds up the re-balancing process after adding or removing nodes. This also helps us reduce the probability of hotspots.\nData replication To ensure high availability and durability, consistent hashing replicates each data item on multiple N nodes in the system where the value N is equivalent to the replication factor.\nThe replication factor is the number of nodes that will receive the copy of the same data. In eventually consistent systems, this is done asynchronously.\nAdvantages Let\u0026rsquo;s look at some advantages of consistent hashing:\nMakes rapid scaling up and down more predictable. Facilitates partitioning and replication across nodes. Enables scalability and availability. Reduces hotspots. Disadvantages Below are some disadvantages of consistent hashing:\nIncreases complexity. Cascading failures. Load distribution can still be uneven. Key management can be expensive when nodes transiently fail. Examples Let\u0026rsquo;s look at some examples where consistent hashing is used:\nData partitioning in Apache Cassandra. Load distribution across multiple storage hosts in Amazon DynamoDB. Database Federation Federation (or functional partitioning) splits up databases by function. The federation architecture makes several distinct physical databases appear as one logical database to end-users.\nAll of the components in a federation are tied together by one or more federal schemas that express the commonality of data throughout the federation. These federated schemas are used to specify the information that can be shared by the federation components and to provide a common basis for communication among them.\nFederation also provides a cohesive, unified view of data derived from multiple sources. The data sources for federated systems can include databases and various other forms of structured and unstructured data.\nCharacteristics Let\u0026rsquo;s look at some key characteristics of a federated database:\nTransparency: Federated database masks user differences and implementations of underlying data sources. Therefore, the users do not need to be aware of where the data is stored. Heterogeneity: Data sources can differ in many ways. A federated database system can handle different hardware, network protocols, data models, etc. Extensibility: New sources may be needed to meet the changing needs of the business. A good federated database system needs to make it easy to add new sources. Autonomy: A Federated database does not change existing data sources, interfaces should remain the same. Data integration: A federated database can integrate data from different protocols, database management systems, etc. Advantages Here are some advantages of federated databases:\nFlexible data sharing. Autonomy among the database components. Access heterogeneous data in a unified way. No tight coupling of applications with legacy databases. Disadvantages Below are some disadvantages of federated databases:\nAdds more hardware and additional complexity. Joining data from two databases is complex. Dependence on autonomous data sources. Query performance and scalability. N-tier architecture N-tier architecture divides an application into logical layers and physical tiers. Layers are a way to separate responsibilities and manage dependencies. Each layer has a specific responsibility. A higher layer can use services in a lower layer, but not the other way around.\nTiers are physically separated, running on separate machines. A tier can call to another tier directly, or use asynchronous messaging. Although each layer might be hosted in its own tier, that\u0026rsquo;s not required. Several layers might be hosted on the same tier. Physically separating the tiers improves scalability and resiliency and adds latency from the additional network communication.\nAn N-tier architecture can be of two types:\nIn a closed layer architecture, a layer can only call the next layer immediately down. In an open layer architecture, a layer can call any of the layers below it. A closed-layer architecture limits the dependencies between layers. However, it might create unnecessary network traffic, if one layer simply passes requests along to the next layer.\nTypes of N-Tier architectures Let\u0026rsquo;s look at some examples of N-Tier architecture:\n3-Tier architecture 3-Tier is widely used and consists of the following different layers:\nPresentation layer: Handles user interactions with the application. Business Logic layer: Accepts the data from the application layer, validates it as per business logic and passes it to the data layer. Data Access layer: Receives the data from the business layer and performs the necessary operation on the database. 2-Tier architecture In this architecture, the presentation layer runs on the client and communicates with a data store. There is no business logic layer or immediate layer between client and server.\nSingle Tier or 1-Tier architecture It is the simplest one as it is equivalent to running the application on a personal computer. All of the required components for an application to run are on a single application or server.\nAdvantages Here are some advantages of using N-tier architecture:\nCan improve availability. Better security as layers can behave like a firewall. Separate tiers allow us to scale them as needed. Improve maintenance as different people can manage different tiers. Disadvantages Below are some disadvantages of N-tier architecture:\nIncreased complexity of the system as a whole. Increased network latency as the number of tiers increases. Expensive as every tier will have its own hardware cost. Difficult to manage network security. Message Brokers A message broker is a software that enables applications, systems, and services to communicate with each other and exchange information. The message broker does this by translating messages between formal messaging protocols. This allows interdependent services to \u0026ldquo;talk\u0026rdquo; with one another directly, even if they were written in different languages or implemented on different platforms.\nMessage brokers can validate, store, route, and deliver messages to the appropriate destinations. They serve as intermediaries between other applications, allowing senders to issue messages without knowing where the receivers are, whether or not they are active, or how many of them there are. This facilitates the decoupling of processes and services within systems.\nModels Message brokers offer two basic message distribution patterns or messaging styles:\nPoint-to-Point messaging: This is the distribution pattern utilized in message queues with a one-to-one relationship between the message\u0026rsquo;s sender and receiver. Publish-subscribe messaging: In this message distribution pattern, often referred to as \u0026ldquo;pub/sub\u0026rdquo;, the producer of each message publishes it to a topic, and multiple message consumers subscribe to topics from which they want to receive messages. We will discuss these messaging patterns in detail in the later tutorials.\nMessage brokers vs Event streaming Message brokers can support two or more messaging patterns, including message queues and pub/sub, while event streaming platforms only offer pub/sub-style distribution patterns. Designed for use with high volumes of messages, event streaming platforms are readily scalable. They\u0026rsquo;re capable of ordering streams of records into categories called topics and storing them for a predetermined amount of time. Unlike message brokers, however, event streaming platforms cannot guarantee message delivery or track which consumers have received the messages.\nEvent streaming platforms offer more scalability than message brokers but fewer features that ensure fault tolerance like message resending, as well as more limited message routing and queueing capabilities.\nMessage brokers vs Enterprise Service Bus (ESB) Enterprise Service Bus (ESB) infrastructure is complex and can be challenging to integrate and expensive to maintain. It\u0026rsquo;s difficult to troubleshoot them when problems occur in production environments, they\u0026rsquo;re not easy to scale, and updating is tedious.\nWhereas message brokers are a \u0026ldquo;lightweight\u0026rdquo; alternative to ESBs that provide similar functionality, a mechanism for inter-service communication, at a lower cost. They\u0026rsquo;re well-suited for use in the microservices architectures that have become more prevalent as ESBs have fallen out of favor.\nExamples Here are some commonly used message brokers:\nNATS Apache Kafka RabbitMQ ActiveMQ Message Queues A message queue is a form of service-to-service communication that facilitates asynchronous communication. It asynchronously receives messages from producers and sends them to consumers.\nQueues are used to effectively manage requests in large-scale distributed systems. In small systems with minimal processing loads and small databases, writes can be predictably fast. However, in more complex and large systems writes can take an almost non-deterministic amount of time.\nWorking Messages are stored in the queue until they are processed and deleted. Each message is processed only once by a single consumer. Here\u0026rsquo;s how it works:\nA producer publishes a job to the queue, then notifies the user of the job status. A consumer picks up the job from the queue, processes it, then signals that the job is complete. Advantages Let\u0026rsquo;s discuss some advantages of using a message queue:\nScalability: Message queues make it possible to scale precisely where we need to. When workloads peak, multiple instances of our application can all add requests to the queue without the risk of collision Decoupling: Message queues remove dependencies between components and significantly simplify the implementation of decoupled applications. Performance: Message queues enable asynchronous communication, which means that the endpoints that are producing and consuming messages interact with the queue, not each other. Producers can add requests to the queue without waiting for them to be processed. Reliability: Queues make our data persistent, and reduce the errors that happen when different parts of our system go offline. Features Now, let\u0026rsquo;s discuss some desired features of message queues:\nPush or Pull Delivery Most message queues provide both push and pull options for retrieving messages. Pull means continuously querying the queue for new messages. Push means that a consumer is notified when a message is available. We can also use long-polling to allow pulls to wait a specified amount of time for new messages to arrive.\nFIFO (First-In-First-Out) Queues In these queues, the oldest (or first) entry, sometimes called the \u0026ldquo;head\u0026rdquo; of the queue, is processed first.\nSchedule or Delay Delivery Many message queues support setting a specific delivery time for a message. If we need to have a common delay for all messages, we can set up a delay queue.\nAt-Least-Once Delivery Message queues may store multiple copies of messages for redundancy and high availability, and resend messages in the event of communication failures or errors to ensure they are delivered at least once.\nExactly-Once Delivery When duplicates can\u0026rsquo;t be tolerated, FIFO (first-in-first-out) message queues will make sure that each message is delivered exactly once (and only once) by filtering out duplicates automatically.\nDead-letter Queues A dead-letter queue is a queue to which other queues can send messages that can\u0026rsquo;t be processed successfully. This makes it easy to set them aside for further inspection without blocking the queue processing or spending CPU cycles on a message that might never be consumed successfully.\nOrdering Most message queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they\u0026rsquo;re sent and that a message is delivered at least once.\nPoison-pill Messages Poison pills are special messages that can be received, but not processed. They are a mechanism used in order to signal a consumer to end its work so it is no longer waiting for new inputs, and are similar to closing a socket in a client/server model.\nSecurity Message queues will authenticate applications that try to access the queue, this allows us to encrypt messages over the network as well as in the queue itself.\nTask Queues Tasks queues receive tasks and their related data, run them, then deliver their results. They can support scheduling and can be used to run computationally-intensive jobs in the background.\nBackpressure If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance. Backpressure can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue. Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later. Clients can retry the request at a later time, perhaps with exponential backoff strategy.\nExamples Following are some widely used message queues:\nAmazon SQS RabbitMQ ActiveMQ ZeroMQ Publish-Subscribe Similar to a message queue, publish-subscribe is also a form of service-to-service communication that facilitates asynchronous communication. In a pub/sub model, any message published to a topic is pushed immediately to all the subscribers of the topic.\nThe subscribers to the message topic often perform different functions, and can each do something different with the message in parallel. The publisher doesn\u0026rsquo;t need to know who is using the information that it is broadcasting, and the subscribers don\u0026rsquo;t need to know where the message comes from. This style of messaging is a bit different than message queues, where the component that sends the message often knows the destination it is sending to.\nWorking Unlike message queues, which batch messages until they are retrieved, message topics transfer messages with little or no queuing and push them out immediately to all subscribers. Here\u0026rsquo;s how it works:\nA message topic provides a lightweight mechanism to broadcast asynchronous event notifications and endpoints that allow software components to connect to the topic in order to send and receive those messages. To broadcast a message, a component called a publisher simply pushes a message to the topic. All components that subscribe to the topic (known as subscribers) will receive every message that was broadcasted. Advantages Let\u0026rsquo;s discuss some advantages of using publish-subscribe:\nEliminate Polling: Message topics allow instantaneous, push-based delivery, eliminating the need for message consumers to periodically check or \u0026ldquo;poll\u0026rdquo; for new information and updates. This promotes faster response time and reduces the delivery latency which can be particularly problematic in systems where delays cannot be tolerated. Dynamic Targeting: Pub/Sub makes the discovery of services easier, more natural, and less error-prone. Instead of maintaining a roster of peers where an application can send messages, a publisher will simply post messages to a topic. Then, any interested party will subscribe its endpoint to the topic, and start receiving these messages. Subscribers can change, upgrade, multiply or disappear and the system dynamically adjusts. Decoupled and Independent Scaling: Publishers and subscribers are decoupled and work independently from each other, which allows us to develop and scale them independently. Simplify Communication: The Publish-Subscribe model reduces complexity by removing all the point-to-point connections with a single connection to a message topic, which will manage subscriptions and decide what messages should be delivered to which endpoints. Features Now, let\u0026rsquo;s discuss some desired features of publish-subscribe:\nPush Delivery Pub/Sub messaging instantly pushes asynchronous event notifications when messages are published to the message topic. Subscribers are notified when a message is available.\nMultiple Delivery Protocols In the Publish-Subscribe model, topics can typically connect to multiple types of endpoints, such as message queues, serverless functions, HTTP servers, etc.\nFanout This scenario happens when a message is sent to a topic and then replicated and pushed to multiple endpoints. Fanout provides asynchronous event notifications which in turn allows for parallel processing.\nFiltering This feature empowers the subscriber to create a message filtering policy so that it will only get the notifications it is interested in, as opposed to receiving every single message posted to the topic.\nDurability Pub/Sub messaging services often provide very high durability, and at least once delivery, by storing copies of the same message on multiple servers.\nSecurity Message topics authenticate applications that try to publish content, this allows us to use encrypted endpoints and encrypt messages in transit over the network.\nExamples Here are some technologies commonly used for publish-subscribe:\nAmazon SNS Google Pub/Sub Enterprise Service Bus (ESB) An Enterprise Service Bus (ESB) is an architectural pattern whereby a centralized software component performs integrations between applications. It performs transformations of data models, handles connectivity, performs message routing, converts communication protocols, and potentially manages the composition of multiple requests. The ESB can make these integrations and transformations available as a service interface for reuse by new applications.\nAdvantages In theory, a centralized ESB offers the potential to standardize and dramatically simplify communication, messaging, and integration between services across the enterprise. Here are some advantages of using an ESB:\nImproved developer productivity: Enables developers to incorporate new technologies into one part of an application without touching the rest of the application. Simpler, more cost-effective scalability: Components can be scaled independently of others. Greater resilience: Failure of one component does not impact the others, and each microservice can adhere to its own availability requirements without risking the availability of other components in the system. Disadvantages While ESBs were deployed successfully in many organizations, in many other organizations the ESB came to be seen as a bottleneck. Here are some disadvantages of using an ESB:\nMaking changes or enhancements to one integration could destabilize others who use that same integration. A single point of failure can bring down all communications. Updates to the ESB often impact existing integrations, so there is significant testing required to perform any update. ESB is centrally managed which makes cross-team collaboration challenging. High configuration and maintenance complexity. Examples Below are some widely used Enterprise Service Bus (ESB) technologies:\nAzure Service Bus IBM App Connect Apache Camel Fuse ESB Monoliths and Microservices Monoliths A monolith is a self-contained and independent application. It is built as a single unit and is responsible for not just a particular task, but can perform every step needed to satisfy a business need.\nAdvantages Following are some advantages of monoliths:\nSimple to develop or debug. Fast and reliable communication. Easy monitoring and testing. Supports ACID transactions. Disadvantages Some common disadvantages of monoliths are:\nMaintenance becomes hard as the codebase grows. Tightly coupled application, hard to extend. Requires commitment to a particular technology stack. On each update, the entire application is redeployed. Reduced reliability as a single bug can bring down the entire system. Difficult to scale or adopt technologies new technologies. Modular monoliths A Modular Monolith is an approach where we build and deploy a single application (that\u0026rsquo;s the Monolith part), but we build it in a way that breaks up the code into independent modules for each of the features needed in our application.\nThis approach reduces the dependencies of a module in such as way that we can enhance or change a module without affecting other modules. When done right, this can be really beneficial in the long term as it reduces the complexity that comes with maintaining a monolith as the system grows.\nMicroservices A microservices architecture consists of a collection of small, autonomous services where each service is self-contained and should implement a single business capability within a bounded context. A bounded context is a natural division of business logic that provides an explicit boundary within which a domain model exists.\nEach service has a separate codebase, which can be managed by a small development team. Services can be deployed independently and a team can update an existing service without rebuilding and redeploying the entire application.\nServices are responsible for persisting their own data or external state (database per service). This differs from the traditional model, where a separate data layer handles data persistence.\nCharacteristics The microservices architecture style has the following characteristics:\nLoosely coupled: Services should be loosely coupled so that they can be independently deployed and scaled. This will lead to the decentralization of development teams and thus, enabling them to develop and deploy faster with minimal constraints and operational dependencies. Small but focused: It\u0026rsquo;s about scope and responsibilities and not size, a service should be focused on a specific problem. Basically, \u0026ldquo;It does one thing and does it well\u0026rdquo;. Ideally, they can be independent of the underlying architecture. Built for businesses: The microservices architecture is usually organized around business capabilities and priorities. Resilience \u0026amp; Fault tolerance: Services should be designed in such a way that they still function in case of failure or errors. In environments with independently deployable services, failure tolerance is of the highest importance. Highly maintainable: Service should be easy to maintainable and test because services that cannot be maintained will be re-written. Advantages Here are some advantages of microservices architecture:\nLoosely coupled services. Services can be deployed independently. Highly agile for multiple development teams. Improves fault tolerance and data isolation. Better scalability as each service can be scaled independently. Eliminates any long-term commitment to a particular technology stack. Disadvantages Microservices architecture brings its own set of challenges:\nComplexity of a distributed system. Testing is more difficult. Expensive to maintain (individual servers, databases, etc.). Inter-service communication has its own challenges. Data integrity and consistency. Network congestion and latency. Best practices Let\u0026rsquo;s discuss some microservices best practices:\nModel services around the business domain. Services should have loose coupling and high functional cohesion. Isolate failures and use resiliency strategies to prevent failures within a service from cascading. Services should only communicate through well-designed APIs. Avoid leaking implementation details. Data storage should be private to the service that owns the data Avoid coupling between services. Causes of coupling include shared database schemas and rigid communication protocols. Decentralize everything. Individual teams are responsible for designing and building services. Avoid sharing code or data schemas. Fail fast by using a circuit breaker to achieve fault tolerance. Ensure that the API changes are backward compatible. Pitfalls Below are some common pitfalls of microservices architecture:\nService boundaries are not based on the business domain. Underestimating how hard is to build a distributed system. Shared database or common dependencies between services. Lack of Business Alignment. Lack of clear ownership. Lack of idempotency. Trying to do everything ACID instead of BASE. Lack of design for fault tolerance may result in cascading failures. Beware of the distributed monolith Distributed Monolith is a system that resembles the microservices architecture but is tightly coupled within itself like a monolithic application. Adopting microservices architecture comes with a lot of advantages. But while making one, there are good chances that we might end up with a distributed monolith.\nOur microservices are just a distributed monolith if any of these apply to it:\nRequires low latency communication. Services don\u0026rsquo;t scale easily. Dependency between services. Sharing the same resources such as databases. Tightly coupled systems. One of the primary reasons to build an application using microservices architecture is to have scalability. Therefore, microservices should have loosely coupled services which enable every service to be independent. The distributed monolith architecture takes this away and causes most components to depend on one another, increasing design complexity.\nMicroservices vs Service-oriented architecture (SOA) You might have seen Service-oriented architecture (SOA) mentioned around the internet, sometimes even interchangeably with microservices, but they are different from each other and the main distinction between the two approaches comes down to scope.\nService-oriented architecture (SOA) defines a way to make software components reusable via service interfaces. These interfaces utilize common communication standards and focus on maximizing application service reusability whereas microservices are built as a collection of various smallest independent service units focused on team autonomy and decoupling.\nWhy you don\u0026rsquo;t need microservices So, you might be wondering, monoliths seem like a bad idea to begin with, why would anyone use that?\nWell, it depends. While each approach has its own advantages and disadvantages, it is advised to start with a monolith when building a new system. It is important to understand, that microservices are not a silver bullet instead they solve an organizational problem. Microservices architecture is about your organizational priorities and team as much as it\u0026rsquo;s about technology.\nBefore making the decision to move to microservices architecture, you need to ask yourself questions like:\n\u0026ldquo;Is the team too large to work effectively on a shared codebase?\u0026rdquo; \u0026ldquo;Are teams blocked on other teams?\u0026rdquo; \u0026ldquo;Does microservices deliver clear business value for us?\u0026rdquo; \u0026ldquo;Is my business mature enough to use microservices?\u0026rdquo; \u0026ldquo;Is our current architecture limiting us with communication overhead?\u0026rdquo; If your application does not require to be broken down into microservices, you don\u0026rsquo;t need this. There is no absolute necessity that all applications should be broken down into microservices.\nWe frequently draw inspiration from companies such as Netflix and their use of microservices, but we overlook the fact that we are not Netflix. They went through a lot of iterations and models before they had a market-ready solution, and this architecture became acceptable for them when they identified and solved the problem they were trying to tackle.\nThat\u0026rsquo;s why it\u0026rsquo;s essential to understand in-depth if your business actually needs microservices. What I\u0026rsquo;m trying to say is microservices are solutions to complex concerns and if your business doesn\u0026rsquo;t have complex issues, you don\u0026rsquo;t need them.\nEvent-Driven Architecture (EDA) Event-Driven Architecture (EDA) is about using events as a way to communicate within a system. Generally, leveraging a message broker to publish and consume events asynchronously. The publisher is unaware of who is consuming an event and the consumers are unaware of each other. Event-Driven Architecture is simply a way of achieving loose coupling between services within a system.\nWhat is an event? An event is a data point that represents state changes in a system. It doesn\u0026rsquo;t specify what should happen and how the change should modify the system, it only notifies the system of a particular state change. When a user makes an action, they trigger an event.\nComponents Event-driven architectures have three key components:\nEvent producers: Publishes an event to the router. Event routers: Filters and pushes the events to consumers. Event consumers: Uses events to reflect changes in the system. Note: Dots in the diagram represents different events in the system.\nPatterns There are several ways to implement the event-driven architecture, and which method we use depends on the use case but here are some common examples:\nSagas Publish-Subscribe Event Sourcing Command and Query Responsibility Segregation (CQRS) Note: Each of these methods is discussed separately.\nAdvantages Let\u0026rsquo;s discuss some advantages:\nDecoupled producers and consumers. Highly scalable and distributed. Easy to add new consumers. Improves agility. Challenges Here are some challenges of event-drive architecture:\nGuaranteed delivery. Error handling is difficult. Event-driven systems are complex in general. Exactly once, in-order processing of events. Use cases Below are some common use cases where event-driven architectures are beneficial:\nMetadata and metrics. Server and security logs. Integrating heterogeneous systems. Fanout and parallel processing. Examples Here are some widely used technologies for implementing event-driven architectures:\nNATS Apache Kafka Amazon EventBridge Amazon SNS Google PubSub Event Sourcing Instead of storing just the current state of the data in a domain, use an append-only store to record the full series of actions taken on that data. The store acts as the system of record and can be used to materialize the domain objects.\nThis can simplify tasks in complex domains, by avoiding the need to synchronize the data model and the business domain, while improving performance, scalability, and responsiveness. It can also provide consistency for transactional data, and maintain full audit trails and history that can enable compensating actions.\nEvent sourcing vs Event-Driven Architecture (EDA) Event sourcing is seemingly constantly being confused with Event-driven Architecture (EDA). Event-driven architecture is about using events to communicate between service boundaries. Generally, leveraging a message broker to publish and consume events asynchronously within other boundaries.\nWhereas, event sourcing is about using events as a state, which is a different approach to storing data. Rather than storing the current state, we\u0026rsquo;re instead going to be storing events. Also, event sourcing is one of the several patterns to implement an event-driven architecture.\nAdvantages Let\u0026rsquo;s discuss some advantages of using event sourcing:\nExcellent for real-time data reporting. Great for fail-safety, data can be reconstituted from the event store. Extremely flexible, any type of message can be stored. Preferred way of achieving audit logs functionality for high compliance systems. Disadvantages Following are the disadvantages of event sourcing:\nRequires an extremely efficient network infrastructure. Requires a reliable way to control message formats, such as a schema registry. Different events will contain different payloads. Command and Query Responsibility Segregation (CQRS) Command Query Responsibility Segregation (CQRS) is an architectural pattern that divides a system\u0026rsquo;s actions into commands and queries. It was first described by Greg Young.\nIn CQRS, a command is an instruction, a directive to perform a specific task. It is an intention to change something and doesn\u0026rsquo;t return a value, only an indication of success or failure. And, a query is a request for information that doesn\u0026rsquo;t change the system\u0026rsquo;s state or cause any side effects.\nThe core principle of CQRS is the separation of commands and queries. They perform fundamentally different roles within a system, and separating them means that each can be optimized as needed, which distributed systems can really benefit from.\nCQRS with Event Sourcing The CQRS pattern is often used along with the Event Sourcing pattern. CQRS-based systems use separate read and write data models, each tailored to relevant tasks and often located in physically separate stores.\nWhen used with the Event Sourcing pattern, the store of events is the write model and is the official source of information. The read model of a CQRS-based system provides materialized views of the data, typically as highly denormalized views.\nAdvantages Let\u0026rsquo;s discuss some advantages of CQRS:\nAllows independent scaling of read and write workloads. Easier scaling, optimizations, and architectural changes. Closer to business logic with loose coupling. The application can avoid complex joins when querying. Clear boundaries between the system behavior. Disadvantages Below are some disadvantages of CQRS:\nMore complex application design. Message failures or duplicate messages can occur. Dealing with eventual consistency is a challenge. Increased system maintenance efforts. Use cases Here are some scenarios where CQRS will be helpful:\nThe performance of data reads must be fine-tuned separately from the performance of data writes. The system is expected to evolve over time and might contain multiple versions of the model, or where business rules change regularly. Integration with other systems, especially in combination with event sourcing, where the temporal failure of one subsystem shouldn\u0026rsquo;t affect the availability of the others. Better security to ensure that only the right domain entities are performing writes on the data. API Gateway The API Gateway is an API management tool that sits between a client and a collection of backend services. It is a single entry point into a system that encapsulates the internal system architecture and provides an API that is tailored to each client. It also has other responsibilities such as authentication, monitoring, load balancing, caching, throttling, logging, etc.\nWhy do we need an API Gateway? The granularity of APIs provided by microservices is often different than what a client needs. Microservices typically provide fine-grained APIs, which means that clients need to interact with multiple services. Hence, an API gateway can provide a single entry point for all clients with some additional features and better management.\nFeatures Below are some desired features of an API Gateway:\nAuthentication and Authorization Service discovery Reverse Proxy Caching Security Retry and Circuit breaking Load balancing Logging, Tracing API composition Rate limiting and throttling Versioning Routing IP whitelisting or blacklisting Advantages Let\u0026rsquo;s look at some advantages of using an API Gateway:\nEncapsulates the internal structure of an API. Provides a centralized view of the API. Simplifies the client code. Monitoring, analytics, tracing, and other such features. Disadvantages Here are some possible disadvantages of an API Gateway:\nPossible single point of failure. Might impact performance. Can become a bottleneck if not scaled properly. Configuration can be challenging. Backend For Frontend (BFF) pattern In the Backend For Frontend (BFF) pattern, we create separate backend services to be consumed by specific frontend applications or interfaces. This pattern is useful when we want to avoid customizing a single backend for multiple interfaces. This pattern was first described by Sam Newman.\nAlso, sometimes the output of data returned by the microservices to the front end is not in the exact format or filtered as needed by the front end. To solve this issue, the frontend should have some logic to reformat the data, and therefore, we can use BFF to shift some of this logic to the intermediate layer.\nThe primary function of the backend for the frontend pattern is to get the required data from the appropriate service, format the data, and sent it to the frontend.\nGraphQL performs really well as a backend for frontend (BFF).\nWhen to use this pattern? We should consider using a Backend For Frontend (BFF) pattern when:\nA shared or general purpose backend service must be maintained with significant development overhead. We want to optimize the backend for the requirements of a specific client. Customizations are made to a general-purpose backend to accommodate multiple interfaces. Examples Following are some widely used gateways technologies:\nAmazon API Gateway Apigee API Gateway Azure API Gateway Kong API Gateway REST, GraphQL, gRPC A good API design is always a crucial part of any system. But it is also important to pick the right API technology. So, in this tutorial, we will briefly discuss different API technologies such as REST, GraphQL, and gRPC.\nWhat\u0026rsquo;s an API? Before we even get into API technologies, let\u0026rsquo;s first understand what is an API.\nAn API is a set of definitions and protocols for building and integrating application software. It\u0026rsquo;s sometimes referred to as a contract between an information provider and an information user establishing the content required from the producer and the content required by the consumer.\nIn other words, if you want to interact with a computer or system to retrieve information or perform a function, an API helps you communicate what you want to that system so it can understand and complete the request.\nREST A REST API (also known as RESTful API) is an application programming interface that conforms to the constraints of REST architectural style and allows for interaction with RESTful web services. REST stands for Representational State Transfer and it was first introduced by Roy Fielding in the year 2000.\nIn REST API, the fundamental unit is a resource.\nConcepts Let\u0026rsquo;s discuss some concepts of a RESTful API.\nConstraints\nIn order for an API to be considered RESTful, it has to conform to these architectural constraints:\nUniform Interface: There should be a uniform way of interacting with a given server. Client-Server: A client-server architecture managed through HTTP. Stateless: No client context shall be stored on the server between requests. Cacheable: Every response should include whether the response is cacheable or not and for how much duration responses can be cached at the client-side. Layered system: An application architecture needs to be composed of multiple layers. Code on demand: Return executable code to support a part of your application. (optional) HTTP Verbs\nHTTP defines a set of request methods to indicate the desired action to be performed for a given resource. Although they can also be nouns, these request methods are sometimes referred to as HTTP verbs. Each of them implements a different semantic, but some common features are shared by a group of them.\nBelow are some commonly used HTTP verbs:\nGET: Request a representation of the specified resource. HEAD: Response is identical to a GET request, but without the response body. POST: Submits an entity to the specified resource, often causing a change in state or side effects on the server. PUT: Replaces all current representations of the target resource with the request payload. DELETE: Deletes the specified resource. PATCH: Applies partial modifications to a resource. HTTP response codes\nHTTP response status codes indicate whether a specific HTTP request has been successfully completed.\nThere are five classes defined by the standard:\n1xx - Informational responses. 2xx - Successful responses. 3xx - Redirection responses. 4xx - Client error responses. 5xx - Server error responses. For example, HTTP 200 means that the request was successful.\nAdvantages Let\u0026rsquo;s discuss some advantages of REST API:\nSimple and easy to understand. Flexible and portable. Good caching support. Client and server are decoupled. Disadvantages Let\u0026rsquo;s discuss some disadvantages of REST API:\nOver-fetching of data. Sometimes multiple round trips to the server are required. Use cases REST APIs are pretty much used universally and are the default standard for designing APIs. Overall REST APIs are quite flexible and can fit almost all scenarios.\nExample Here\u0026rsquo;s an example usage of a REST API that operates on a users resource.\nURI HTTP verb Description /users GET Get all users /users/{id} GET Get a user by id /users POST Add a new user /users/{id} PATCH Update a user by id /users/{id} DELETE Delete a user by id There is so much more to learn when it comes to REST APIs, I will highly recommend looking into Hypermedia as the Engine of Application State (HATEOAS).\nGraphQL GraphQL is a query language and server-side runtime for APIs that prioritizes giving clients exactly the data they request and no more. It was developed by Facebook and later open-sourced in 2015.\nGraphQL is designed to make APIs fast, flexible, and developer-friendly. Additionally, GraphQL gives API maintainers the flexibility to add or deprecate fields without impacting existing queries. Developers can build APIs with whatever methods they prefer, and the GraphQL specification will ensure they function in predictable ways to clients.\nIn GraphQL, the fundamental unit is a query.\nConcepts Let\u0026rsquo;s briefly discuss some key concepts in GraphQL:\nSchema\nA GraphQL schema describes the functionality clients can utilize once they connect to the GraphQL server.\nQueries\nA query is a request made by the client. It can consist of fields and arguments for the query. The operation type of a query can also be a mutation which provides a way to modify server-side data.\nResolvers\nResolver is a collection of functions that generate responses for a GraphQL query. In simple terms, a resolver acts as a GraphQL query handler.\nAdvantages Let\u0026rsquo;s discuss some advantages of GraphQL:\nEliminates over-fetching of data. Strongly defined schema. Code generation support. Payload optimization. Disadvantages Let\u0026rsquo;s discuss some disadvantages of GraphQL:\nShifts complexity to server-side. Caching becomes hard. Versioning is ambiguous. N+1 problem. Use cases GraphQL proves to be essential in the following scenarios:\nReducing app bandwidth usage as we can query multiple resources in a single query. Rapid prototyping for complex systems. When we are working with a graph-like data model. Example Here\u0026rsquo;s a GraphQL schema that defines a User type and a Query type.\ntype Query { getUser: User } type User { id: ID name: String city: String state: String } Using the above schema, the client can request the required fields easily without having to fetch the entire resource or guess what the API might return.\n{ getUser { id name city } } This will give the following response to the client.\n{ \u0026#34;getUser\u0026#34;: { \u0026#34;id\u0026#34;: 123, \u0026#34;name\u0026#34;: \u0026#34;Karan\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;San Francisco\u0026#34; } } Learn more about GraphQL at graphql.org.\ngRPC gRPC is a modern open-source high-performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking, authentication and much more.\nConcepts Let\u0026rsquo;s discuss some key concepts of gRPC.\nProtocol buffers\nProtocol buffers provide a language and platform-neutral extensible mechanism for serializing structured data in a forward and backward-compatible way. It\u0026rsquo;s like JSON, except it\u0026rsquo;s smaller and faster, and it generates native language bindings.\nService definition\nLike many RPC systems, gRPC is based on the idea of defining a service and specifying the methods that can be called remotely with their parameters and return types. gRPC uses protocol buffers as the Interface Definition Language (IDL) for describing both the service interface and the structure of the payload messages.\nAdvantages Let\u0026rsquo;s discuss some advantages of gRPC:\nLightweight and efficient. High performance. Built-in code generation support. Bi-directional streaming. Disadvantages Let\u0026rsquo;s discuss some disadvantages of gRPC:\nRelatively new compared to REST and GraphQL. Limited browser support. Steeper learning curve. Not human readable. Use cases Below are some good use cases for gRPC:\nReal-time communication via bi-directional streaming. Efficient inter-service communication in microservices. Low latency and high throughput communication. Polyglot environments. Example Here\u0026rsquo;s a basic example of a gRPC service defined in a *.proto file. Using this definition, we can easily code generate the HelloService service in the programming language of our choice.\nservice HelloService { rpc SayHello (HelloRequest) returns (HelloResponse); } message HelloRequest { string greeting = 1; } message HelloResponse { string reply = 1; } REST vs GraphQL vs gRPC Now that we know how these API designing techniques work, let\u0026rsquo;s compare them based on the following parameters:\nWill it cause tight coupling? How chatty (distinct API calls to get needed information) are the APIs? What\u0026rsquo;s the performance like? How complex is it to integrate? How well does the caching work? Built-in tooling and code generation? What\u0026rsquo;s API discoverability like? How easy is it to version APIs? Type Coupling Chattiness Performance Complexity Caching Codegen Discoverability Versioning REST Low High Good Medium Great Bad Good Easy GraphQL Medium Low Good High Custom Good Good Custom gRPC High Medium Great Low Custom Great Bad Hard Which API technology is better? Well, the answer is none of them. There is no silver bullet as each of these technologies has its own advantages and disadvantages. Users only care about using our APIs in a consistent way, so make sure to focus on your domain and requirements when designing your API.\nLong polling, WebSockets, Server-Sent Events (SSE) Web applications were initially developed around a client-server model, where the web client is always the initiator of transactions like requesting data from the server. Thus, there was no mechanism for the server to independently send, or push, data to the client without the client first making a request. Let\u0026rsquo;s discuss some approaches to overcome this problem.\nLong polling HTTP Long polling is a technique used to push information to a client as soon as possible from the server. As a result, the server does not have to wait for the client to send a request.\nIn Long polling, the server does not close the connection once it receives a request from the client. Instead, the server responds only if any new message is available or a timeout threshold is reached.\nOnce the client receives a response, it immediately sends a new request to the server to have a new pending connection to send data to the client, and the operation is repeated. With this approach, the server emulates a real-time server push feature.\nWorking Let\u0026rsquo;s understand how long polling works:\nThe client makes an initial request and waits for a response. The server receives the request and delays sending anything until an update is available. Once an update is available, the response is sent to the client. The client receives the response and makes a new request immediately or after some defined interval to establish a connection again. Advantages Here are some advantages of long polling:\nEasy to implement, good for small-scale projects. Nearly universally supported. Disadvantages A major downside of long polling is that it is usually not scalable. Below are some of the other reasons:\nCreates a new connection each time, which can be intensive on the server. Reliable message ordering can be an issue for multiple requests. Increased latency as the server needs to wait for a new request. WebSockets WebSocket provides full-duplex communication channels over a single TCP connection. It is a persistent connection between a client and a server that both parties can use to start sending data at any time.\nThe client establishes a WebSocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables the communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server.\nThis is made possible by providing a standardized way for the server to send content to the client without being asked and allowing for messages to be passed back and forth while keeping the connection open.\nWorking Let\u0026rsquo;s understand how WebSockets work:\nThe client initiates a WebSocket handshake process by sending a request. The request also contains an HTTP Upgrade header that allows the request to switch to the WebSocket protocol (ws://). The server sends a response to the client, acknowledging the WebSocket handshake request. A WebSocket connection will be opened once the client receives a successful handshake response. Now the client and server can start sending data in both directions allowing real-time communication. The connection is closed once the server or the client decides to close the connection. Advantages Below are some advantages of WebSockets:\nFull-duplex asynchronous messaging. Better origin-based security model. Lightweight for both client and server. Disadvantages Let\u0026rsquo;s discuss some disadvantages of WebSockets:\nTerminated connections aren\u0026rsquo;t automatically recovered. Older browsers don\u0026rsquo;t support WebSockets (becoming less relevant). Server-Sent Events (SSE) Server-Sent Events (SSE) is a way of establishing long-term communication between client and server that enables the server to proactively push data to the client.\nIt is unidirectional, meaning once the client sends the request it can only receive the responses without the ability to send new requests over the same connection.\nWorking Let\u0026rsquo;s understand how server-sent events work:\nThe client makes a request to the server. The connection between client and server is established and it remains open. The server sends responses or events to the client when new data is available. Advantages Simple to implement and use for both client and server. Supported by most browsers. No trouble with firewalls. Disadvantages Unidirectional nature can be limiting. Limitation for the maximum number of open connections. Does not support binary data. Geohashing and Quadtrees Geohashing Geohashing is a geocoding method used to encode geographic coordinates such as latitude and longitude into short alphanumeric strings. It was created by Gustavo Niemeyer in 2008.\nFor example, San Francisco with coordinates 37.7564, -122.4016 can be represented in geohash as 9q8yy9mf.\nHow does Geohashing work? Geohash is a hierarchical spatial index that uses Base-32 alphabet encoding, the first character in a geohash identifies the initial location as one of the 32 cells. This cell will also contain 32 cells. This means that to represent a point, the world is recursively divided into smaller and smaller cells with each additional bit until the desired precision is attained. The precision factor also determines the size of the cell.\nGeohashing guarantees that points are spatially closer if their Geohashes share a longer prefix which means the more characters in the string, the more precise the location. For example, geohashes 9q8yy9mf and 9q8yy9vx are spatially closer as they share the prefix 9q8yy9.\nGeohashing can also be used to provide a degree of anonymity as we don\u0026rsquo;t need to expose the exact location of the user because depending on the length of the geohash we just know they are somewhere within an area.\nThe cell sizes of the geohashes of different lengths are as follows:\nGeohash length Cell width Cell height 1 5000 km 5000 km 2 1250 km 1250 km 3 156 km 156 km 4 39.1 km 19.5 km 5 4.89 km 4.89 km 6 1.22 km 0.61 km 7 153 m 153 m 8 38.2 m 19.1 m 9 4.77 m 4.77 m 10 1.19 m 0.596 m 11 149 mm 149 mm 12 37.2 mm 18.6 mm Use cases Here are some common use cases for Geohashing:\nIt is a simple way to represent and store a location in a database. It can also be shared on social media as URLs since it is easier to share, and remember than latitudes and longitudes. We can efficiently find the nearest neighbors of a point through very simple string comparisons and efficient searching of indexes. Examples Geohashing is widely used and it is supported by popular databases.\nMySQL Redis Amazon DynamoDB Google Cloud Firestore Quadtrees A quadtree is a tree data structure in which each internal node has exactly four children. They are often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions. Each child or leaf node stores spatial information. Quadtrees are the two-dimensional analog of Octrees which are used to partition three-dimensional space.\nTypes of Quadtrees Quadtrees may be classified according to the type of data they represent, including areas, points, lines, and curves. The following are common types of quadtrees:\nPoint quadtrees Point-region (PR) quadtrees Polygonal map (PM) quadtrees Compressed quadtrees Edge quadtrees Why do we need Quadtrees? Aren\u0026rsquo;t latitude and longitude enough? Why do we need quadtrees? While in theory using latitude and longitude we can determine things such as how close points are to each other using euclidean distance, for practical use cases it is simply not scalable because of its CPU-intensive nature with large data sets.\nQuadtrees enable us to search points within a two-dimensional range efficiently, where those points are defined as latitude/longitude coordinates or as cartesian (x, y) coordinates. Additionally, we can save further computation by only subdividing a node after a certain threshold. And with the application of mapping algorithms such as the Hilbert curve, we can easily improve range query performance.\nUse cases Below are some common uses of quadtrees:\nImage representation, processing, and compression. Spacial indexing and range queries. Location-based services like Google Maps, Uber, etc. Mesh generation and computer graphics. Sparse data storage. Circuit breaker The circuit breaker is a design pattern used to detect failures and encapsulates the logic of preventing a failure from constantly recurring during maintenance, temporary external system failure, or unexpected system difficulties.\nThe basic idea behind the circuit breaker is very simple. We wrap a protected function call in a circuit breaker object, which monitors for failures. Once the failures reach a certain threshold, the circuit breaker trips, and all further calls to the circuit breaker return with an error, without the protected call being made at all. Usually, we\u0026rsquo;ll also want some kind of monitor alert if the circuit breaker trips.\nWhy do we need circuit breaking? It\u0026rsquo;s common for software systems to make remote calls to software running in different processes, probably on different machines across a network. One of the big differences between in-memory calls and remote calls is that remote calls can fail, or hang without a response until some timeout limit is reached. What\u0026rsquo;s worse if we have many callers on an unresponsive supplier, then we can run out of critical resources leading to cascading failures across multiple systems.\nStates Let\u0026rsquo;s discuss circuit breaker states:\nClosed When everything is normal, the circuit breakers remain closed, and all the request passes through to the services as normal. If the number of failures increases beyond the threshold, the circuit breaker trips and goes into an open state.\nOpen In this state circuit breaker returns an error immediately without even invoking the services. The Circuit breakers move into the half-open state after a certain timeout period elapses. Usually, it will have a monitoring system where the timeout will be specified.\nHalf-open In this state, the circuit breaker allows a limited number of requests from the service to pass through and invoke the operation. If the requests are successful, then the circuit breaker will go to the closed state. However, if the requests continue to fail, then it goes back to the open state.\nRate Limiting Rate limiting refers to preventing the frequency of an operation from exceeding a defined limit. In large-scale systems, rate limiting is commonly used to protect underlying services and resources. Rate limiting is generally used as a defensive mechanism in distributed systems, so that shared resources can maintain availability. It also protects our APIs from unintended or malicious overuse by limiting the number of requests that can reach our API in a given period of time.\nWhy do we need Rate Limiting? Rate limiting is a very important part of any large-scale system and it can be used to accomplish the following:\nAvoid resource starvation as a result of Denial of Service (DoS) attacks. Rate Limiting helps in controlling operational costs by putting a virtual cap on the auto-scaling of resources which if not monitored might lead to exponential bills. Rate limiting can be used as defense or mitigation against some common attacks. For APIs that process massive amounts of data, rate limiting can be used to control the flow of that data. Algorithms There are various algorithms for API rate limiting, each with its advantages and disadvantages. Let\u0026rsquo;s briefly discuss some of these algorithms:\nLeaky Bucket Leaky Bucket is an algorithm that provides a simple, intuitive approach to rate limiting via a queue. When registering a request, the system appends it to the end of the queue. Processing for the first item on the queue occurs at a regular interval or first-in, first-out (FIFO). If the queue is full, then additional requests are discarded (or leaked).\nToken Bucket Here we use a concept of a bucket. When a request comes in, a token from the bucket must be taken and processed. The request will be refused if no token is available in the bucket, and the requester will have to try again later. As a result, the token bucket gets refreshed after a certain time period.\nFixed Window The system uses a window size of n seconds to track the fixed window algorithm rate. Each incoming request increments the counter for the window. It discards the request if the counter exceeds a threshold.\nSliding Log Sliding Log rate-limiting involves tracking a time-stamped log for each request. The system stores these logs in a time-sorted hash set or table. It also discards logs with timestamps beyond a threshold. When a new request comes in, we calculate the sum of logs to determine the request rate. If the request would exceed the threshold rate, then it is held.\nSliding Window Sliding Window is a hybrid approach that combines the fixed window algorithm\u0026rsquo;s low processing cost and the sliding log\u0026rsquo;s improved boundary conditions. Like the fixed window algorithm, we track a counter for each fixed window. Next, we account for a weighted value of the previous window\u0026rsquo;s request rate based on the current timestamp to smooth out bursts of traffic.\nRate Limiting in Distributed Systems Rate Limiting becomes complicated when distributed systems are involved. The two broad problems that come with rate limiting in distributed systems are:\nInconsistencies When using a cluster of multiple nodes, we might need to enforce a global rate limit policy. Because if each node were to track its rate limit, a consumer could exceed a global rate limit when sending requests to different nodes. The greater the number of nodes, the more likely the user will exceed the global limit.\nThe simplest way to solve this problem is to use sticky sessions in our load balancers so that each consumer gets sent to exactly one node but this causes a lack of fault tolerance and scaling problems. Another approach might be to use a centralized data store like Redis but this will increase latency and cause race conditions.\nRace Conditions This issue happens when we use a naive \u0026ldquo;get-then-set\u0026rdquo; approach, in which we retrieve the current rate limit counter, increment it, and then push it back to the datastore. This model\u0026rsquo;s problem is that additional requests can come through in the time it takes to perform a full cycle of read-increment-store, each attempting to store the increment counter with an invalid (lower) counter value. This allows a consumer to send a very large number of requests to bypass the rate limiting controls.\nOne way to avoid this problem is to use some sort of distributed locking mechanism around the key, preventing any other processes from accessing or writing to the counter. Though the lock will become a significant bottleneck and will not scale well. A better approach might be to use a \u0026ldquo;set-then-get\u0026rdquo; approach, allowing us to quickly increment and check counter values without letting the atomic operations get in the way.\nService Discovery Service discovery is the detection of services within a computer network. Service Discovery Protocol (SDP) is a networking standard that accomplishes the detection of networks by identifying resources.\nWhy do we need Service Discovery? In a monolithic application, services invoke one another through language-level methods or procedure calls. However, modern microservices-based applications typically run in virtualized or containerized environments where the number of instances of a service and their locations change dynamically. Consequently, we need a mechanism that enables the clients of service to make requests to a dynamically changing set of ephemeral service instances.\nImplementations There are two main service discovery patterns:\nClient-side discovery In this approach, the client obtains the location of another service by querying a service registry which is responsible for managing and storing the network locations of all the services.\nServer-side discovery In this approach, we use an intermediate component such as a load balancer. The client makes a request to the service via a load balancer which then forwards the request to an available service instance.\nService Registry A service registry is basically a database containing the network locations of service instances to which the clients can reach out. A Service Registry must be highly available and up-to-date.\nService Registration We also need a way to obtain service information, often known as service registration. Let\u0026rsquo;s look at two possible service registration approaches:\nSelf-Registration When using the self-registration model, a service instance is responsible for registering and de-registering itself in the Service Registry. In addition, if necessary, a service instance sends heartbeat requests to keep its registration alive.\nThird-party Registration The registry keeps track of changes to running instances by polling the deployment environment or subscribing to events. When it detects a newly available service instance, it records it in its database. The Service Registry also de-registers terminated service instances.\nService mesh Service-to-service communication is essential in a distributed application but routing this communication, both within and across application clusters, becomes increasingly complex as the number of services grows. Service mesh enables managed, observable, and secure communication between individual services. It works with a service discovery protocol to detect services. Istio and envoy are some of the most commonly used service mesh technologies.\nExamples Here are some commonly used service discovery infrastructure tools:\netcd Consul Apache Thrift Apache Zookeeper SLA, SLO, SLI Let\u0026rsquo;s briefly discuss SLA, SLO, and SLI. These are mostly related to the business and site reliability side of things but good to know nonetheless.\nWhy are they important? SLAs, SLOs, and SLIs allow companies to define, track and monitor the promises made for a service to its users. Together, SLAs, SLOs, and SLIs should help teams generate more user trust in their services with an added emphasis on continuous improvement to incident management and response processes.\nSLA An SLA, or Service Level Agreement, is an agreement made between a company and its users of a given service. The SLA defines the different promises that the company makes to users regarding specific metrics, such as service availability.\nSLAs are often written by a company\u0026rsquo;s business or legal team.\nSLO An SLO, or Service Level Objective, is the promise that a company makes to users regarding a specific metric such as incident response or uptime. SLOs exist within an SLA as individual promises contained within the full user agreement. The SLO is the specific goal that the service must meet in order to comply with the SLA. SLOs should always be simple, clearly defined, and easily measured to determine whether or not the objective is being fulfilled.\nSLI An SLI, or Service Level Indicator, is a key metric used to determine whether or not the SLO is being met. It is the measured value of the metric described within the SLO. In order to remain in compliance with the SLA, the SLI\u0026rsquo;s value must always meet or exceed the value determined by the SLO.\nDisaster recovery Disaster recovery (DR) is a process of regaining access and functionality of the infrastructure after events like a natural disaster, cyber attack, or even business disruptions.\nDisaster recovery relies upon the replication of data and computer processing in an off-premises location not affected by the disaster. When servers go down because of a disaster, a business needs to recover lost data from a second location where the data is backed up. Ideally, an organization can transfer its computer processing to that remote location as well in order to continue operations.\nDisaster Recovery is often not actively discussed during system design interviews but it\u0026rsquo;s important to have some basic understanding of this topic. You can learn more about disaster recovery from AWS Well-Architected Framework.\nWhy is disaster recovery important? Disaster recovery can have the following benefits:\nMinimize interruption and downtime Limit damages Fast restoration Better customer retention Terms Let\u0026rsquo;s discuss some important terms relevantly for disaster recovery:\nRTO Recovery Time Objective (RTO) is the maximum acceptable delay between the interruption of service and restoration of service. This determines what is considered an acceptable time window when service is unavailable.\nRPO Recovery Point Objective (RPO) is the maximum acceptable amount of time since the last data recovery point. This determines what is considered an acceptable loss of data between the last recovery point and the interruption of service.\nStrategies A variety of disaster recovery (DR) strategies can be part of a disaster recovery plan.\nBack-up This is the simplest type of disaster recovery and involves storing data off-site or on a removable drive.\nCold Site In this type of disaster recovery, an organization sets up basic infrastructure in a second site.\nHot site A hot site maintains up-to-date copies of data at all times. Hot sites are time-consuming to set up and more expensive than cold sites, but they dramatically reduce downtime.\nVirtual Machines (VMs) and Containers Before we discuss virtualization vs containerization, let\u0026rsquo;s learn what are virtual machines (VMs) and Containers.\nVirtual Machines (VM) A Virtual Machine (VM) is a virtual environment that functions as a virtual computer system with its own CPU, memory, network interface, and storage, created on a physical hardware system. A software called a hypervisor separates the machine\u0026rsquo;s resources from the hardware and provisions them appropriately so they can be used by the VM.\nVMs are isolated from the rest of the system, and multiple VMs can exist on a single piece of hardware, like a server. They can be moved between host servers depending on the demand or to use resources more efficiently.\nWhat is a Hypervisor? A Hypervisor sometimes called a Virtual Machine Monitor (VMM), isolates the operating system and resources from the virtual machines and enables the creation and management of those VMs. The hypervisor treats resources like CPU, memory, and storage as a pool of resources that can be easily reallocated between existing guests or new virtual machines.\nWhy use a Virtual Machine? Server consolidation is a top reason to use VMs. Most operating system and application deployments only use a small amount of the physical resources available. By virtualizing our servers, we can place many virtual servers onto each physical server to improve hardware utilization. This keeps us from needing to purchase additional physical resources.\nA VM provides an environment that is isolated from the rest of a system, so whatever is running inside a VM won\u0026rsquo;t interfere with anything else running on the host hardware. Because VMs are isolated, they are a good option for testing new applications or setting up a production environment. We can also run a single-purpose VM to support a specific use case.\nContainers A container is a standard unit of software that packages up code and all its dependencies such as specific versions of runtimes and libraries so that the application runs quickly and reliably from one computing environment to another. Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of the target environment.\nWhy do we need containers? Let\u0026rsquo;s discuss some advantages of using containers:\nSeparation of responsibility\nContainerization provides a clear separation of responsibility, as developers focus on application logic and dependencies, while operations teams can focus on deployment and management.\nWorkload portability\nContainers can run virtually anywhere, greatly easing development and deployment.\nApplication isolation\nContainers virtualize CPU, memory, storage, and network resources at the operating system level, providing developers with a view of the OS logically isolated from other applications.\nAgile development\nContainers allow developers to move much more quickly by avoiding concerns about dependencies and environments.\nEfficient operations\nContainers are lightweight and allow us to use just the computing resources we need.\nVirtualization vs Containerization In traditional virtualization, a hypervisor virtualizes physical hardware. The result is that each virtual machine contains a guest OS, a virtual copy of the hardware that the OS requires to run, and an application and its associated libraries and dependencies.\nInstead of virtualizing the underlying hardware, containers virtualize the operating system so each container contains only the application and its dependencies making them much more lightweight than VMs. Containers also share the OS kernel and use a fraction of the memory VMs require.\nOAuth 2.0 and OpenID Connect (OIDC) OAuth 2.0 OAuth 2.0, which stands for Open Authorization, is a standard designed to provide consented access to resources on behalf of the user, without ever sharing the user\u0026rsquo;s credentials. OAuth 2.0 is an authorization protocol and not an authentication protocol, it is designed primarily as a means of granting access to a set of resources, for example, remote APIs or user\u0026rsquo;s data.\nConcepts The OAuth 2.0 protocol defines the following entities:\nResource Owner: The user or system that owns the protected resources and can grant access to them. Client: The client is the system that requires access to the protected resources. Authorization Server: This server receives requests from the Client for Access Tokens and issues them upon successful authentication and consent by the Resource Owner. Resource Server: A server that protects the user\u0026rsquo;s resources and receives access requests from the Client. It accepts and validates an Access Token from the Client and returns the appropriate resources. Scopes: They are used to specify exactly the reason for which access to resources may be granted. Acceptable scope values, and which resources they relate to, are dependent on the Resource Server. Access Token: A piece of data that represents the authorization to access resources on behalf of the end-user. How does OAuth 2.0 work? Let\u0026rsquo;s learn how OAuth 2.0 works:\nThe client requests authorization from the Authorization Server, supplying the client id and secret as identification. It also provides the scopes and an endpoint URI to send the Access Token or the Authorization Code. The Authorization Server authenticates the client and verifies that the requested scopes are permitted. The resource owner interacts with the authorization server to grant access. The Authorization Server redirects back to the client with either an Authorization Code or Access Token, depending on the grant type. A Refresh Token may also be returned. With the Access Token, the client can request access to the resource from the Resource Server. Disadvantages Here are the most common disadvantages of OAuth 2.0:\nLacks built-in security features. No standard implementation. No common set of scopes. OpenID Connect OAuth 2.0 is designed only for authorization, for granting access to data and features from one application to another. OpenID Connect (OIDC) is a thin layer that sits on top of OAuth 2.0 that adds login and profile information about the person who is logged in.\nWhen an Authorization Server supports OIDC, it is sometimes called an identity provider (IdP), since it provides information about the Resource Owner back to the Client. OpenID Connect is relatively new, resulting in lower adoption and industry implementation of best practices compared to OAuth.\nConcepts The OpenID Connect (OIDC) protocol defines the following entities:\nRelying Party: The current application. OpenID Provider: This is essentially an intermediate service that provides a one-time code to the Relying Party. Token Endpoint: A web server that accepts the One-Time Code (OTC) and provides an access code that\u0026rsquo;s valid for an hour. The main difference between OIDC and OAuth 2.0 is that the token is provided using JSON Web Token (JWT). UserInfo Endpoint: The Relying Party communicates with this endpoint, providing a secure token and receiving information about the end-user Both OAuth 2.0 and OIDC are easy to implement and are JSON based, which is supported by most web and mobile applications. However, the OpenID Connect (OIDC) specification is more strict than that of basic OAuth.\nSingle Sign-On (SSO) Single Sign-On (SSO) is an authentication process in which a user is provided access to multiple applications or websites by using only a single set of login credentials. This prevents the need for the user to log separately into the different applications.\nThe user credentials and other identifying information are stored and managed by a centralized system called Identity Provider (IdP). The Identity Provider is a trusted system that provides access to other websites and applications.\nSingle Sign-On (SSO) based authentication systems are commonly used in enterprise environments where employees require access to multiple applications of their organizations.\nComponents Let\u0026rsquo;s discuss some key components of Single Sign-On (SSO).\nIdentity Provider (IdP) User Identity information is stored and managed by a centralized system called Identity Provider (IdP). The Identity Provider authenticates the user and provides access to the service provider.\nThe identity provider can directly authenticate the user by validating a username and password or by validating an assertion about the user\u0026rsquo;s identity as presented by a separate identity provider. The identity provider handles the management of user identities in order to free the service provider from this responsibility.\nService Provider A service provider provides services to the end-user. They rely on identity providers to assert the identity of a user, and typically certain attributes about the user are managed by the identity provider. Service providers may also maintain a local account for the user along with attributes that are unique to their service.\nIdentity Broker An identity broker acts as an intermediary that connects multiple service providers with various different identity providers. Using Identity Broker, we can perform single sign-on over any application without the hassle of the protocol it follows.\nSAML Security Assertion Markup Language is an open standard that allows clients to share security information about identity, authentication, and permission across different systems. SAML is implemented with the Extensible Markup Language (XML) standard for sharing data.\nSAML specifically enables identity federation, making it possible for identity providers (IdPs) to seamlessly and securely pass authenticated identities and their attributes to service providers.\nHow does SSO work? Now, let\u0026rsquo;s discuss how Single Sign-On works:\nThe user requests a resource from their desired application. The application redirects the user to the Identity Provider (IdP) for authentication. The user signs in with their credentials (usually, username and password). Identity Provider (IdP) sends a Single Sign-On response back to the client application. The application grants access to the user. SAML vs OAuth 2.0 and OpenID Connect (OIDC) There are many differences between SAML, OAuth, and OIDC. SAML uses XML to pass messages, while OAuth and OIDC use JSON. OAuth provides a simpler experience, while SAML is geared towards enterprise security.\nOAuth and OIDC use RESTful communication extensively, which is why mobile, and modern web applications find OAuth and OIDC a better experience for the user. SAML, on the other hand, drops a session cookie in a browser that allows a user to access certain web pages. This is great for short-lived workloads.\nOIDC is developer-friendly and simpler to implement, which broadens the use cases for which it might be implemented. It can be implemented from scratch pretty fast, via freely available libraries in all common programming languages. SAML can be complex to install and maintain, which only enterprise-size companies can handle well.\nOpenID Connect is essentially a layer on top of the OAuth framework. Therefore, it can offer a built-in layer of permission that asks a user to agree to what the service provider might access. Although SAML is also capable of allowing consent flow, it achieves this by hard-coding carried out by a developer and not as part of its protocol.\nBoth of these authentication protocols are good at what they do. As always, a lot depends on our specific use cases and target audience.\nAdvantages Following are the benefits of using Single Sign-On:\nEase of use as users only need to remember one set of credentials. Ease of access without having to go through a lengthy authorization process. Enforced security and compliance to protect sensitive data. Simplifying the management with reduced IT support cost and admin time. Disadvantages Here are some disadvantages of Single Sign-On:\nSingle Password Vulnerability, if the main SSO password gets compromised, all the supported applications get compromised. The authentication process using Single Sign-On is slower than traditional authentication as every application has to request the SSO provider for verification. Examples These are some commonly used Identity Providers (IdP):\nOkta Google Auth0 OneLogin SSL, TLS, mTLS Let\u0026rsquo;s briefly discuss some important communication security protocols such as SSL, TLS, and mTLS. I would say that from a \u0026ldquo;big picture\u0026rdquo; system design perspective, this topic is not very important but still good to know about.\nSSL SSL stands for Secure Sockets Layer, and it refers to a protocol for encrypting and securing communications that take place on the internet. It was first developed in 1995 but since has been deprecated in favor of TLS (Transport Layer Security).\nWhy is it called an SSL certificate if it is deprecated? Most major certificate providers still refer to certificates as SSL certificates, which is why the naming convention persists.\nWhy was SSL so important? Originally, data on the web was transmitted in plaintext that anyone could read if they intercepted the message. SSL was created to correct this problem and protect user privacy. By encrypting any data that goes between the user and a web server, SSL also stops certain kinds of cyber attacks by preventing attackers from tampering with data in transit.\nTLS Transport Layer Security, or TLS, is a widely adopted security protocol designed to facilitate privacy and data security for communications over the internet. TLS evolved from a previous encryption protocol called Secure Sockets Layer (SSL). A primary use case of TLS is encrypting the communication between web applications and servers.\nThere are three main components to what the TLS protocol accomplishes:\nEncryption: hides the data being transferred from third parties. Authentication: ensures that the parties exchanging information are who they claim to be. Integrity: verifies that the data has not been forged or tampered with. mTLS Mutual TLS, or mTLS, is a method for mutual authentication. mTLS ensures that the parties at each end of a network connection are who they claim to be by verifying that they both have the correct private key. The information within their respective TLS certificates provides additional verification.\nWhy use mTLS? mTLS helps ensure that the traffic is secure and trusted in both directions between a client and server. This provides an additional layer of security for users who log in to an organization\u0026rsquo;s network or applications. It also verifies connections with client devices that do not follow a login process, such as Internet of Things (IoT) devices.\nNowadays, mTLS is commonly used by microservices or distributed systems in a zero trust security model to verify each other.\nSystem Design Interviews System design is a very extensive topic and system design interviews are designed to evaluate your capability to produce technical solutions to abstract problems, as such, they\u0026rsquo;re not designed for a specific answer. The unique aspect of system design interviews is the two-way nature between the candidate and the interviewer.\nExpectations are quite different at different engineering levels as well. Because someone with a lot of practical experience will approach it quite differently from someone who\u0026rsquo;s new in the industry. As a result, it\u0026rsquo;s hard to come up with a single strategy that will help us stay organized during the interview.\nLet\u0026rsquo;s look at some common strategies for the system design interviews:\nRequirements clarifications System design interview questions, by nature, are vague or abstract. Asking questions about the exact scope of the problem, and clarifying functional requirements early in the interview is essential. Usually, requirements are divided into three parts:\nFunctional requirements These are the requirements that the end user specifically demands as basic functionalities that the system should offer. All these functionalities need to be necessarily incorporated into the system as part of the contract.\nFor example:\n\u0026ldquo;What are the features that we need to design for this system?\u0026rdquo; \u0026ldquo;What are the edge cases we need to consider, if any, in our design?\u0026rdquo; Non-functional requirements These are the quality constraints that the system must satisfy according to the project contract. The priority or extent to which these factors are implemented varies from one project to another. They are also called non-behavioral requirements. For example, portability, maintainability, reliability, scalability, security, etc.\nFor example:\n\u0026ldquo;Each request should be processed with the minimum latency\u0026rdquo; \u0026ldquo;System should be highly available\u0026rdquo; Extended requirements These are basically \u0026ldquo;nice to have\u0026rdquo; requirements that might be out of the scope of the system.\nFor example:\n\u0026ldquo;Our system should record metrics and analytics\u0026rdquo; \u0026ldquo;Service health and performance monitoring?\u0026rdquo; Estimation and Constraints Estimate the scale of the system we\u0026rsquo;re going to design. It is important to ask questions such as:\n\u0026ldquo;What is the desired scale that this system will need to handle?\u0026rdquo; \u0026ldquo;What is the read/write ratio of our system?\u0026rdquo; \u0026ldquo;How many requests per second?\u0026rdquo; \u0026ldquo;How much storage will be needed?\u0026rdquo; These questions will help us scale our design later.\nData model design Once we have the estimations, we can start with defining the database schema. Doing so in the early stages of the interview would help us to understand the data flow which is the core of every system. In this step, we basically define all the entities and relationships between them.\n\u0026ldquo;What are the different entities in the system?\u0026rdquo; \u0026ldquo;What are the relationships between these entities?\u0026rdquo; \u0026ldquo;How many tables do we need?\u0026rdquo; \u0026ldquo;Is NoSQL a better choice here?\u0026rdquo; API design Next, we can start designing APIs for the system. These APIs will help us define the expectations from the system explicitly. We don\u0026rsquo;t have to write any code, just a simple interface defining the API requirements such as parameters, functions, classes, types, entities, etc.\nFor example:\ncreateUser(name: string, email: string): User It is advised to keep the interface as simple as possible and come back to it later when covering extended requirements.\nHigh-level component design Now we have established our data model and API design, it\u0026rsquo;s time to identify system components (such as Load Balancers, API Gateway, etc.) that are needed to solve our problem and draft the first design of our system.\n\u0026ldquo;Is it best to design a monolithic or a microservices architecture?\u0026rdquo; \u0026ldquo;What type of database should we use?\u0026rdquo; Once we have a basic diagram, we can start discussing with the interviewer how the system will work from the client\u0026rsquo;s perspective.\nDetailed design Now it\u0026rsquo;s time to go into detail about the major components of the system we designed. As always discuss with the interviewer which component may need further improvements.\nHere is a good opportunity to demonstrate your experience in the areas of your expertise. Present different approaches, advantages, and disadvantages. Explain your design decisions, and back them up with examples. This is also a good time to discuss any additional features the system might be able to support, though this is optional.\n\u0026ldquo;How should we partition our data?\u0026rdquo; \u0026ldquo;What about load distribution?\u0026rdquo; \u0026ldquo;Should we use cache?\u0026rdquo; \u0026ldquo;How will we handle a sudden spike in traffic?\u0026rdquo; Also, try not to be too opinionated about certain technologies, statements like \u0026ldquo;I believe that NoSQL databases are just better, SQL databases are not scalable\u0026rdquo; reflect poorly. As someone who has interviewed a lot of people over the years, my two cents here would be to be humble about what you know and what you do not. Use your existing knowledge with examples to navigate this part of the interview.\nIdentify and resolve bottlenecks Finally, it\u0026rsquo;s time to discuss bottlenecks and approaches to mitigate them. Here are some important questions to ask:\n\u0026ldquo;Do we have enough database replicas?\u0026rdquo; \u0026ldquo;Is there any single point of failure?\u0026rdquo; \u0026ldquo;Is database sharding required?\u0026rdquo; \u0026ldquo;How can we make our system more robust?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; Make sure to read the engineering blog of the company you\u0026rsquo;re interviewing with. This will help you get a sense of what technology stack they\u0026rsquo;re using and which problems are important to them.\nURL Shortener Let\u0026rsquo;s design a URL shortener, similar to services like Bitly, TinyURL.\nWhat is a URL Shortener? A URL shortener service creates an alias or a short URL for a long URL. Users are redirected to the original URL when they visit these short links.\nFor example, the following long URL can be changed to a shorter URL.\nLong URL: https://karanpratapsingh.com/courses/system-design/url-shortener\nShort URL: https://bit.ly/3I71d3o\nWhy do we need a URL shortener? URL shortener saves space in general when we are sharing URLs. Users are also less likely to mistype shorter URLs. Moreover, we can also optimize links across devices, this allows us to track individual links.\nRequirements Our URL shortening system should meet the following requirements:\nFunctional requirements Given a URL, our service should generate a shorter and unique alias for it. Users should be redirected to the original URL when they visit the short link. Links should expire after a default timespan. Non-functional requirements High availability with minimal latency. The system should be scalable and efficient. Extended requirements Prevent abuse of services. Record analytics and metrics for redirections. Estimation and Constraints Let\u0026rsquo;s start with the estimation and constraints.\nNote: Make sure to check any scale or traffic related assumptions with your interviewer.\nTraffic This will be a read-heavy system, so let\u0026rsquo;s assume a 100:1 read/write ratio with 100 million links generated per month.\nReads/Writes Per month\nFor reads per month:\n$$ 100 \\times 100 \\space million = 10 \\space billion/month $$\nSimilarly for writes:\n$$ 1 \\times 100 \\space million = 100 \\space million/month $$\nWhat would be Requests Per Second (RPS) for our system?\n100 million requests per month translate into 40 requests per second.\n$$ \\frac{100 \\space million}{(30 \\space days \\times 24 \\space hrs \\times 3600 \\space seconds)} = \\sim 40 \\space URLs/second $$\nAnd with a 100:1 read/write ratio, the number of redirections will be:\n$$ 100 \\times 40 \\space URLs/second = 4000 \\space requests/second $$\nBandwidth Since we expect about 40 URLs every second, and if we assume each request is of size 500 bytes then the total incoming data for then write requests would be:\n$$ 40 \\times 500 \\space bytes = 20 \\space KB/second $$\nSimilarly, for the read requests, since we expect about 4K redirections, the total outgoing data would be:\n$$ 4000 \\space URLs/second \\times 500 \\space bytes = \\sim 2 \\space MB/second $$\nStorage For storage, we will assume we store each link or record in our database for 10 years. Since we expect around 100M new requests every month, the total number of records we will need to store would be:\n$$ 100 \\space million \\times 10\\space years \\times 12 \\space months = 12 \\space billion $$\nLike earlier, if we assume each stored recorded will be approximately 500 bytes. We will need around 6TB of storage:\n$$ 12 \\space billion \\times 500 \\space bytes = 6 \\space TB $$\nCache For caching, we will follow the classic Pareto principle also known as the 80/20 rule. This means that 80% of the requests are for 20% of the data, so we can cache around 20% of our requests.\nSince we get around 4K read or redirection requests each second. This translates into 350M requests per day.\n$$ 4000 \\space URLs/second \\times 24 \\space hours \\times 3600 \\space seconds = \\sim 350 \\space million \\space requests/day $$\nHence, we will need around 35GB of memory per day.\n$$ 20 \\space percent \\times 350 \\space million \\times 500 \\space bytes = 35 \\space GB/day $$\nHigh-level estimate Here is our high-level estimate:\nType Estimate Writes (New URLs) 40/s Reads (Redirection) 4K/s Bandwidth (Incoming) 20 KB/s Bandwidth (Outgoing) 2 MB/s Storage (10 years) 6 TB Memory (Caching) ~35 GB/day Data model design Next, we will focus on the data model design. Here is our database schema:\nInitially, we can get started with just two tables:\nusers\nStores user\u0026rsquo;s details such as name, email, createdAt, etc.\nurls\nContains the new short URL\u0026rsquo;s properties such as expiration, hash, originalURL, and userID of the user who created the short URL. We can also use the hash column as an index to improve the query performance.\nWhat kind of database should we use? Since the data is not strongly relational, NoSQL databases such as Amazon DynamoDB, Apache Cassandra, or MongoDB will be a better choice here, if we do decide to use an SQL database then we can use something like Azure SQL Database or Amazon RDS.\nFor more details, refer to SQL vs NoSQL.\nAPI design Let us do a basic API design for our services:\nCreate URL This API should create a new short URL in our system given an original URL.\ncreateURL(apiKey: string, originalURL: string, expiration?: Date): string Parameters\nAPI Key (string): API key provided by the user.\nOriginal Url (string): Original URL to be shortened.\nExpiration (Date): Expiration date of the new URL (optional).\nReturns\nShort URL (string): New shortened URL.\nGet URL This API should retrieve the original URL from a given short URL.\ngetURL(apiKey: string, shortURL: string): string Parameters\nAPI Key (string): API key provided by the user.\nShort Url (string): Short URL mapped to the original URL.\nReturns\nOriginal URL (string): Original URL to be retrieved.\nDelete URL This API should delete a given shortURL from our system.\ndeleteURL(apiKey: string, shortURL: string): boolean Parameters\nAPI Key (string): API key provided by the user.\nShort Url (string): Short URL to be deleted.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nWhy do we need an API key? As you must\u0026rsquo;ve noticed, we\u0026rsquo;re using an API key to prevent abuse of our services. Using this API key we can limit the users to a certain number of requests per second or minute. This is quite a standard practice for developer APIs and should cover our extended requirement.\nHigh-level design Now let us do a high-level design of our system.\nURL Encoding Our system\u0026rsquo;s primary goal is to shorten a given URL, let\u0026rsquo;s look at different approaches:\nBase62 Approach\nIn this approach, we can encode the original URL using Base62 which consists of the capital letters A-Z, the lower case letters a-z, and the numbers 0-9.\n$$ Number \\space of \\space URLs = 62^N $$\nWhere,\nN: Number of characters in the generated URL.\nSo, if we want to generate a URL that is 7 characters long, we will generate ~3.5 trillion different URLs.\n$$ \\begin{gather*} 62^5 = \\sim 916 \\space million \\space URLs \\ 62^6 = \\sim 56.8 \\space billion \\space URLs \\ 62^7 = \\sim 3.5 \\space trillion \\space URLs \\end{gather*} $$\nThis is the simplest solution here, but it does not guarantee non-duplicate or collision-resistant keys.\nMD5 Approach\nThe MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value (or 32 hexadecimal digits). We can use these 32 hexadecimal digits for generating 7 characters long URL.\n$$ MD5(original_url) \\rightarrow base62encode \\rightarrow hash $$\nHowever, this creates a new issue for us, which is duplication and collision. We can try to re-compute the hash until we find a unique one but that will increase the overhead of our systems. It\u0026rsquo;s better to look for more scalable approaches.\nCounter Approach\nIn this approach, we will start with a single server which will maintain the count of the keys generated. Once our service receives a request, it can reach out to the counter which returns a unique number and increments the counter. When the next request comes the counter again returns the unique number and this goes on.\n$$ Counter(0-3.5 \\space trillion) \\rightarrow base62encode \\rightarrow hash $$\nThe problem with this approach is that it can quickly become a single point for failure. And if we run multiple instances of the counter we can have collision as it\u0026rsquo;s essentially a distributed system.\nTo solve this issue we can use a distributed system manager such as Zookeeper which can provide distributed synchronization. Zookeeper can maintain multiple ranges for our servers.\n$$ \\begin{align*} \u0026amp; Range \\space 1: \\space 1 \\rightarrow 1,000,000 \\ \u0026amp; Range \\space 2: \\space 1,000,001 \\rightarrow 2,000,000 \\ \u0026amp; Range \\space 3: \\space 2,000,001 \\rightarrow 3,000,000 \\ \u0026amp; \u0026hellip; \\end{align*} $$\nOnce a server reaches its maximum range Zookeeper will assign an unused counter range to the new server. This approach can guarantee non-duplicate and collision-resistant URLs. Also, we can run multiple instances of Zookeeper to remove the single point of failure.\nKey Generation Service (KGS) As we discussed, generating a unique key at scale without duplication and collisions can be a bit of a challenge. To solve this problem, we can create a standalone Key Generation Service (KGS) that generates a unique key ahead of time and stores it in a separate database for later use. This approach can make things simple for us.\nHow to handle concurrent access?\nOnce the key is used, we can mark it in the database to make sure we don\u0026rsquo;t reuse it, however, if there are multiple server instances reading data concurrently, two or more servers might try to use the same key.\nThe easiest way to solve this would be to store keys in two tables. As soon as a key is used, we move it to a separate table with appropriate locking in place. Also, to improve reads, we can keep some of the keys in memory.\nKGS database estimations\nAs per our discussion, we can generate up to ~56.8 billion unique 6 character long keys which will result in us having to store 300 GB of keys.\n$$ 6 \\space characters \\times 56.8 \\space billion = \\sim 390 \\space GB $$\nWhile 390 GB seems like a lot for this simple use case, it is important to remember this is for the entirety of our service lifetime and the size of the keys database would not increase like our main database.\nCaching Now, let\u0026rsquo;s talk about caching. As per our estimations, we will require around ~35 GB of memory per day to cache 20% of the incoming requests to our services. For this use case, we can use Redis or Memcached servers alongside our API server.\nFor more details, refer to caching.\nDesign Now that we have identified some core components, let\u0026rsquo;s do the first draft of our system design.\nHere\u0026rsquo;s how it works:\nCreating a new URL\nWhen a user creates a new URL, our API server requests a new unique key from the Key Generation Service (KGS). Key Generation Service provides a unique key to the API server and marks the key as used. API server writes the new URL entry to the database and cache. Our service returns an HTTP 201 (Created) response to the user. Accessing a URL\nWhen a client navigates to a certain short URL, the request is sent to the API servers. The request first hits the cache, and if the entry is not found there then it is retrieved from the database and an HTTP 301 (Redirect) is issued to the original URL. If the key is still not found in the database, an HTTP 404 (Not found) error is sent to the user. Detailed design It\u0026rsquo;s time to discuss the finer details of our design.\nData Partitioning To scale out our databases we will need to partition our data. Horizontal partitioning (aka Sharding) can be a good first step. We can use partitions schemes such as:\nHash-Based Partitioning List-Based Partitioning Range Based Partitioning Composite Partitioning The above approaches can still cause uneven data and load distribution, we can solve this using Consistent hashing.\nFor more details, refer to Sharding and Consistent Hashing.\nDatabase cleanup This is more of a maintenance step for our services and depends on whether we keep the expired entries or remove them. If we do decide to remove expired entries, we can approach this in two different ways:\nActive cleanup\nIn active cleanup, we will run a separate cleanup service which will periodically remove expired links from our storage and cache. This will be a very lightweight service like a cron job.\nPassive cleanup\nFor passive cleanup, we can remove the entry when a user tries to access an expired link. This can ensure a lazy cleanup of our database and cache.\nCache Now let us talk about caching.\nWhich cache eviction policy to use?\nAs we discussed before, we can use solutions like Redis or Memcached and cache 20% of the daily traffic but what kind of cache eviction policy would best fit our needs?\nLeast Recently Used (LRU) can be a good policy for our system. In this policy, we discard the least recently used key first.\nHow to handle cache miss?\nWhenever there is a cache miss, our servers can hit the database directly and update the cache with the new entries.\nMetrics and Analytics Recording analytics and metrics is one of our extended requirements. We can store and update metadata like visitor\u0026rsquo;s country, platform, the number of views, etc alongside the URL entry in our database.\nSecurity For security, we can introduce private URLs and authorization. A separate table can be used to store user ids that have permission to access a specific URL. If a user does not have proper permissions, we can return an HTTP 401 (Unauthorized) error.\nWe can also use an API Gateway as they can support capabilities like authorization, rate limiting, and load balancing out of the box.\nIdentify and resolve bottlenecks Let us identify and resolve bottlenecks such as single points of failure in our design:\n\u0026ldquo;What if the API service or Key Generation Service crashes?\u0026rdquo; \u0026ldquo;How will we distribute our traffic between our components?\u0026rdquo; \u0026ldquo;How can we reduce the load on our database?\u0026rdquo; \u0026ldquo;What if the key database used by KGS fails?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; To make our system more resilient we can do the following:\nRunning multiple instances of our Servers and Key Generation Service. Introducing load balancers between clients, servers, databases, and cache servers. Using multiple read replicas for our database as it\u0026rsquo;s a read-heavy system. Standby replica for our key database in case it fails. Multiple instances and replicas for our distributed cache. WhatsApp Let\u0026rsquo;s design a WhatsApp like instant messaging service, similar to services like WhatsApp, Facebook Messenger, and WeChat.\nWhat is WhatsApp? WhatsApp is a chat application that provides instant messaging services to its users. It is one of the most used mobile applications on the planet connecting over 2 billion users in 180+ countries. WhatsApp is also available on the web.\nRequirements Our system should meet the following requirements:\nFunctional requirements Should support one-on-one chat. Group chats (max 100 people). Should support file sharing (image, video, etc.). Non-functional requirements High availability with minimal latency. The system should be scalable and efficient. Extended requirements Sent, Delivered, and Read receipts of the messages. Show the last seen time of users. Push notifications. Estimation and Constraints Let\u0026rsquo;s start with the estimation and constraints.\nNote: Make sure to check any scale or traffic-related assumptions with your interviewer.\nTraffic Let us assume we have 50 million daily active users (DAU) and on average each user sends at least 10 messages to 4 different people every day. This gives us 2 billion messages per day.\n$$ 50 \\space million \\times 20 \\space messages = 2 \\space billion/day $$\nMessages can also contain media such as images, videos, or other files. We can assume that 5 percent of messages are media files shared by the users, which gives us additional 200 million files we would need to store.\n$$ 5 \\space percent \\times 2 \\space billion = 200 \\space million/day $$\nWhat would be Requests Per Second (RPS) for our system?\n2 billion requests per day translate into 24K requests per second.\n$$ \\frac{2 \\space billion}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 24K \\space requests/second $$\nStorage If we assume each message on average is 100 bytes, we will require about 200 GB of database storage every day.\n$$ 2 \\space billion \\times 100 \\space bytes = \\sim 200 \\space GB/day $$\nAs per our requirements, we also know that around 5 percent of our daily messages (100 million) are media files. If we assume each file is 50 KB on average, we will require 10 TB of storage every day.\n$$ 100 \\space million \\times 100 \\space KB = 10 \\space TB/day $$\nAnd for 10 years, we will require about 38 PB of storage.\n$$ (10 \\space TB + 0.2 \\space TB) \\times 10 \\space years \\times 365 \\space days = \\sim 38 \\space PB $$\nBandwidth As our system is handling 10.2 TB of ingress every day, we will require a minimum bandwidth of around 120 MB per second.\n$$ \\frac{10.2 \\space TB}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 120 \\space MB/second $$\nHigh-level estimate Here is our high-level estimate:\nType Estimate Daily active users (DAU) 50 million Requests per second (RPS) 24K/s Storage (per day) ~10.2 TB Storage (10 years) ~38 PB Bandwidth ~120 MB/s Data model design This is the general data model which reflects our requirements.\nWe have the following tables:\nusers\nThis table will contain a user\u0026rsquo;s information such as name, phoneNumber, and other details.\nmessages\nAs the name suggests, this table will store messages with properties such as type (text, image, video, etc.), content, and timestamps for message delivery. The message will also have a corresponding chatID or groupID.\nchats\nThis table basically represents a private chat between two users and can contain multiple messages.\nusers_chats\nThis table maps users and chats as multiple users can have multiple chats (N:M relationship) and vice versa.\ngroups\nThis table represents a group between multiple users.\nusers_groups\nThis table maps users and groups as multiple users can be a part of multiple groups (N:M relationship) and vice versa.\nWhat kind of database should we use? While our data model seems quite relational, we don\u0026rsquo;t necessarily need to store everything in a single database, as this can limit our scalability and quickly become a bottleneck.\nWe will split the data between different services each having ownership over a particular table. Then we can use a relational database such as PostgreSQL or a distributed NoSQL database such as Apache Cassandra for our use case.\nAPI design Let us do a basic API design for our services:\nGet all chats or groups This API will get all chats or groups for a given userID.\ngetAll(userID: UUID): Chat[] | Group[] Parameters\nUser ID (UUID): ID of the current user.\nReturns\nResult (Chat[] | Group[]): All the chats and groups the user is a part of.\nGet messages Get all messages for a user given the channelID (chat or group id).\ngetMessages(userID: UUID, channelID: UUID): Message[] Parameters\nUser ID (UUID): ID of the current user.\nChannel ID (UUID): ID of the channel (chat or group) from which messages need to be retrieved.\nReturns\nMessages (Message[]): All the messages in a given chat or group.\nSend message Send a message from a user to a channel (chat or group).\nsendMessage(userID: UUID, channelID: UUID, message: Message): boolean Parameters\nUser ID (UUID): ID of the current user.\nChannel ID (UUID): ID of the channel (chat or group) user wants to send a message to.\nMessage (Message): The message (text, image, video, etc.) that the user wants to send.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nJoin or leave a group Send a message from a user to a channel (chat or group).\njoinGroup(userID: UUID, channelID: UUID): boolean leaveGroup(userID: UUID, channelID: UUID): boolean Parameters\nUser ID (UUID): ID of the current user.\nChannel ID (UUID): ID of the channel (chat or group) the user wants to join or leave.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nHigh-level design Now let us do a high-level design of our system.\nArchitecture We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model. Let\u0026rsquo;s try to divide our system into some core services.\nUser Service\nThis is an HTTP-based service that handles user-related concerns such as authentication and user information.\nChat Service\nThe chat service will use WebSockets and establish connections with the client to handle chat and group message-related functionality. We can also use cache to keep track of all the active connections sort of like sessions which will help us determine if the user is online or not.\nNotification Service\nThis service will simply send push notifications to the users. It will be discussed in detail separately.\nPresence Service\nThe presence service will keep track of the last seen status of all users. It will be discussed in detail separately.\nMedia service\nThis service will handle the media (images, videos, files, etc.) uploads. It will be discussed in detail separately.\nWhat about inter-service communication and service discovery?\nSince our architecture is microservices-based, services will be communicating with each other as well. Generally, REST or HTTP performs well but we can further improve the performance using gRPC which is more lightweight and efficient.\nService discovery is another thing we will have to take into account. We can also use a service mesh that enables managed, observable, and secure communication between individual services.\nNote: Learn more about REST, GraphQL, gRPC and how they compare with each other.\nReal-time messaging How do we efficiently send and receive messages? We have two different options:\nPull model\nThe client can periodically send an HTTP request to servers to check if there are any new messages. This can be achieved via something like Long polling.\nPush model\nThe client opens a long-lived connection with the server and once new data is available it will be pushed to the client. We can use WebSockets or Server-Sent Events (SSE) for this.\nThe pull model approach is not scalable as it will create unnecessary request overhead on our servers and most of the time the response will be empty, thus wasting our resources. To minimize latency, using the push model with WebSockets is a better choice because then we can push data to the client once it\u0026rsquo;s available without any delay given the connection is open with the client. Also, WebSockets provide full-duplex communication, unlike Server-Sent Events (SSE) which are only unidirectional.\nNote: Learn more about Long polling, WebSockets, Server-Sent Events (SSE).\nLast seen To implement the last seen functionality, we can use a heartbeat mechanism, where the client can periodically ping the servers indicating its liveness. Since this needs to be as low overhead as possible, we can store the last active timestamp in the cache as follows:\nKey Value User A 2022-07-01T14:32:50 User B 2022-07-05T05:10:35 User C 2022-07-10T04:33:25 This will give us the last time the user was active. This functionality will be handled by the presence service combined with Redis or Memcached as our cache.\nAnother way to implement this is to track the latest action of the user, once the last activity crosses a certain threshold, such as \u0026ldquo;user hasn\u0026rsquo;t performed any action in the last 30 seconds\u0026rdquo;, we can show the user as offline and last seen with the last recorded timestamp. This will be more of a lazy update approach and might benefit us over heartbeat in certain cases.\nNotifications Once a message is sent in a chat or a group, we will first check if the recipient is active or not, we can get this information by taking the user\u0026rsquo;s active connection and last seen into consideration.\nIf the recipient is not active, the chat service will add an event to a message queue with additional metadata such as the client\u0026rsquo;s device platform which will be used to route the notification to the correct platform later on.\nThe notification service will then consume the event from the message queue and forward the request to Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNS) based on the client\u0026rsquo;s device platform (Android, iOS, web, etc). We can also add support for email and SMS.\nWhy are we using a message queue?\nSince most message queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they\u0026rsquo;re sent and that a message is delivered at least once which is an important part of our service functionality.\nWhile this seems like a classic publish-subscribe use case, it is actually not as mobile devices and browsers each have their own way of handling push notifications. Usually, notifications are handled externally via Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNS) unlike message fan-out which we commonly see in backend services. We can use something like Amazon SQS or RabbitMQ to support this functionality.\nRead receipts Handling read receipts can be tricky, for this use case we can wait for some sort of Acknowledgment (ACK) from the client to determine if the message was delivered and update the corresponding deliveredAt field. Similarly, we will mark message the message seen once the user opens the chat and update the corresponding seenAt timestamp field.\nDesign Now that we have identified some core components, let\u0026rsquo;s do the first draft of our system design.\nDetailed design It\u0026rsquo;s time to discuss our design decisions in detail.\nData Partitioning To scale out our databases we will need to partition our data. Horizontal partitioning (aka Sharding) can be a good first step. We can use partitions schemes such as:\nHash-Based Partitioning List-Based Partitioning Range Based Partitioning Composite Partitioning The above approaches can still cause uneven data and load distribution, we can solve this using Consistent hashing.\nFor more details, refer to Sharding and Consistent Hashing.\nCaching In a messaging application, we have to be careful about using cache as our users expect the latest data, but many users will be requesting the same messages, especially in a group chat. So, to prevent usage spikes from our resources we can cache older messages.\nSome group chats can have thousands of messages and sending that over the network will be really inefficient, to improve efficiency we can add pagination to our system APIs. This decision will be helpful for users with limited network bandwidth as they won\u0026rsquo;t have to retrieve old messages unless requested.\nWhich cache eviction policy to use?\nWe can use solutions like Redis or Memcached and cache 20% of the daily traffic but what kind of cache eviction policy would best fit our needs?\nLeast Recently Used (LRU) can be a good policy for our system. In this policy, we discard the least recently used key first.\nHow to handle cache miss?\nWhenever there is a cache miss, our servers can hit the database directly and update the cache with the new entries.\nFor more details, refer to Caching.\nMedia access and storage As we know, most of our storage space will be used for storing media files such as images, videos, or other files. Our media service will be handling both access and storage of the user media files.\nBut where can we store files at scale? Well, object storage is what we\u0026rsquo;re looking for. Object stores break data files up into pieces called objects. It then stores those objects in a single repository, which can be spread out across multiple networked systems. We can also use distributed file storage such as HDFS or GlusterFS.\nFun fact: WhatsApp deletes media on its servers once it has been downloaded by the user.\nWe can use object stores like Amazon S3, Azure Blob Storage, or Google Cloud Storage for this use case.\nContent Delivery Network (CDN) Content Delivery Network (CDN) increases content availability and redundancy while reducing bandwidth costs. Generally, static files such as images, and videos are served from CDN. We can use services like Amazon CloudFront or Cloudflare CDN for this use case.\nAPI gateway Since we will be using multiple protocols like HTTP, WebSocket, TCP/IP, deploying multiple L4 (transport layer) or L7 (application layer) type load balancers separately for each protocol will be expensive. Instead, we can use an API Gateway that supports multiple protocols without any issues.\nAPI Gateway can also offer other features such as authentication, authorization, rate limiting, throttling, and API versioning which will improve the quality of our services.\nWe can use services like Amazon API Gateway or Azure API Gateway for this use case.\nIdentify and resolve bottlenecks Let us identify and resolve bottlenecks such as single points of failure in our design:\n\u0026ldquo;What if one of our services crashes?\u0026rdquo; \u0026ldquo;How will we distribute our traffic between our components?\u0026rdquo; \u0026ldquo;How can we reduce the load on our database?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; \u0026ldquo;Wouldn\u0026rsquo;t API Gateway be a single point of failure?\u0026rdquo; \u0026ldquo;How can we make our notification system more robust?\u0026rdquo; \u0026ldquo;How can we reduce media storage costs\u0026rdquo;? \u0026ldquo;Does chat service has too much responsibility?\u0026rdquo; To make our system more resilient we can do the following:\nRunning multiple instances of each of our services. Introducing load balancers between clients, servers, databases, and cache servers. Using multiple read replicas for our databases. Multiple instances and replicas for our distributed cache. We can have a standby replica of our API Gateway. Exactly once delivery and message ordering is challenging in a distributed system, we can use a dedicated message broker such as Apache Kafka or NATS to make our notification system more robust. We can add media processing and compression capabilities to the media service to compress large files similar to WhatsApp which will save a lot of storage space and reduce cost. We can create a group service separate from the chat service to further decouple our services. Twitter Let\u0026rsquo;s design a Twitter like social media service, similar to services like Facebook, Instagram, etc.\nWhat is Twitter? Twitter is a social media service where users can read or post short messages (up to 280 characters) called tweets. It is available on the web and mobile platforms such as Android and iOS.\nRequirements Our system should meet the following requirements:\nFunctional requirements Should be able to post new tweets (can be text, image, video, etc.). Should be able to follow other users. Should have a newsfeed feature consisting of tweets from the people the user is following. Should be able to search tweets. Non-Functional requirements High availability with minimal latency. The system should be scalable and efficient. Extended requirements Metrics and analytics. Retweet functionality. Favorite tweets. Estimation and Constraints Let\u0026rsquo;s start with the estimation and constraints.\nNote: Make sure to check any scale or traffic-related assumptions with your interviewer.\nTraffic This will be a read-heavy system, let us assume we have 1 billion total users with 200 million daily active users (DAU), and on average each user tweets 5 times a day. This gives us 1 billion tweets per day.\n$$ 200 \\space million \\times 5 \\space messages = 1 \\space billion/day $$\nTweets can also contain media such as images, or videos. We can assume that 10 percent of tweets are media files shared by the users, which gives us additional 100 million files we would need to store.\n$$ 10 \\space percent \\times 1 \\space billion = 100 \\space million/day $$\nWhat would be Requests Per Second (RPS) for our system?\n1 billion requests per day translate into 12K requests per second.\n$$ \\frac{1 \\space billion}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 12K \\space requests/second $$\nStorage If we assume each message on average is 100 bytes, we will require about 100 GB of database storage every day.\n$$ 1 \\space billion \\times 100 \\space bytes = \\sim 100 \\space GB/day $$\nWe also know that around 10 percent of our daily messages (100 million) are media files per our requirements. If we assume each file is 50 KB on average, we will require 5 TB of storage every day.\n$$ 100 \\space million \\times 100 \\space KB = 5 \\space TB/day $$\nAnd for 10 years, we will require about 19 PB of storage.\n$$ (5 \\space TB + 0.1 \\space TB) \\times 365 \\space days \\times 10 \\space years = \\sim 19 \\space PB $$\nBandwidth As our system is handling 5.1 TB of ingress every day, we will require a minimum bandwidth of around 60 MB per second.\n$$ \\frac{5.1 \\space TB}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 60 \\space MB/second $$\nHigh-level estimate Here is our high-level estimate:\nType Estimate Daily active users (DAU) 100 million Requests per second (RPS) 12K/s Storage (per day) ~5.1 TB Storage (10 years) ~19 PB Bandwidth ~60 MB/s Data model design This is the general data model which reflects our requirements.\nWe have the following tables:\nusers\nThis table will contain a user\u0026rsquo;s information such as name, email, dob, and other details.\ntweets\nAs the name suggests, this table will store tweets and their properties such as type (text, image, video, etc.), content, etc. We will also store the corresponding userID.\nfavorites\nThis table maps tweets with users for the favorite tweets functionality in our application.\nfollowers\nThis table maps the followers and followees as users can follow each other (N:M relationship).\nfeeds\nThis table stores feed properties with the corresponding userID.\nfeeds_tweets\nThis table maps tweets and feed (N:M relationship).\nWhat kind of database should we use? While our data model seems quite relational, we don\u0026rsquo;t necessarily need to store everything in a single database, as this can limit our scalability and quickly become a bottleneck.\nWe will split the data between different services each having ownership over a particular table. Then we can use a relational database such as PostgreSQL or a distributed NoSQL database such as Apache Cassandra for our use case.\nAPI design Let us do a basic API design for our services:\nPost a tweet This API will allow the user to post a tweet on the platform.\npostTweet(userID: UUID, content: string, mediaURL?: string): boolean Parameters\nUser ID (UUID): ID of the user.\nContent (string): Contents of the tweet.\nMedia URL (string): URL of the attached media (optional).\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nFollow or unfollow a user This API will allow the user to follow or unfollow another user.\nfollow(followerID: UUID, followeeID: UUID): boolean unfollow(followerID: UUID, followeeID: UUID): boolean Parameters\nFollower ID (UUID): ID of the current user.\nFollowee ID (UUID): ID of the user we want to follow or unfollow.\nMedia URL (string): URL of the attached media (optional).\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nGet newsfeed This API will return all the tweets to be shown within a given newsfeed.\ngetNewsfeed(userID: UUID): Tweet[] Parameters\nUser ID (UUID): ID of the user.\nReturns\nTweets (Tweet[]): All the tweets to be shown within a given newsfeed.\nHigh-level design Now let us do a high-level design of our system.\nArchitecture We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model. Let\u0026rsquo;s try to divide our system into some core services.\nUser Service\nThis service handles user-related concerns such as authentication and user information.\nNewsfeed Service\nThis service will handle the generation and publishing of user newsfeeds. It will be discussed in detail separately.\nTweet Service\nThe tweet service will handle tweet-related use cases such as posting a tweet, favorites, etc.\nSearch Service\nThe service is responsible for handling search-related functionality. It will be discussed in detail separately.\nMedia service\nThis service will handle the media (images, videos, files, etc.) uploads. It will be discussed in detail separately.\nNotification Service\nThis service will simply send push notifications to the users.\nAnalytics Service\nThis service will be used for metrics and analytics use cases.\nWhat about inter-service communication and service discovery?\nSince our architecture is microservices-based, services will be communicating with each other as well. Generally, REST or HTTP performs well but we can further improve the performance using gRPC which is more lightweight and efficient.\nService discovery is another thing we will have to take into account. We can also use a service mesh that enables managed, observable, and secure communication between individual services.\nNote: Learn more about REST, GraphQL, gRPC and how they compare with each other.\nNewsfeed When it comes to the newsfeed, it seems easy enough to implement, but there are a lot of things that can make or break this feature. So, let\u0026rsquo;s divide our problem into two parts:\nGeneration\nLet\u0026rsquo;s assume we want to generate the feed for user A, we will perform the following steps:\nRetrieve the IDs of all the users and entities (hashtags, topics, etc.) user A follows. Fetch the relevant tweets for each of the retrieved IDs. Use a ranking algorithm to rank the tweets based on parameters such as relevance, time, engagement, etc. Return the ranked tweets data to the client in a paginated manner. Feed generation is an intensive process and can take quite a lot of time, especially for users following a lot of people. To improve the performance, the feed can be pre-generated and stored in the cache, then we can have a mechanism to periodically update the feed and apply our ranking algorithm to the new tweets.\nPublishing\nPublishing is the step where the feed data is pushed according to each specific user. This can be a quite heavy operation, as a user may have millions of friends or followers. To deal with this, we have three different approaches:\nPull Model (or Fan-out on load) When a user creates a tweet, and a follower reloads their newsfeed, the feed is created and stored in memory. The most recent feed is only loaded when the user requests it. This approach reduces the number of write operations on our database.\nThe downside of this approach is that the users will not be able to view recent feeds unless they \u0026ldquo;pull\u0026rdquo; the data from the server, which will increase the number of read operations on the server.\nPush Model (or Fan-out on write) In this model, once a user creates a tweet, it is \u0026ldquo;pushed\u0026rdquo; to all the follower\u0026rsquo;s feeds immediately. This prevents the system from having to go through a user\u0026rsquo;s entire followers list to check for updates.\nHowever, the downside of this approach is that it would increase the number of write operations on the database.\nHybrid Model A third approach is a hybrid model between the pull and push model. It combines the beneficial features of the above two models and tries to provide a balanced approach between the two.\nThe hybrid model allows only users with a lesser number of followers to use the push model and for users with a higher number of followers celebrities, the pull model will be used.\nRanking Algorithm As we discussed, we will need a ranking algorithm to rank each tweet according to its relevance to each specific user.\nFor example, Facebook used to utilize an EdgeRank algorithm, here, the rank of each feed item is described by:\n$$ Rank = Affinity \\times Weight \\times Decay $$\nWhere,\nAffinity: is the \u0026ldquo;closeness\u0026rdquo; of the user to the creator of the edge. If a user frequently likes, comments, or messages the edge creator, then the value of affinity will be higher, resulting in a higher rank for the post.\nWeight: is the value assigned according to each edge. A comment can have a higher weightage than likes, and thus a post with more comments is more likely to get a higher rank.\nDecay: is the measure of the creation of the edge. The older the edge, the lesser will be the value of decay and eventually the rank.\nNowadays, algorithms are much more complex and ranking is done using machine learning models which can take thousands of factors into consideration.\nRetweets Retweets are one of our extended requirements. To implement this feature we can simply create a new tweet with the user id of the user retweeting the original tweet and then modify the type enum and content property of the new tweet to link it with the original tweet.\nFor example, the type enum property can be of type tweet, similar to text, video, etc and content can be the id of the original tweet. Here the first row indicates the original tweet while the second row is how we can represent a retweet.\nid userID type content createdAt ad34-291a-45f6-b36c 7a2c-62c4-4dc8-b1bb text Hey, this is my first tweet… 1658905644054 f064-49ad-9aa2-84a6 6aa2-2bc9-4331-879f tweet ad34-291a-45f6-b36c 1658906165427 This is a very basic implementation, to improve this we can create a separate table itself to store retweets.\nSearch Sometimes traditional DBMS are not performant enough, we need something which allows us to store, search, and analyze huge volumes of data quickly and in near real-time and give results within milliseconds. Elasticsearch can help us with this use case.\nElasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. It is built on top of Apache Lucene.\nHow do we identify trending topics?\nTrending functionality will be based on top of the search functionality. We can cache the most frequently searched queries, hashtags, and topics in the last N seconds and update them every M seconds using some sort of batch job mechanism. Our ranking algorithm can also be applied to the trending topics to give them more weight and personalize them for the user.\nNotifications Push notifications are an integral part of any social media platform. We can use a message queue or a message broker such as Apache Kafka with the notification service to dispatch requests to Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNS) which will handle the delivery of the push notifications to user devices.\nFor more details, refer to the WhatsApp system design where we discuss push notifications.\nDetailed design It\u0026rsquo;s time to discuss our design decisions in detail.\nData Partitioning To scale out our databases we will need to partition our data. Horizontal partitioning (aka Sharding) can be a good first step. We can use partitions schemes such as:\nHash-Based Partitioning List-Based Partitioning Range Based Partitioning Composite Partitioning The above approaches can still cause uneven data and load distribution, we can solve this using Consistent hashing.\nFor more details, refer to Sharding and Consistent Hashing.\nMutual friends For mutual friends, we can build a social graph for every user. Each node in the graph will represent a user and a directional edge will represent followers and followees. After that, we can traverse the followers of a user to find and suggest a mutual friend. This would require a graph database such as Neo4j and ArangoDB.\nThis is a pretty simple algorithm, to improve our suggestion accuracy, we will need to incorporate a recommendation model which uses machine learning as part of our algorithm.\nMetrics and Analytics Recording analytics and metrics is one of our extended requirements. As we will be using Apache Kafka to publish all sorts of events, we can process these events and run analytics on the data using Apache Spark which is an open-source unified analytics engine for large-scale data processing.\nCaching In a social media application, we have to be careful about using cache as our users expect the latest data. So, to prevent usage spikes from our resources we can cache the top 20% of the tweets.\nTo further improve efficiency we can add pagination to our system APIs. This decision will be helpful for users with limited network bandwidth as they won\u0026rsquo;t have to retrieve old messages unless requested.\nWhich cache eviction policy to use?\nWe can use solutions like Redis or Memcached and cache 20% of the daily traffic but what kind of cache eviction policy would best fit our needs?\nLeast Recently Used (LRU) can be a good policy for our system. In this policy, we discard the least recently used key first.\nHow to handle cache miss?\nWhenever there is a cache miss, our servers can hit the database directly and update the cache with the new entries.\nFor more details, refer to Caching.\nMedia access and storage As we know, most of our storage space will be used for storing media files such as images, videos, or other files. Our media service will be handling both access and storage of the user media files.\nBut where can we store files at scale? Well, object storage is what we\u0026rsquo;re looking for. Object stores break data files up into pieces called objects. It then stores those objects in a single repository, which can be spread out across multiple networked systems. We can also use distributed file storage such as HDFS or GlusterFS.\nContent Delivery Network (CDN) Content Delivery Network (CDN) increases content availability and redundancy while reducing bandwidth costs. Generally, static files such as images, and videos are served from CDN. We can use services like Amazon CloudFront or Cloudflare CDN for this use case.\nIdentify and resolve bottlenecks Let us identify and resolve bottlenecks such as single points of failure in our design:\n\u0026ldquo;What if one of our services crashes?\u0026rdquo; \u0026ldquo;How will we distribute our traffic between our components?\u0026rdquo; \u0026ldquo;How can we reduce the load on our database?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; \u0026ldquo;How can we make our notification system more robust?\u0026rdquo; \u0026ldquo;How can we reduce media storage costs\u0026rdquo;? To make our system more resilient we can do the following:\nRunning multiple instances of each of our services. Introducing load balancers between clients, servers, databases, and cache servers. Using multiple read replicas for our databases. Multiple instances and replicas for our distributed cache. Exactly once delivery and message ordering is challenging in a distributed system, we can use a dedicated message broker such as Apache Kafka or NATS to make our notification system more robust. We can add media processing and compression capabilities to the media service to compress large files which will save a lot of storage space and reduce cost. Netflix Let\u0026rsquo;s design a Netflix like video streaming service, similar to services like Amazon Prime Video, Disney Plus, Hulu, Youtube, Vimeo, etc.\nWhat is Netflix? Netflix is a subscription-based streaming service that allows its members to watch TV shows and movies on an internet-connected device. It is available on platforms such as the Web, iOS, Android, TV, etc.\nRequirements Our system should meet the following requirements:\nFunctional requirements Users should be able to stream and share videos. The content team (or users in YouTube\u0026rsquo;s case) should be able to upload new videos (movies, tv shows episodes, and other content). Users should be able to search for videos using titles or tags. Users should be able to comment on a video similar to YouTube. Non-Functional requirements High availability with minimal latency. High reliability, no uploads should be lost. The system should be scalable and efficient. Extended requirements Certain content should be geo-blocked. Resume video playback from the point user left off. Record metrics and analytics of videos. Estimation and Constraints Let\u0026rsquo;s start with the estimation and constraints.\nNote: Make sure to check any scale or traffic-related assumptions with your interviewer.\nTraffic This will be a read-heavy system, let us assume we have 1 billion total users with 200 million daily active users (DAU), and on average each user watches 5 videos a day. This gives us 1 billion videos watched per day.\n$$ 200 \\space million \\times 5 \\space videos = 1 \\space billion/day $$\nAssuming, a 200:1 read/write ratio, about 50 million videos will be uploaded every day.\n$$ \\frac{1}{200} \\times 1 \\space billion = 50 \\space million/day $$\nWhat would be Requests Per Second (RPS) for our system?\n1 billion requests per day translate into 12K requests per second.\n$$ \\frac{1 \\space billion}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 12K \\space requests/second $$\nStorage If we assume each video is 100 MB on average, we will require about 5 PB of storage every day.\n$$ 50 \\space million \\times 100 \\space MB = 5 \\space PB/day $$\nAnd for 10 years, we will require an astounding 18,250 PB of storage.\n$$ 5 \\space PB \\times 365 \\space days \\times 10 \\space years = \\sim 18,250 \\space PB $$\nBandwidth As our system is handling 5 PB of ingress every day, we will require a minimum bandwidth of around 58 GB per second.\n$$ \\frac{5 \\space PB}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 58 \\space GB/second $$\nHigh-level estimate Here is our high-level estimate:\nType Estimate Daily active users (DAU) 200 million Requests per second (RPS) 12K/s Storage (per day) ~5 PB Storage (10 years) ~18,250 PB Bandwidth ~58 GB/s Data model design This is the general data model which reflects our requirements.\nWe have the following tables:\nusers\nThis table will contain a user\u0026rsquo;s information such as name, email, dob, and other details.\nvideos\nAs the name suggests, this table will store videos and their properties such as title, streamURL, tags, etc. We will also store the corresponding userID.\ntags\nThis table will simply store tags associated with a video.\nviews\nThis table helps us to store all the views received on a video.\ncomments\nThis table stores all the comments received on a video (like YouTube).\nWhat kind of database should we use? While our data model seems quite relational, we don\u0026rsquo;t necessarily need to store everything in a single database, as this can limit our scalability and quickly become a bottleneck.\nWe will split the data between different services each having ownership over a particular table. Then we can use a relational database such as PostgreSQL or a distributed NoSQL database such as Apache Cassandra for our use case.\nAPI design Let us do a basic API design for our services:\nUpload a video Given a byte stream, this API enables video to be uploaded to our service.\nuploadVideo(title: string, description: string, data: Stream\u0026lt;byte\u0026gt;, tags?: string[]): boolean Parameters\nTitle (string): Title of the new video.\nDescription (string): Description of the new video.\nData (Byte[]): Byte stream of the video data.\nTags (string[]): Tags for the video (optional).\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nStreaming a video This API allows our users to stream a video with the preferred codec and resolution.\nstreamVideo(videoID: UUID, codec: Enum\u0026lt;string\u0026gt;, resolution: Tuple\u0026lt;int\u0026gt;, offset?: int): VideoStream Parameters\nVideo ID (UUID): ID of the video that needs to be streamed.\nCodec (Enum\u0026lt;string\u0026gt;): Required codec of the requested video, such as h.265, h.264, VP9, etc.\nResolution (Tuple\u0026lt;int\u0026gt;): Resolution of the requested video.\nOffset (int): Offset of the video stream in seconds to stream data from any point in the video (optional).\nReturns\nStream (VideoStream): Data stream of the requested video.\nSearch for a video This API will enable our users to search for a video based on its title or tags.\nsearchVideo(query: string, nextPage?: string): Video[] Parameters\nQuery (string): Search query from the user.\nNext Page (string): Token for the next page, this can be used for pagination (optional).\nReturns\nVideos (Video[]): All the videos available for a particular search query.\nAdd a comment This API will allow our users to post a comment on a video (like YouTube).\ncomment(videoID: UUID, comment: string): boolean Parameters\nVideoID (UUID): ID of the video user wants to comment on.\nComment (string): The text content of the comment.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nHigh-level design Now let us do a high-level design of our system.\nArchitecture We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model. Let\u0026rsquo;s try to divide our system into some core services.\nUser Service\nThis service handles user-related concerns such as authentication and user information.\nStream Service\nThe tweet service will handle video streaming-related functionality.\nSearch Service\nThe service is responsible for handling search-related functionality. It will be discussed in detail separately.\nMedia service\nThis service will handle the video uploads and processing. It will be discussed in detail separately.\nAnalytics Service\nThis service will be used for metrics and analytics use cases.\nWhat about inter-service communication and service discovery?\nSince our architecture is microservices-based, services will be communicating with each other as well. Generally, REST or HTTP performs well but we can further improve the performance using gRPC which is more lightweight and efficient.\nService discovery is another thing we will have to take into account. We can also use a service mesh that enables managed, observable, and secure communication between individual services.\nNote: Learn more about REST, GraphQL, gRPC and how they compare with each other.\nVideo processing There are so many variables in play when it comes to processing a video. For example, an average data size of two-hour raw 8K footage from a high-end camera can easily be up to 4 TB, thus we need to have some kind of processing to reduce both storage and delivery costs.\nHere\u0026rsquo;s how we can process videos once they\u0026rsquo;re uploaded by the content team (or users in YouTube\u0026rsquo;s case) and are queued for processing in our message queue.\nLet\u0026rsquo;s discuss how this works:\nFile Chunker This is the first step of our processing pipeline. File chunking is the process of splitting a file into smaller pieces called chunks. It can help us eliminate duplicate copies of repeating data on storage, and reduces the amount of data sent over the network by only selecting changed chunks.\nUsually, a video file can be split into equal size chunks based on timestamps but Netflix instead splits chunks based on scenes, this slight variation becomes a huge factor for a better user experience as whenever the client requests a chunk from the server, there is a lower chance of interruption as a complete scene will be retrieved.\nContent Filter This step checks if the video adheres to the content policy of the platform, this can be pre-approved in the case of Netflix as per the content rating of the media or can be strictly enforced like YouTube.\nThis entire step is done by a machine learning model which performs copyright, piracy, and NSFW checks. If issues are found, we can push the task to a dead-letter queue (DLQ) and someone from the moderation team can do further inspection.\nTranscoder Transcoding is a process in which the original data is decoded to an intermediate uncompressed format, which is then encoded into the target format. This process uses different codecs to perform bitrate adjustment, image downsampling, or re-encoding the media.\nThis results in a smaller size file and a much more optimized format for the target devices. Standalone solutions such as FFmpeg or cloud-based solutions like AWS Elemental MediaConvert can be used to implement this step of the pipeline.\nQuality Conversion This is the last step of the processing pipeline and as the name suggests, this step handles the conversion of the transcoded media from the previous step into different resolutions such as 4K, 1440p, 1080p, 720p, etc.\nThis allows us to fetch the desired quality of the video as per the user\u0026rsquo;s request, and once the media file finishes processing, it will be uploaded to a distributed file storage such as HDFS, GlusterFS, or an object storage such as Amazon S3 for later retrieval during streaming.\nNote: We can add additional steps such as subtitles and thumbnails generation as part of our pipeline.\nWhy are we using a message queue?\nProcessing videos as a long-running task makes much more sense, and a message queue also decouples our video processing pipeline from the uploads functionality. We can use something like Amazon SQS or RabbitMQ to support this.\nVideo streaming Video streaming is a challenging task from both the client and server perspectives. Moreover, internet connection speeds vary quite a lot between different users. To make sure users don\u0026rsquo;t re-fetch the same content, we can use a Content Delivery Network (CDN).\nNetflix takes this a step further with its Open Connect program. In this approach, they partner with thousands of Internet Service Providers (ISPs) to localize their traffic and deliver their content more efficiently.\nWhat is the difference between Netflix\u0026rsquo;s Open Connect and a traditional Content Delivery Network (CDN)?\nNetflix Open Connect is our purpose-built Content Delivery Network (CDN) responsible for serving Netflix\u0026rsquo;s video traffic. Around 95% of the traffic globally is delivered via direct connections between Open Connect and the ISPs their customers use to access the internet.\nCurrently, they have Open Connect Appliances (OCAs) in over 1000 separate locations around the world. In case of issues, Open Connect Appliances (OCAs) can failover, and the traffic can be re-routed to Netflix servers.\nAdditionally, we can use Adaptive bitrate streaming protocols such as HTTP Live Streaming (HLS) which is designed for reliability and it dynamically adapts to network conditions by optimizing playback for the available speed of the connections.\nLastly, for playing the video from where the user left off (part of our extended requirements), we can simply use the offset property we stored in the views table to retrieve the scene chunk at that particular timestamp and resume the playback for the user.\nSearching Sometimes traditional DBMS are not performant enough, we need something which allows us to store, search, and analyze huge volumes of data quickly and in near real-time and give results within milliseconds. Elasticsearch can help us with this use case.\nElasticsearch is a distributed, free and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. It is built on top of Apache Lucene.\nHow do we identify trending content?\nTrending functionality will be based on top of the search functionality. We can cache the most frequently searched queries in the last N seconds and update them every M seconds using some sort of batch job mechanism.\nSharing Sharing content is an important part of any platform, for this, we can have some sort of URL shortener service in place that can generate short URLs for the users to share.\nFor more details, refer to the URL Shortener system design.\nDetailed design It\u0026rsquo;s time to discuss our design decisions in detail.\nData Partitioning To scale out our databases we will need to partition our data. Horizontal partitioning (aka Sharding) can be a good first step. We can use partitions schemes such as:\nHash-Based Partitioning List-Based Partitioning Range Based Partitioning Composite Partitioning The above approaches can still cause uneven data and load distribution, we can solve this using Consistent hashing.\nFor more details, refer to Sharding and Consistent Hashing.\nGeo-blocking Platforms like Netflix and YouTube use Geo-blocking to restrict content in certain geographical areas or countries. This is primarily done due to legal distribution laws that Netflix has to adhere to when they make a deal with the production and distribution companies. In the case of YouTube, this will be controlled by the user during the publishing of the content.\nWe can determine the user\u0026rsquo;s location either using their IP or region settings in their profile then use services like Amazon CloudFront which supports a geographic restrictions feature or a geolocation routing policy with Amazon Route53 to restrict the content and re-route the user to an error page if the content is not available in that particular region or country.\nRecommendations Netflix uses a machine learning model which uses the user\u0026rsquo;s viewing history to predict what the user might like to watch next, an algorithm like Collaborative Filtering can be used.\nHowever, Netflix (like YouTube) uses its own algorithm called Netflix Recommendation Engine which can track several data points such as:\nUser profile information like age, gender, and location. Browsing and scrolling behavior of the user. Time and date a user watched a title. The device which was used to stream the content. The number of searches and what terms were searched. For more detail, refer to Netflix recommendation research.\nMetrics and Analytics Recording analytics and metrics is one of our extended requirements. We can capture the data from different services and run analytics on the data using Apache Spark which is an open-source unified analytics engine for large-scale data processing. Additionally, we can store critical metadata in the views table to increase data points within our data.\nCaching In a streaming platform, caching is important. We have to be able to cache as much static media content as possible to improve user experience. We can use solutions like Redis or Memcached but what kind of cache eviction policy would best fit our needs?\nWhich cache eviction policy to use?\nLeast Recently Used (LRU) can be a good policy for our system. In this policy, we discard the least recently used key first.\nHow to handle cache miss?\nWhenever there is a cache miss, our servers can hit the database directly and update the cache with the new entries.\nFor more details, refer to Caching.\nMedia streaming and storage As most of our storage space will be used for storing media files such as thumbnails and videos. Per our discussion earlier, the media service will be handling both the upload and processing of media files.\nWe will use distributed file storage such as HDFS, GlusterFS, or an object storage such as Amazon S3 for storage and streaming of the content.\nContent Delivery Network (CDN) Content Delivery Network (CDN) increases content availability and redundancy while reducing bandwidth costs. Generally, static files such as images, and videos are served from CDN. We can use services like Amazon CloudFront or Cloudflare CDN for this use case.\nIdentify and resolve bottlenecks Let us identify and resolve bottlenecks such as single points of failure in our design:\n\u0026ldquo;What if one of our services crashes?\u0026rdquo; \u0026ldquo;How will we distribute our traffic between our components?\u0026rdquo; \u0026ldquo;How can we reduce the load on our database?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; To make our system more resilient we can do the following:\nRunning multiple instances of each of our services. Introducing load balancers between clients, servers, databases, and cache servers. Using multiple read replicas for our databases. Multiple instances and replicas for our distributed cache. Uber Let\u0026rsquo;s design an Uber like ride-hailing service, similar to services like Lyft, OLA Cabs, etc.\nWhat is Uber? Uber is a mobility service provider, allowing users to book rides and a driver to transport them in a way similar to a taxi. It is available on the web and mobile platforms such as Android and iOS.\nRequirements Our system should meet the following requirements:\nFunctional requirements We will design our system for two types of users: Customers and Drivers.\nCustomers\nCustomers should be able to see all the cabs in the vicinity with an ETA and pricing information. Customers should be able to book a cab to a destination. Customers should be able to see the location of the driver. Drivers\nDrivers should be able to accept or deny the customer requested ride. Once a driver accepts the ride, they should see the pickup location of the customer. Drivers should be able to mark the trip as complete on reaching the destination. Non-Functional requirements High reliability. High availability with minimal latency. The system should be scalable and efficient. Extended requirements Customers can rate the trip after it\u0026rsquo;s completed. Payment processing. Metrics and analytics. Estimation and Constraints Let\u0026rsquo;s start with the estimation and constraints.\nNote: Make sure to check any scale or traffic-related assumptions with your interviewer.\nTraffic Let us assume we have 100 million daily active users (DAU) with 1 million drivers and on average our platform enables 10 million rides daily.\nIf on average each user performs 10 actions (such as request a check available rides, fares, book rides, etc.) we will have to handle 1 billion requests daily.\n$$ 100 \\space million \\times 10 \\space actions = 1 \\space billion/day $$\nWhat would be Requests Per Second (RPS) for our system?\n1 billion requests per day translate into 12K requests per second.\n$$ \\frac{1 \\space billion}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 12K \\space requests/second $$\nStorage If we assume each message on average is 400 bytes, we will require about 400 GB of database storage every day.\n$$ 1 \\space billion \\times 400 \\space bytes = \\sim 400 \\space GB/day $$\nAnd for 10 years, we will require about 1.4 PB of storage.\n$$ 400 \\space GB \\times 10 \\space years \\times 365 \\space days = \\sim 1.4 \\space PB $$\nBandwidth As our system is handling 400 GB of ingress every day, we will require a minimum bandwidth of around 4 MB per second.\n$$ \\frac{400 \\space GB}{(24 \\space hrs \\times 3600 \\space seconds)} = \\sim 5 \\space MB/second $$\nHigh-level estimate Here is our high-level estimate:\nType Estimate Daily active users (DAU) 100 million Requests per second (RPS) 12K/s Storage (per day) ~400 GB Storage (10 years) ~1.4 PB Bandwidth ~5 MB/s Data model design This is the general data model which reflects our requirements.\nWe have the following tables:\ncustomers\nThis table will contain a customer\u0026rsquo;s information such as name, email, and other details.\ndrivers\nThis table will contain a driver\u0026rsquo;s information such as name, email, dob and other details.\ntrips\nThis table represents the trip taken by the customer and stores data such as source, destination, and status of the trip.\ncabs\nThis table stores data such as the registration number, and type (like Uber Go, Uber XL, etc.) of the cab that the driver will be driving.\nratings\nAs the name suggests, this table stores the rating and feedback for the trip.\npayments\nThe payments table contains the payment-related data with the corresponding tripID.\nWhat kind of database should we use? While our data model seems quite relational, we don\u0026rsquo;t necessarily need to store everything in a single database, as this can limit our scalability and quickly become a bottleneck.\nWe will split the data between different services each having ownership over a particular table. Then we can use a relational database such as PostgreSQL or a distributed NoSQL database such as Apache Cassandra for our use case.\nAPI design Let us do a basic API design for our services:\nRequest a Ride Through this API, customers will be able to request a ride.\nrequestRide(customerID: UUID, source: Tuple\u0026lt;float\u0026gt;, destination: Tuple\u0026lt;float\u0026gt;, cabType: Enum\u0026lt;string\u0026gt;, paymentMethod: Enum\u0026lt;string\u0026gt;): Ride Parameters\nCustomer ID (UUID): ID of the customer.\nSource (Tuple\u0026lt;float\u0026gt;): Tuple containing the latitude and longitude of the trip\u0026rsquo;s starting location.\nDestination (Tuple\u0026lt;float\u0026gt;): Tuple containing the latitude and longitude of the trip\u0026rsquo;s destination.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nCancel the Ride This API will allow customers to cancel the ride.\ncancelRide(customerID: UUID, reason?: string): boolean Parameters\nCustomer ID (UUID): ID of the customer.\nReason (UUID): Reason for canceling the ride (optional).\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nAccept or Deny the Ride This API will allow the driver to accept or deny the trip.\nacceptRide(driverID: UUID, rideID: UUID): boolean denyRide(driverID: UUID, rideID: UUID): boolean Parameters\nDriver ID (UUID): ID of the driver.\nRide ID (UUID): ID of the customer requested ride.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nStart or End the Trip Using this API, a driver will be able to start and end the trip.\nstartTrip(driverID: UUID, tripID: UUID): boolean endTrip(driverID: UUID, tripID: UUID): boolean Parameters\nDriver ID (UUID): ID of the driver.\nTrip ID (UUID): ID of the requested trip.\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nRate the Trip This API will enable customers to rate the trip.\nrateTrip(customerID: UUID, tripID: UUID, rating: int, feedback?: string): boolean Parameters\nCustomer ID (UUID): ID of the customer.\nTrip ID (UUID): ID of the completed trip.\nRating (int): Rating of the trip.\nFeedback (string): Feedback about the trip by the customer (optional).\nReturns\nResult (boolean): Represents whether the operation was successful or not.\nHigh-level design Now let us do a high-level design of our system.\nArchitecture We will be using microservices architecture since it will make it easier to horizontally scale and decouple our services. Each service will have ownership of its own data model. Let\u0026rsquo;s try to divide our system into some core services.\nCustomer Service\nThis service handles customer-related concerns such as authentication and customer information.\nDriver Service\nThis service handles driver-related concerns such as authentication and driver information.\nRide Service\nThis service will be responsible for ride matching and quadtree aggregation. It will be discussed in detail separately.\nTrip Service\nThis service handles trip-related functionality in our system.\nPayment Service\nThis service will be responsible for handling payments in our system.\nNotification Service\nThis service will simply send push notifications to the users. It will be discussed in detail separately.\nAnalytics Service\nThis service will be used for metrics and analytics use cases.\nWhat about inter-service communication and service discovery?\nSince our architecture is microservices-based, services will be communicating with each other as well. Generally, REST or HTTP performs well but we can further improve the performance using gRPC which is more lightweight and efficient.\nService discovery is another thing we will have to take into account. We can also use a service mesh that enables managed, observable, and secure communication between individual services.\nNote: Learn more about REST, GraphQL, gRPC and how they compare with each other.\nHow is the service expected to work? Here\u0026rsquo;s how our service is expected to work:\nCustomer requests a ride by specifying the source, destination, cab type, payment method, etc. Ride service registers this request, finds nearby drivers, and calculates the estimated time of arrival (ETA). The request is then broadcasted to the nearby drivers for them to accept or deny. If the driver accepts, the customer is notified about the live location of the driver with the estimated time of arrival (ETA) while they wait for pickup. The customer is picked up and the driver can start the trip. Once the destination is reached, the driver will mark the ride as complete and collect payment. After the payment is complete, the customer can leave a rating and feedback for the trip if they like. Location Tracking How do we efficiently send and receive live location data from the client (customers and drivers) to our backend? We have two different options:\nPull model\nThe client can periodically send an HTTP request to servers to report its current location and receive ETA and pricing information. This can be achieved via something like Long polling.\nPush model\nThe client opens a long-lived connection with the server and once new data is available it will be pushed to the client. We can use WebSockets or Server-Sent Events (SSE) for this.\nThe pull model approach is not scalable as it will create unnecessary request overhead on our servers and most of the time the response will be empty, thus wasting our resources. To minimize latency, using the push model with WebSockets is a better choice because then we can push data to the client once it\u0026rsquo;s available without any delay given the connection is open with the client. Also, WebSockets provide full-duplex communication, unlike Server-Sent Events (SSE) which are only unidirectional.\nAdditionally, the client application should have some sort of background job mechanism to ping GPS location while the application is in the background.\nNote: Learn more about Long polling, WebSockets, Server-Sent Events (SSE).\nRide Matching We need a way to efficiently store and query nearby drivers. Let\u0026rsquo;s explore different solutions we can incorporate into our design.\nSQL\nWe already have access to the latitude and longitude of our customers, and with databases like PostgreSQL and MySQL we can perform a query to find nearby driver locations given a latitude and longitude (X, Y) within a radius (R).\nSELECT * FROM locations WHERE lat BETWEEN X-R AND X+R AND long BETWEEN Y-R AND Y+R However, this is not scalable, and performing this query on large datasets will be quite slow.\nGeohashing\nGeohashing is a geocoding method used to encode geographic coordinates such as latitude and longitude into short alphanumeric strings. It was created by Gustavo Niemeyer in 2008.\nGeohash is a hierarchical spatial index that uses Base-32 alphabet encoding, the first character in a geohash identifies the initial location as one of the 32 cells. This cell will also contain 32 cells. This means that to represent a point, the world is recursively divided into smaller and smaller cells with each additional bit until the desired precision is attained. The precision factor also determines the size of the cell.\nFor example, San Francisco with coordinates 37.7564, -122.4016 can be represented in geohash as 9q8yy9mf.\nNow, using the customer\u0026rsquo;s geohash we can determine the nearest available driver by simply comparing it with the driver\u0026rsquo;s geohash. For better performance, we will index and store the geohash of the driver in memory for faster retrieval.\nQuadtrees\nA Quadtree is a tree data structure in which each internal node has exactly four children. They are often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions. Each child or leaf node stores spatial information. Quadtrees are the two-dimensional analog of Octrees which are used to partition three-dimensional space.\nQuadtrees enable us to search points within a two-dimensional range efficiently, where those points are defined as latitude/longitude coordinates or as cartesian (x, y) coordinates.\nWe can save further computation by only subdividing a node after a certain threshold.\nQuadtree seems perfect for our use case, we can update the Quadtree every time we receive a new location update from the driver. To reduce the load on the quadtree servers we can use an in-memory datastore such as Redis to cache the latest updates. And with the application of mapping algorithms such as the Hilbert curve, we can perform efficient range queries to find nearby drivers for the customer.\nWhat about race conditions?\nRace conditions can easily occur when a large number of customers will be requesting rides simultaneously. To avoid this, we can wrap our ride matching logic in a Mutex to avoid any race conditions. Furthermore, every action should be transactional in nature.\nFor more details, refer to Transactions and Distributed Transactions.\nHow to find the best drivers nearby?\nOnce we have a list of nearby drivers from the Quadtree servers, we can perform some sort of ranking based on parameters like average ratings, relevance, past customer feedback, etc. This will allow us to broadcast notifications to the best available drivers first.\nDealing with high demand\nIn cases of high demand, we can use the concept of Surge Pricing. Surge pricing is a dynamic pricing method where prices are temporarily increased as a reaction to increased demand and mostly limited supply. This surge price can be added to the base price of the trip.\nFor more details, learn how surge pricing works with Uber.\nPayments Handling payments at scale is challenging, to simplify our system we can use a third-party payment processor like Stripe or PayPal. Once the payment is complete, the payment processor will redirect the user back to our application and we can set up a webhook to capture all the payment-related data.\nNotifications Push notifications will be an integral part of our platform. We can use a message queue or a message broker such as Apache Kafka with the notification service to dispatch requests to Firebase Cloud Messaging (FCM) or Apple Push Notification Service (APNS) which will handle the delivery of the push notifications to user devices.\nFor more details, refer to the WhatsApp system design where we discuss push notifications.\nDetailed design It\u0026rsquo;s time to discuss our design decisions in detail.\nData Partitioning To scale out our databases we will need to partition our data. Horizontal partitioning (aka Sharding) can be a good first step. We can shard our database either based on existing partition schemes or regions. If we divide the locations into regions using let\u0026rsquo;s say zip codes, we can effectively store all the data in a given region on a fixed node. But this can still cause uneven data and load distribution, we can solve this using Consistent hashing.\nFor more details, refer to Sharding and Consistent Hashing.\nMetrics and Analytics Recording analytics and metrics is one of our extended requirements. We can capture the data from different services and run analytics on the data using Apache Spark which is an open-source unified analytics engine for large-scale data processing. Additionally, we can store critical metadata in the views table to increase data points within our data.\nCaching In a location services-based platform, caching is important. We have to be able to cache the recent locations of the customers and drivers for fast retrieval. We can use solutions like Redis or Memcached but what kind of cache eviction policy would best fit our needs?\nWhich cache eviction policy to use?\nLeast Recently Used (LRU) can be a good policy for our system. In this policy, we discard the least recently used key first.\nHow to handle cache miss?\nWhenever there is a cache miss, our servers can hit the database directly and update the cache with the new entries.\nFor more details, refer to Caching.\nIdentify and resolve bottlenecks Let us identify and resolve bottlenecks such as single points of failure in our design:\n\u0026ldquo;What if one of our services crashes?\u0026rdquo; \u0026ldquo;How will we distribute our traffic between our components?\u0026rdquo; \u0026ldquo;How can we reduce the load on our database?\u0026rdquo; \u0026ldquo;How to improve the availability of our cache?\u0026rdquo; \u0026ldquo;How can we make our notification system more robust?\u0026rdquo; To make our system more resilient we can do the following:\nRunning multiple instances of each of our services. Introducing load balancers between clients, servers, databases, and cache servers. Using multiple read replicas for our databases. Multiple instances and replicas for our distributed cache. Exactly once delivery and message ordering is challenging in a distributed system, we can use a dedicated message broker such as Apache Kafka or NATS to make our notification system more robust. Next Steps Congratulations, you\u0026rsquo;ve finished the course!\nNow that you know the fundamentals of System Design, here are some additional resources:\nDistributed Systems (by Dr. Martin Kleppmann) System Design Interview: An Insider\u0026rsquo;s Guide Microservices (by Chris Richardson) Serverless computing Kubernetes It is also recommended to actively follow engineering blogs of companies putting what we learned in the course into practice at scale:\nMicrosoft Engineering Google Research Blog Netflix Tech Blog AWS Blog Facebook Engineering Uber Engineering Blog Airbnb Engineering GitHub Engineering Blog Intel Software Blog LinkedIn Engineering Paypal Developer Blog Twitter Engineering Last but not least, volunteer for new projects at your company, and learn from senior engineers and architects to further improve your system design skills.\nI hope this course was a great learning experience. I would love to hear feedback from you.\nWishing you all the best for further learning!\nReferences Here are the resources that were referenced while creating this course.\nCloudflare learning center IBM Blogs Fastly Blogs NS1 Blogs Grokking the System Design Interview System Design Primer AWS Blogs Martin Fowler PagerDuty resources VMWare Blogs Blog Disclaimer: This webpage is a modification of @karanpratapsingh\u0026rsquo;s system-design repository with CC BY-NC-ND 4.0 license.\n","permalink":"https://samirpaul1.github.io/blog/posts/system-design-course/","summary":"System Design Course","title":"System Design Course"},{"content":" This webapp uses Huffman Coding for Text Compression and De-compression. Made with JavaScript, HTML5 and CSS3. Live Demo: samirpaul1.github.io/txt-compressor Repository: github.com/SamirPaul1/txt-compressor About this application: An online text(.txt) file compressor, decompressor which uses Huffman Algorithm to encode/compress files by 35% and decode them back to the original size. This tool assigns a variable-length code to the characters of the uploaded file based on the frequency of occurrence. Then converts characters to that special code which takes less size than the original ASCII codes. Huffman code forms a binary tree assigning the most frequent characters with the smallest codes and longer codes for the least frequent characters. A Huffman code is a tree, built bottom up, starting with the list of different characters appearing in a text and their frequency. With this lossless data compression method, this tool can compress the file size by 35 to 40%. As file size gets reduced and original characters get changed to special characters so this encoding also improves security by encrypting the file during file sharing. With the decoding feature, the user can decode the encoded file and get back the original file of the previous size. I have used JavaScript to implement the algorithms so that browser can compile the code and HTML, CSS to make the website responsive. Additional instructions and warnings are provided if steps are not followed correctly. An Info page is added to give more information about tecnique of Lossless Data Compression with Huffman coding. Video Demo: Landing Page: Upload File Select Action (Compress / De-compress) Wait for File Download\nFile gets downloaded automatically when selected process is complete.\nCompression - Compression Ratio is also displayed\nDe-compression Additional Instructions and Warnings are provided if the above steps are not followed correctly About the tecnique of Lossless Data Compression with Huffman coding. ","permalink":"https://samirpaul1.github.io/blog/posts/text-file-compressor-de-compressor-web-app/","summary":"Text File Compressor De-compressor Web App","title":"Text File Compressor De-compressor Web App"},{"content":" I am an undergraduate student at the National Institute of Technology Durgapur. I enjoy problem-solving and coding. Always strive to bring 100% to the work I do. I am passionate about developing complex applications that solve real-world problems impacting millions of users. Download CV\nNATIONAL INSTITUTE OF TECHNOLOGY DURGAPUR\nB.Tech (Electronics and Communication Engineering) Nov 2020 - July 2024\n• 9/10 CGPA.\nSILIGURI BARADAKANTA VIDYAPITH\nWBCHSE \u0026amp; WBBSE 2013 - 2020\n• WBCHSE (Senior Secondary Class XII) 92.00% PCM \u0026 Computer Science.\n• WBBSE (Secondary Class X) 96.14% General Studies.\nAMAZON\nMachine Learning Trainee Jun 2022 - July 2022\n• Learned and implemented various concepts of Machine Learning(Supervised-Unsupervised Learning, Neural Networks).\nNITI AAYOG\nData Management and Analytics Intern July 2021 – Oct 2021\n• Worked with NDAP in the data sourcing workstream. Created and analysed datasets for PSUs and autonomous bodies.\n• Used Python NumPy and Pandas in Jupyter notebook to create and visualise PyCharts.\n• Worked on SDG Index and analysed the upcoming use of AR-VR in the non-gaming sector in India.\nTHINK INDIA\nPublic Policy Intern Jun 2021 - July 2021\n• Created reports and blog posts for Think India and worked under the guidance of the Nyaypravah organization.\nCoding Profiles\nSamirPaul1 across coding platforms.\nGitHub • LeetCode • GeeksforGeeks • CodeChef • HackerRank Languages And Tools\nDSAlgo This repository consists of many problems and their solutions in Python 3 on Data Structures and Algorithms which I have come across while Learning DSA on various competitive programming sites. The Git repo is available: Here.⇗\nPortfolio Website A Project for making a blog website attaching about me, contacts, live-map location-services, send a message, all certificates, fetching all current activities on GitHub for opensource contributions. It is a fully customizable website also. The Git repo is available: Here.⇗\nMusic Player A Music Player App developed using JavaScript, CSS, and HTML. The user can make collection of songs, click the Forward play, Backward play, Playlist loop and Shuffle music buttons to change the songs. The source code is available: Here.⇗\nCodeforcesVisualizer Made a Codeforces Visualizer. Users can input any Username and can see particular user ratings as well as other pieces of information in the form of tables and graphs.(HTML,CSS,Jquery,JS charts,Codeforces API). The Git repo is available: Here.⇗\nMake Notes App Users can Make Important Notes as well as save them for future reference. You can mark important as well as non-important which makes it very easy to distinguish between different notes.(Html,Bootstrap,CSS,Javascript). The Git repo is available: Here.⇗\nWeather App A Weather App that pulls from the OpenWeatherMap API to allow users to search for and view the forecast in cities worldwide. Built with JavaScript, CSS, HTML, Theodinproject, OpenWeatherMap API. The Git repo is available: Here.⇗\nText File Compressor An online .txt file compressor, de-compressor tool uses Huffman Coding for Lossless Data Compression. An Info Page is also added about the compression technique and the compression ratio is also displayed. (HTML, CSS, PHP, Javascript). The Git repo is available: Here.⇗\nOnline PDF Compressor An online PDF file compression tool. Python Flask is used to upload a file to a temporary location on the server then using the PDFNetPython library that file gets reduced and automatically removed after 1 hour. (Python3, Flask, C, Shell, Nix, HTML, CSS, JavaScript). Source code available: Here.⇗\nLinktree Clone A simple webpage to group all profiles on social networks in one place. A clone of popular social media referencing site Linktree, under the MIT license. The website is made responsive (with HTML5 and CSS3). The GitHub repository is available: Here.⇗\n✅ Some of my verified skills and certifications:\n","permalink":"https://samirpaul1.github.io/blog/about/","summary":"About","title":"About"}]